[
  {
    "Step Name": "Adapter Execute",
    "Step Details": "Detailed Description",
    "Step Properties": [
      "Service Columns - This element is used to define new columns of specific types based on adapter's service output format.\n\t\t\t\tThe columns are created automatically if the particular adapter is connected.",
      "Mapping - ",
      "Adapter Name - Name of the adapter target.",
      "Service Name - Name of the adapter's service.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Adapter Reader",
    "Step Details": "Detailed Description",
    "Step Properties": [
      "Adapter Name - Name of the adapter target.",
      "Service Name - Name of the adapter's service.",
      "Columns - This element is used to define new columns of specific types based on adapter's service output format.\n\t\t\t\tThe columns are created automatically if the particular adapter is connected.",
      "Shadow Columns - These columns, together with the columns property, define the record format of the output endpoint. The shadow\n\t\t\t\tcolumns are not visible outside of the Plan file where the input step is used.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Adapter Select",
    "Step Details": "Detailed Description This step reads data from an adapter's service (optionally parameterized by some data from the input data flow).\n\t\tFor every entry of the result set it creates a copy of the processed input row with data from the result set row\n\t\tadded according to the given mappings. If no row is returned from the service, the appropriate record is removed from the\n\t\tdata flow.",
    "Step Properties": [
      "Service Columns - This element is used to define new columns of specific types based on adapter's service output format.\n\t\t\t\tThe columns are created automatically if the particular adapter is connected.",
      "Mapping - Maps input data flow columns to adapter's service input columns (service parameters).",
      "Adapter Name - Name of the adapter target.",
      "Service Name - Name of the adapter's service.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Adapter Writer",
    "Step Details": "Detailed Description",
    "Step Properties": [
      "Column Mappings - Maps input data flow columns to adapter's service input columns.",
      "Adapter Name - Name of the adapter target.",
      "Service Name - Name of the adapter's service.",
      "Write All Columns - Specifies whether to write out all columns as defined in the input data flow format. This attribute is exclusive\n\t\t\t\tto column definitions. If this value is set to true, then there must be no columns defined\n\t\t\t\tin the columns element, otherwise an error is reported.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Address Doctor",
    "Step Details": "Detailed Description The Correction Only (BATCH) processing mode is designed for batch processing of data \n\t\twithout output selection possible. This step, which simulates this mode, \n\t\twill process the input and will try to autocorrect it (if needed), providing a one result per line of input. If you want the step to create a detailed log of actions performed during the plan run, click on\n\t\t\tthe dropdown triangle next to the Run button and select Run Configurations . In the Run Configurations dialogue go to the Runtimes tab and in the VM Arguments field write -DaddressDoctor.debugLog=debug.xml . This specification will\n\t\t\tsave debug.xml into the folder with the plan. Alternatively, you can specify an arbitrary\n\t\t\tfull path.",
    "Step Properties": [
      "Data Folder - The folder containing configuration file(s) SetConfig.xml and optionally Parameters.xml.",
      "Parameters File - The path to specific parameters configuration file.",
      "Input Elements - List of input address elements source.",
      "Output Elements - List of output address elements mapping.",
      "Result Parameters - List of parsing result output mapping.",
      "Process Status - Name of column to store parsing process status.",
      "Element Scorer - Element which stores scoring settings for address elements.",
      "Scorer - Element which stores basic scoring settings.",
      "Id - Step identification string.",
      "When Condition - Expression that must be satisfied for this step to be executed."
    ]
  },
  {
    "Step Name": "Address Identifier",
    "Step Details": "Detailed Description This step performs address identification based on reference data and a parsing rule definition.\n\t\t\tAddress identification means that the step returns a unique address ID corresponding to input\n\t\t\tvalues (city, street, street number, etc.) which are parsed and compared with reference data within the\n\t\t\tidentification process. The step checks relationships between particular components and supplies missing relationships. Address patterns are defined as string describing allowed text in the input.\n\t\t\tThe allowed text can be defined by both, ordinary strings and components.\n\t\t\tThe component definition syntax is {COMPONENT_NAME [:spec][!]} .\n\t\t\tAvailable component names are: DISTRICT CITY CITY_DISTRICT - small part of a city STREET POST_OFFICE ZIP PSC - see ZIP CITY_PART - bigger part of a city (usually consisting of city districts) LRN SN * - universal component matching any text including spaces Exclamation mark ( ! ) indicates that text matched by the component\n\t\t\tmust be found in dictionary (exact or approximative match). Spec indicates additional specification for the component.\n\t\t\tCurrently only few components use additional specification.\n\t\t\tFor LRN and SN components the values available are: CZ for parsing numbers according to czech rules. {LRN:CZ} SK for parsing numbers according to slovak rules. {SN:SK} When LRN and SN are used without additional specification, it is assumed that czech rules are requested.\n\t\t\tComponent * defines a trailing characters as its additional specification. {*:;} matches any text till the first ; character. The algorithm uses several scorers as opposed to ordinary steps that use only one. \n\t\t\tThe scorers and their associated scoring entries are summarized in the following table: Scorer Entry Attribute Scorer AI_CITY AI_CD AI_STREET AI_SN AI_LRN AI_PC AI_CITY_STREET AI_STREET_SN AI_CITY_LRN AI_PC_CITY AI_CITY_CD AI_PC_CITY_STREET AI_CITY_CD_STREET AI_CITY_STREET_SN AI_PC_CITY_CD AI_NOT_VERIFIED AI_NO_ADDRESS AI_NO_ADDRESS_ID AI_NO_PATTERN AI_ID_MISMATCH AI_SWAP_LRN_SN AI_UNDECIDED_LRN_SN AI_EVN AI_LRN_OR_EVN AI_CD_SUFFIX Parser Scorer AI_STREET_REPL_PART AI_STREET_REPL_FULL AI_LOCALITY_REPL_PART AI_LOCALITY_REPL_FULL AI_LOCALITY_PART_REPL_PART AI_LOCALITY_PART_REPL_FULL AI_POST_OFFICE_REPL_PART AI_POST_OFFICE_REPL_FULL AI_CITY_PART_REPL_PART AI_CITY_PART_REPL_FULL AI_DISTRICT_REPL_PART AI_DISTRICT_REPL_FULL AI_ZIP_REPL_PART AI_ZIP_REPL_FULL Scorer AI_DISTRICT_NULL AI_LOCALITY_NULL AI_LOCALITY_PART_NULL AI_STREET_NULL AI_LRN_NULL AI_SN_NULL AI_ZIP_NULL AI_POST_OFFICE_NULL AI_MUNICIPALITY_PART_NULL AI_STREET_MISSING AI_STREET_SURPLUS AI_DISTRICT_CHANGED AI_LOCALITY_CHANGED AI_LOCALITY_PART_CHANGED AI_STREET_CHANGED AI_LRN_CHANGED AI_SN_CHANGED AI_ZIP_CHANGED AI_POST_OFFICE_CHANGED AI_MUNICIPALITY_PART_CHANGED AI_LRN_TYPE_CHANGED AI_DISTRICT_SET AI_LOCALITY_SET AI_LOCALITY_PART_SET AI_STREET_SET AI_LRN_SET AI_SN_SET AI_ZIP_SET AI_POST_OFFICE_SET AI_MUNICIPALITY_PART_SET Ambiguity Scorer AI_AMBIG_CL AI_AMBIG_CL_CD AI_AMBIG_CL_ZCD AI_AMBIG_CS_N AI_AMBIG_SN_Q AI_AMBIG_ZCD_L AI_AMBIG_ZCS_L AI_AMBIG_ZCS_N AI_AMBIG_ZC_L",
    "Step Properties": [
      "Analyzed Address Layout - String that contains the formula for a address string composition from the input fields.\n\t\t\tThe address is sequentially processed by the parsing routine which searches for all\n\t\t\tpatterns to which the address string may match. The formula contains characters and components\n\t\t\twhich represent the value of a column. Components are enclosed in curly brackets ('{' and '}') and\n\t\t\teach contains a column name.",
      "Config File - String that contains the name of the configuration file which contains the specification\n\t\t\tof component parsing by templates (element parser factory) and\n\t\t\talso the specification of component identification and verification\n\t\t\tas against the UIR-ADR (element verifier factory).",
      "Num Approx Matches - Number specifying the maximum number of components which may be found by the approximative search.\n\t\t\tMinimum length of input text that can be searched approximatively is 3 for all text components\n\t\t\t(all components but SN and LRN).",
      "Errors - Column that stores the bitwise sum of particular error codes (replaced by the functionality of the scorer).",
      "Error Explanation - Column that stores a list of error descriptions (replaced by the functionality of the scorer).",
      "Parser Rule Name - Column that stores the name of the applied rule (rules are defined in the parser factory element).",
      "Has Streets - Column (type BOOLEAN) that stores information about the existence of the street(s) in the recognized address.\n\t\t\tIf the address is not detected, then the value isnull. If a\n\t\t\tdecision about street existence cannot be made, then the value is alsonull.",
      "Trashed - Column that stores all parts of the input that have been detected but are not part of the address.",
      "Output Components - Specifies columns where to store particular component values of the address.\n\t\t\t \tBy default this property is set so that no component is stored in any column.",
      "Ambiguity Scorer - Scores cases when address id cannot be determined due to ambiguity.",
      "Parser Scorer - Scores cases when partial and/or full replacements have been applied during input parsing.",
      "Attribute Scorer - Scores relations of address attributes (components).",
      "Clean Accidential Czech Letters - Specifies whether the algorithm should fix accidental errors caused by replacing czech letters with digits (ř -> 5).\n                Default value: false.",
      "Scorer - Element which stores basic scoring settings.",
      "Id - Step identification string.",
      "When Condition - Expression that must be satisfied for this step to be executed."
    ]
  },
  {
    "Step Name": "AdvancedProfilingStep",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Advanced Server Components",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Advanced Usage",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Aggregating Column Assigner",
    "Step Details": "Detailed Description Calculates aggregation expression based on one or more grouping.\n\t\t\tThe result of expression is assigned to record column similar to ColumnAssigner step.\n\t\t\tEach grouping is defined in the Aggregations element which defines\n\t\t\tgrouping key components and a set of aggregating expressions.",
    "Step Properties": [
      "Aggregations - Set of definitions of aggregation.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Alter Format",
    "Step Details": "Detailed Description Performs transformation of the input data format.\n\n\t\tThe output data is given new data formats, either by removing or adding new column definitions.\n\t\tNew columns are added behind existing columns of the input format, and can be initialized\n\t\tas part of the column definition. Columns can be replaced (the data type can be changed and values converted)\n\t\tby both removing and adding. The position of replaced column will be the same as in input format. In the following example, the column data will be removed, then a new column newday will be added.\n\t\t\tNext, the column id will be converted to a string by first being removed, then added as a column with the\n\t\t\tsame name, and evaluated with the expression toString(). Other format columns will remain unchanged.",
    "Step Properties": [
      "Added Columns - List of columns to be added.",
      "Removed Columns - List of columns to be removed.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Anonymizer CZ",
    "Step Details": "Detailed Description This step anonymizes the input data. The content of the output data is different than the input, but\n\t\t\tall the essential characteristics of the data are retained. The step supports the following data types: Data Type Description Date (of birth) Date is mostly used in conjunction with birth number (RC) and therefore it preserves the RC eras\n\t\t\t\t\t\t\t(see below). If the date is outside of the allowed range the transformation doesn't take place,\n\t\t\t\t\t\t  the input value is sent to output unchanged and the AA_BD_NOT_CHANGED flag is set. This step shifts the date within the original RC era. The older the date (meaning how far\n\t\t\t\t\t\t\tit is from Birth Day Max ), the wider the range from which the transformed date is\n\t\t\t\t\t\t\tchosen from. The minimum range is 1 year (ie. +/- 6 months from the input date) This range gets\n\t\t\t\t\t\t\twider for dates\tfurther in the past(relative to Birth Day Max . At least one year\n\t\t\t\t\t\t\tdifference is required between Birth Day Max and Birth Day Min to ensure the minimum range. For the step to be able to generate dates that are random enough and within the right RC era, the\n\t\t\t\t\t\t\tfollowing restrictions apply to Birth Day Max and Birth Day Min : minimum 1 year difference between Birth Day Min and Birth Day Max Birth Day Min must not be within 1 year before 1.1.1954\n\t\t\t\t\t\t\t\t\tor within 1 year before 1.4.2004 Birth Day Max must not be within 1 year after 1.1.1954\n\t\t\t\t\t\t\t\t\tor within 1 year after 1.4.2004 Birth Number (RC) The transformation of RC follows these rules: The transformation occurs only if the date part of the RC is valid, not artificial and consists of\n\t\t\t\t\t\t\t\t\t\teither 6,9 or 10 characters. Otherwise the input is copied to the output and the AA_RC_NOT_CHANGED is set. if a date is on the input and the input date is outside the allowed range: the input date is copied to the output and the input RC is\n\t\t\t\t\t\t\t\t\t\t\t\tcopied to the output. The AA_BD_NOT_CHANGED and AA_RC_NOT_CHANGED flags\n\t\t\t\t\t\t\t\t\t\t\t\tare set. the input date matches the date from the RC:  both dates are transformed to the same date the input date doesn't match the date from the RC:  both dates are transformed independently and\n\t\t\t\t\t\t\t\t\t\t\t\tthe AA_RC_MISMATCH flag is set. if there is no date on the input and the date from RC is outside the allowed range: the input value is copied to the output and the AA_RC_NOT_CHANGED is set. the date from RC is within the allowed range: the date is transformed and the AA_RC_CENTURY_GUESSED flag is set. The missing year from the RC is guessed as\n\t\t\t\t\t\t\t\t\t\t\t\tthe last possible year before Birth Day Max preserves the era of the input RC, i.e., the output RC is from the same era as the input RC. 3 eras\n\t\t\t\t\t\t\t\t\t\tare recognized (as intervals): < Birth Day Min ,31.12.1953>, <1.1.1954,31.3.2004>,\n\t\t\t\t\t\t\t\t\t\t  <1.4.2004, Birth Day Max > preserves the sex from RC preserves the extended form of the month preserves the length of the RC - the trailer in 10-digit RC is modified (see below), the trailer in\n\t\t\t\t\t\t\t\t\t\t9-digit RC are sent to the output unmodified preserves the CRC characteristics for 10-digit RC if the input RC has a valid CRC - the output RC also has a valid CRC (the last digit of the\n\t\t\t\t\t\t\t\t\t\t\t\t\ttrailer is modified accordingly) if the input RC has an invalid CRC - the output RC also has an invalid CRC, which differs from\n\t\t\t\t\t\t\t\t\t\t\t\t\tthe correct one by the same value as on the input (the last one or two digits of the trailer are\n\t\t\t\t\t\t\t\t\t\t\t\t\tmodified accordingly) First Name The transformation occurs for known names. It preserves: roughly the number and the positions of capital letters sex (based on the name) The quality of the transformation (preserving sex etc..) depends on the dictionary used. When creating\n\t\t\t\t\t\t\tthe dictionary, the names must be divided into 9 groups (masculine, feminine and neutral for first name, last name and \n\t\t\t\t\t\t\tboth) and within these groups suitable replacements with similar frequency occurrences must be found. The transformation of a name (first and last) consists of its replacement with another name using a Name Lookup File Name dictionary. Single and multiple word names are processed. Each token is\n\t\t\t\t\t\t\tconsidered a name if it consists only of letters and/or the apostrophe character ('). Other tokens\n\t\t\t\t\t\t\t(i.e., non-letter) are considered to be delimiters and are not transformed by the step. The transformed\n\t\t\t\t\t\t\tnames are sent to the output separated by the original delimiters. In case the name (or one of the names, if the input contains more than one token) is not found in the\n\t\t\t\t\t\t\tdictionary, the input as a whole is considered invalid, the appropriate flag is set\n\t\t\t\t\t\t\t( AA_FN_NOT_CHANGED or AA_LN_NOT_CHANGED ) and the input is copied to the output\n\t\t\t\t\t\t\tunmodified. The dictionary is a Lookup table . \n\t\t\t\t\t\t\tThe input name is used as the key and the value is the replacement for the original name. Both values must be \n\t\t\t\t\t\t\ttransformed to uppercase before being inserted into the dictionary (the dictionary must be created with the doUppercase parameter set to on). Last name The same rules and restrictions apply as with first names. This step ensures that for all pairs of identical input values, identical output values will be generated.\n\t\t\tIf any input value is empty, then the output is also empty (no flag is set).",
    "Step Properties": [
      "Rc - Input/output for RC.",
      "Birth Date - Input/output for birth date.",
      "First Name - Input/output for first name.",
      "Last Name - Input/output for last name.",
      "Birth Day Max - The maximum possible date which:is considered valid in the input.can appear in the output.Default value: The day of execution of the configuration.",
      "Birth Day Min - The minimum possible date which:is considered valid in the input.can appear in the output.The minimum difference betweenBirth Day MinandBirth Day Maxis 1 year.Default value:1.1.1901",
      "Change Names - Determines whether first and last names will be processed. If set to true, the propertiesFirst NameandLast Nameand the propertyName Lookup File Nameare required.Default value:false",
      "Name Lookup File Name - Filename of the dictionary containing the replacement names. Required ifChange Namesis set totrue",
      "When Condition - Expression that must be satisfied for this step to be executed.",
      "Id - Step identification string.",
      "Scorer - Element which stores basic scoring settings."
    ]
  },
  {
    "Step Name": "Apply Replacements",
    "Step Details": "Detailed Description This step has two modes in dependence on onlyFullReplacement . First it only looks for the input in the dictionary and if it finds then it puts\n\t\t    the dictionary value to the out column. Second \n\t\t\tit performs replacements of all known (defined) strings within the given text string, where \n\t\t\tthe final value is stored in the data output. Transformations are stored in the specified dictionary \n            as a pair of the source value (text to be replaced) and final value (replacement).\n\t\t\tThis step replaces the longest substring of the input that equals the source definition value\n\t\t\tin the dictionary containing the replacement values.",
    "Step Properties": [
      "In - Column that contains the input text.",
      "Out - Column that stores the transformed output text.",
      "Replacements File Name - Dictionary file that contains the source value of the replacement\n\t\t\t\t(text to be replaced) and final value (replacement).\n\t\t\t\tFor more information about the dictionary see a (detailed description).",
      "Tokenizer - Setting of parsing mechanism used to tokenize input strings.\n\t\t\t\tThe default tokenizer setting distinguishes words, numbers and other single characters, except\n\t\t\t\tfor whitespace characters (space, tab). In the following dictionary, the default setting is used.\n\t\t\t\tNote that when onlyFullReplacement set on, this mechanism is not used.",
      "Only Full Replacement - If true then only the whole input string is looked up in the dictionary otherwise it looks for substrings\n  \t\t\t    split by tokenizer.",
      "When Condition - Expression that must be satisfied for this step to be executed.",
      "Id - Step identification string.",
      "Ignored Separators - Definition of characters that might be ignored when input token\n\t\t\t\tcannot be matched to expected token in translation rule. \n\t\t\t\tExpected token can be ignored only if it is defined as ignorable,\n\t\t\t\twith input token either being skipped when it is ignorable as well, or preserved.The set is defined by means ofcharacter set.\n\t\t\t\tFor a detailed description seeStrip titlesand itsexample.Note that whenOnly Full Replacementset on, this mechanism is not used.Default value: none.",
      "Preserve Unsupported Chars - If true then unsupported characters immediately following replaced text will be copied to\n\t\t\t    output, otherwise these characters will be removed. For detailed description of these characters,\n                seeMatching Value Generator Config.Default value:true",
      "Scorer - Element which stores basic scoring settings."
    ]
  },
  {
    "Step Name": "Async Writer",
    "Step Details": "Detailed Description This step is used as writer. Unlike common writers (file, jdbc, ..), the step\n\t\t\tsends incoming records into queue which is read by backend process. The backend process (asynchronous writer) is defined as online service component in service configuration file.",
    "Step Properties": [
      "Service Name - Name of asynchronous writer online service.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Async Writer Component",
    "Step Details": "Detailed Description Defines parameters for connecting and sending records from AsyncWriter step. The component takes out the records from a queue created by the AsyncWriter step and sends them to the target (e.g., for processing by a DQC plan) in batches of 25 records. If the queue is empty, the component waits for a record appearance for a time specified in the Latency Time parameter. After the timeout expires (AsyncWriter did not provide the records fast enough), the target stops its activity (e.g., a DQC plan finishes). If the AsyncWriter writes new records after the timeout expiration, the target is started again to process the records.",
    "Step Properties": [
      "Disabled - Specifies whether component should be disabled.",
      "Latency Time - Maximum amount of time (s) the component waits before moving records read from the queue into the target.Default value: 60",
      "Name - Name of the service. Used in theService Nameproperty of the AsyncWriter step.",
      "Queue Size - Maximal size of a record queue.Default: 100",
      "Target - Specification of target into which records are submitted. Possible implementations: Logger Target, Plan Target."
    ]
  },
  {
    "Step Name": "Authentication & Authorization",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Authentication Service",
    "Step Details": "Detailed Description Protects invoking of services by using a username and password and enriches the request with the identity of the request originator. The component may be used for internal communication requests as well as for requests going to the web service port. In\tthe second case you have to use RequiredRoleFiler in the Http Dispatcher . You can define several different authentication methods. When resolving identity, all defined methods are asked one by one until one of them returns some identity. Therefore, it is important to put the methods in correct order.",
    "Step Properties": [
      "Disabled - Specifies whether component should be disabled.",
      "Methods - Definition of the authentication method.",
      "Role Mapping Provider - Assigns roles based on the roles resolved by the authentication method and requester's IP address."
    ]
  },
  {
    "Step Name": "Avro Reader",
    "Step Details": "Detailed Description The step can create several output streams from one Avro file where rows of one stream can be\n\t\tlogically children to rows in another stream. Eg. if a file contains client records\n\t\twith each client having several addresses, it can be read as a stream of clients\n\t\tand a separate stream of addresses of all clients.\n\t\tThe streams can also be independent.",
    "Step Properties": [
      "File Name - The source Avro file.",
      "Data Streams - Output endpoints defined on the root level.",
      "Data Format Parameters - General parameters for data formatting. This configuration is applied to\n\t\t\tall specified columns unless the column defines its owndataFormaParameterssection.\n\t\t\tFor more information, seeDataFormatParameters",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Avro Writer",
    "Step Details": "Detailed Description Writes data to a standard .avro file, including the schema and the actual data in the binary format.",
    "Step Properties": [
      "Columns - Definition of columns to be written to the Avro file.",
      "File Name - Path to the .avro file where the data should be written.",
      "Record Name - Name of the Avro document.",
      "Write All Columns - Specifies whether to write all columns as defined in the input format. If this value is set to true, there must be no columns defined in the Columns element.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Basic Server Components",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Buffer",
    "Step Details": "Detailed Description The step continuously writes input data into buffer and reads and sends them\n\t\t\tto output. Both writing and reading are independent, so performance of steps\n\t\t\tafter this one doesn't affect performance and processor utilization in prior steps. It brings advantage in multiprocessor environment with enabled parallelism - place\n\t\t\tBuffer after high speed, cpu consuming and parallelized steps (filters) and before heavy loading step that cannot\n\t\t\tbe parallelized (writers, builders, ...)",
    "Step Properties": [
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "CAAddressesBuildLookupData",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Address Identifier CA",
    "Step Details": "Detailed Description Address Validity Levels Value Description DELIVERY_POINT Address identified to specific delivery point (house, unit, PO BOX etc.) BUILDING Address identified up to specific building, building unit is ambiguous STREET Address identified up to specific street, ambiguous house on street (box in rural route for RR addresses) CITY Address identified to city level only (ambiguous street) or addresses within rural areas. POSTAL_CODE Address identified to postal code level. Mainly for Large Volume Receivers addresses. NULL No address component (or component combination) was found in reference data. Address invalid. Address Statuses Value Description V CA POST SERP VALIDITY CODE: No or minimal correction was done to the input value. Software package is able to detect all address components. The result address is valid. N CA POST SERP VALIDITY CODE: The result address is invalid. Software package is unable to detect all address components or make valid corrections. C CA POST SERP VALIDITY CODE: An invalid address is “correctable” “C” when there are one or more components missing or inconsistent from an otherwise valid address; and only one address can be derived from the information provided. Address Labels Value Description VALID Address is correct or differs only in standardization (state name, street direction etc.) CORRECTED_MINOR Address quality is good, minor corrections performed (typos in names, small differences etc.) CORRECTED_MAJOR Address quality is poor, address components corrected (different city or several minor corrections in different address elements) UNKNOWN Address is invalid and was not parsed, or is ambiguous (unidentified to delivery point) INSUFFICIENT_INPUT Input address elements are empty or not sufficient to be identified. Data Quality Scorings Value Description 0 Valid value. No modification was done to the input value or LVR identified. < 400 Corrected value - corrected minor. Small modification was done to the input value (special places removed), but resulting value is still valid and verified. >= 400 delivery point found Corrected value - corrected major. Large modification was done to the input value. Resulting value is unsafe but still verified. >= 400 delivery point not found Unknown value. Input value is null or has wrong structure. Address was not identified. Explanation Codes Value Description A_ADDR_TYPE_DIFFERENT Address Type different. Score 50-1000. A_BOX_MISSING_MORE_IN_RANGE Postal Box number missing. More in range. Score 0. A_BOX_MISSING_SINGLE_RANGE Postal Box number missing. Single range. Score 0. A_BOX_OUTSIDE_SINGLE_RANGE Postal Box number outside single range. Score 0. A_FINAL_ADDRESS_TOO_FAR Too many differences in cleansed output. Score 1000. A_FINAL_ADDRESS_TYPE_SWITCH_TOO_LARGE Address type correction possibly too large. Score 500. A_FINAL_CIVICRR_MISSING_RR_INFO Route service type in civic address missing. Score 50. A_FINAL_ROUTE_NUMBER_MISSING Route service number missing. Score 100. A_FINAL_ROUTE_TYPE_MISSING Route type missing. Score 100. A_FINAL_STATION_NAME_MISSING Station name missing. Score 20-50. A_FINAL_STATION_TYPE_MISSING Station type missing. Score 20-50. A_FINAL_STREET_DIRECTION_DIFFERENT Street direction different. Score 100. A_FINAL_STREET_DIRECTION_MISSING Street direction missing. Score 50. A_FINAL_STREET_TYPE_ADDED Street type added. Score 100. A_FINAL_STREET_TYPE_DIFFERENT Street type different. Score 250. A_FINAL_STREET_TYPE_MISSING Street type missing. Score 50. A_LVR_POSTAL_CODE_FOUND Large volume receivers postal code identified. Score 0. A_MUNICIPALITY_ALTERNATE Alternative municipality name identified. Score 1-20. A_MUNICIPALITY_ALTERNATE_AND_DIFF_PROVINCE Alternative municipality name identified. Province different. Score 1000. A_MUNICIPALITY_ALTERNATE_INVALID Invalid alternative municipality name. Score 0-30. A_MUNICIPALITY_ALTERNATE_VALID Valid alternative municipality name. Score 0. A_MUNICIPALITY_CORRECTED Municipality name corrected. Score 10 * difference. Max 30. A_MUNICIPALITY_DIFFERENT Municipality name different. Score 500-1000. A_MUNICIPALITY_MISSING Municipality name missing. Score 0-40. A_POSTAL_CODE_CORRECTED Postal Code corrected. Score 200. A_POSTAL_CODE_AS_POSTAL_CODE3 Postal Code 3 identified. Score 10. A_POSTAL_CODE_CONGRUENT_6 Postal Code 6 identified. Score 0. A_POSTAL_CODE_CORRECTED_CHARACTER_CHANGE Postal Code corrected. Character found and changed to digit. Score 30. A_POSTAL_CODE_DIFFERENT Postal Code different. Score 500-1000. A_POSTAL_CODE_MISSING Postal Code missing. Score 0-1000. A_PROVINCE_CORRECTED Province code corrected. Score 30. A_PROVINCE_DIFFERENT Province code different. Score 100-1000. A_PROVINCE_INVALID Invalid province code. Score 50. A_PROVINCE_MISSING Province code missing. Score 0-40. A_PROVINCE_STANDARDIZED Province code standardized. Score 0. A_ROUTE_NUMBER_DIFFERENT Route service number different. Score 100-1000. A_ROUTE_NUMBER_MISSING Route service number missing. Score 100. A_ROUTE_NUMBER_ON_GD_ADDRESS Route service number at general delivery address type. Score 500. A_ROUTE_TYPE_DIFFERENT Route service type different. Score 100-1000. A_ROUTE_TYPE_IDENTIFIER_CORRECTED Route service type corrected. Score 30. A_ROUTE_TYPE_MISSING Route service type missing. Score 50. A_ROUTE_TYPE_ON_CIVIC_ADDRESS Route service type at civic address type. Score 250. A_SN_OUTSIDE_RANGE Civic number outside range. Score 0. A_SN_SUFFIX_ADDED Civic number suffix added. Score 0. A_SN_SUFFIX_OUTSIDE_RANGE Civic number suffix outside range. Score 0. A_SRC_LVR_POSTAL_CODE_OVERRIDE Large volume receivers Postal Code identified. All explanation codes overriden. Address\n\t\t               output has to be the same as input according to CA Post. Score back to 0. A_STATION_NAME_ADDED Station name added. Score 70-500. A_STATION_NAME_CORRECTED Station name corrected. Score 10 * difference. Max 30. A_STATION_NAME_DIFFERENT Station name different. Score 100. A_STATION_TYPE_ADDED Station type added. Score 70. A_STATION_TYPE_CORRECTED Station type corrected. Score 30. A_STATION_TYPE_STANDARDIZED Station type standardized. Score 0. A_STATION_TYPE_WNAME_DIFFERENT Input and output station type differs. Station name is missing. Score 50. A_STATION_TYPE_WONAME_DIFFERENT Input and output station type differs. Score 30. A_STREET_ALTERNATE Alternative street name identified. Score 0. A_STREET_DIRECTION_ADDED Street direction added. Score 50. A_STREET_DIRECTION_CORRECTED Street direction corrected. Score 30. A_STREET_DIRECTION_DIFFERENT Street direction different. Score 1000. A_STREET_DIRECTION_STANDARDIZED Street direction standardized. Score 0. A_STREET_NAME_CORRECTED Street name corrected. Score 10-1000 A_STREET_NAME_DIFFERENT Street name different. Score 1000. A_STREET_NAME_MISSING Street name missing. Score 500. A_STREET_TYPE_AMBIGUOUS Street type ambiguous. Score 0. A_STREET_TYPE_CORRECTED Street type corrected. Score 30. A_STREET_TYPE_DIFFERENT Street type different. Score 1000. A_SUITE_ADDED Suite/unit added. Score 0. A_SUITE_MISSING_MORE_IN_RANGE Suite/unit missing. More in range. Score 0. A_SUITE_MISSING_SINGLE_RANGE Suite/unit missing. Single range. Score 0. A_SUITE_MULTIPLE_OUTSIDE_MORE_IN_RANGE Suite/unit multiple outside range. More in range. Score 0. A_SUITE_MULTIPLE_OUTSIDE_SINGLE_RANGE Suite/unit multiple outside range. Single range. Score 0. A_SUITE_OUTSIDE_MORE_IN_RANGE Suite/unit outside range. More in range. Score 0. A_SUITE_OUTSIDE_SINGLE_RANGE Suite/unit outside range. Single range. Score 0. A_SUITE_WITH_HASH_EXTRACTED \"#\" extracted from suite/unit. Score 10. A_UNIT_INTERVAL_CUT Suite/unit interval corrected. Score 0. CIVIC_RR_MATCHED Civic route service address type matched. Score 0. GD_RR_FOUND Route service address at general delivery address type identified. Score 0. Record Types Value Description 1 Street Address Record, Point of Call managed 2 Street Served By Route Record 3 Lock Box Address Record 4 Route Service Address Record 5 General Delivery Address Record Street Directions Value Description N North/Nord S South/Sud E East/Est W West O Ouest NE Northeast/Nord-est NW Northwest NO Nord-ouest SE Southeast/Sud-est SW Southwest SO Sud-ouest Delivery Installation Types Value Description BDP Bureau De Poste CC Concession Commerciale CDO Commercial Dealership Outlet CMC Community Mail Centre CPC Centre Postal Communautaire CSP Comptoir Service Postal LCD Letter Carrier Depot PDF Poste De Facteurs PO Post Office RPO Retail Postal Outlet STN Station SUCC Succursale Route Service Types Value Description RR Rural Route SS Suburban Service MR Mobile Route GD General Delivery",
    "Step Properties": [
      "Input - Input column mappings.",
      "Output - Output column mappings.",
      "Cleansed - Cleansed output column mappings. Available for identified addresses only.",
      "Id - Step identification string.",
      "Data Dir - Directory with lookup files for Canadian addresses. Must point to directory named 'data'\n                in the standard folder hierarchy (<project>/data/ext/lkp/ca_address_identifier)."
    ]
  },
  {
    "Step Name": "Canopy Clustering",
    "Step Details": "Detailed Description The algorithm takes input values defined in Components and calculates distances to selected centroid record. These distances are averaged and \n\t\t\tbased on the loose and tight thresholds the record is possibly assigned to the cluster \n\t\t\tdefined by the centroid record.",
    "Step Properties": [
      "Id - Step identification string.",
      "Components - Definition of tokens.",
      "Sorting Key - Definition of sorting input records. It determines order in which centroid records\n\t\t\t\tare selected.",
      "Locale - Default locale for sorting.",
      "Output Strategy - Type of the output.",
      "Minimal Index - Minimum value of averaged distance. Values below this limit prevent records be assigned\n\t\t\t\tto a cluster.",
      "Loose Threshold - Minimum distance value for the record to become part of the cluster.",
      "Tight Threshold - Minimum distance value for the record to become part of the cluster and stop its \n\t\t\t\tfurther processing.",
      "Cluster Id Column - Column name where to store cluster if of the record. Record can be member of more that\n\t\t\t\tone cluster.",
      "Cluster Role Column - Column name where to store role of the record in its respective cluster.\n\t\t\t\tThe possible roles are: \"C\" for centroid record, \"I\" for record whose distance to centroid\n\t\t\t\tis equal to 1.0, \"T\" for record whose distance is greater or equal to tight threshold\n\t\t\t\tand \"L\" for record whose distance is between loose and tight threshold.",
      "Average Index - Column name where to store weighted average distance.",
      "Record Descriptor Column - Column name where to store record descriptor."
    ]
  },
  {
    "Step Name": "Cassandra Reader",
    "Step Details": "Detailed Description This step is implemented only as MapReduce step running on Hadoop. Local run is not supported. It is impossible to run this step from GUI. You must run plan from command line with \"guava-18.0.jar\" added to Hadoop distributed cache. Tested on Cassandra v2.2.4. It enable user to read data from Cassandra's keyspace and column family.",
    "Step Properties": [
      "Keyspace - Cassandra's keyspace",
      "Column Family - Cassandra's keyspace",
      "Columns - Definition of columns (corresponding by order, names and types).",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Cassandra Writer",
    "Step Details": "Detailed Description This step is implemented only as MapReduce step running on Hadoop. Local run is not supported. It is impossible to run this step from GUI. You must run plan from command line with \"guava-18.0.jar\" added to Hadoop distributed cache. Tested on Cassandra v2.2.4. It enable user to write data into Cassandra's keyspace and column family.",
    "Step Properties": [
      "Keyspace - Cassandra's keyspace",
      "Column Family - Cassandra's keyspace",
      "Column Mappings - Definition of columns (corresponding by order, names and types).",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Loqate Cass",
    "Step Details": "Detailed Description",
    "Step Properties": [
      "Data Folder - The folder contains configuration locate.ini file, Loqate license file and reference data files (data packs).\n                The step expects data files with .lfs format (extracted from the original .lfz archive).",
      "Input Elements - List of input address elements source.",
      "Output Elements - List of output address elements mapping.",
      "Server Options - List of additional server options (advanced use).",
      "Default Country - Default country, used when country identification is not found in input elements.",
      "Geocoding - Use Geo location process.",
      "Max Results - Maximal number of output proposals. When > 1 or not specified, usesSearchprocess of Loqate server.",
      "Record Descriptor Column - Name of the column of type string to store the identification of record in group. The identifier\n\t\t\t\thas the form<group_id>:<record_count>:<record_number>.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Catalog Item Reader",
    "Step Details": "Detailed Description Note: Please note that this step only works when executed using Remote Execution. Step will not read any data when only Local Execution is used.",
    "Step Properties": [
      "Catalog Item - You will be provided with aSelect Catalog Itemdialog where you choose catalog item (Catalog item names are listed here.) Once you choose the one catalog item identification of that catalog item will be stored in this property.",
      "Columns - Definition of columns which will be read from the catalog item.",
      "Id - Step identification string.",
      "Server Name - Connection details for connecting the ONE 2.0 Metadata Server. (File Explorer>Servers)",
      "Shadow Columns - TheShadow Columnsare used by steps to define new columns of a specific type in the output format. The created columns can contain initial data as a result ofDefault Expression.",
      "Workflow State - State of the catalog item thatCatalog Item Readerstep will read.draft: the latest version available.published: the latest approved version available."
    ]
  },
  {
    "Step Name": "Character Groups Analyzer",
    "Step Details": "Detailed Description This step serves as a profiler of input characters. It classifies each character against another character (or a string) according to a character group that the original character belongs to. It is possible to define whether a single character or a sequence of characters belonging to a single group are classified with a single character or string.",
    "Step Properties": [
      "When Condition - Expression that must be satisfied for this step to be executed.",
      "Id - Step identification string.",
      "Analyzed Columns - List of columns to analyze.",
      "Default Character Groups - List of default defined character groups. These groups are used for\n\t\t\t\tanalyzed columns which do not have their own character group definition.",
      "Copy Unknown Characters - Specifies if characters, that do not belong to any character group specified, should be copied (true) or skipped in the output column.\n\t\t\t\tValue is used for analyzed columns which do not have their own character group definition.Default:true"
    ]
  },
  {
    "Step Name": "Check Point",
    "Step Details": "Detailed Description This step reads ALL records from the input and then starts writing\n\t\t\tthem to the output. This means that steps following this point will start their work\n\t\t\tonly when all preceding steps have finished.",
    "Step Properties": [
      "Folder - Folder used for the temporary file.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "DQ Attribute Aggregation Results",
    "Step Details": "Detailed Description Returns results of certain DQ aggregations for attributes.",
    "Step Properties": [
      "Attribute Id - .",
      "Catalog Item Id - .",
      "Aggregation Id - .",
      "Limit - .",
      "Time Range From - .",
      "Time Range To - .",
      "Server Name - Connection details for connecting theONE Web Application. (File\n                Explorer>Servers)",
      "Shadow Columns - TheShadow Columnsare\n                used by steps to define new columns of a specific type in the output format. The created columns can contain initial data as a result\n                ofDefault Expression.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "DQ Catalog Item Aggregation Results",
    "Step Details": "Detailed Description Returns results of DQ aggregations for catalog item.",
    "Step Properties": [
      "Catalog Item Id - .",
      "Aggregation Id - .",
      "Limit - .",
      "Time Range From - .",
      "Time Range To - .",
      "Server Name - Connection details for connecting theONE Web Application. (File\n                Explorer>Servers)",
      "Shadow Columns - TheShadow Columnsare\n                used by steps to define new columns of a specific type in the output format. The created columns can contain initial data as a result\n                ofDefault Expression.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "CII",
    "Step Details": "Detailed Description This step has metrics which are each computed for each input row\n\t\tinteger score using four boolean expressions placed on the metric. This score\n\t\tis then written to a given column unique for each metric. In the end it creates a report containing: number of all rows for each of the four boolean expressions there is a column with the count of positively\n                evaluated rows.",
    "Step Properties": [
      "Metrics - An array of metrics.",
      "Default CII Score - It computes for each row and for each metric an integer score value based on\n\t\t\tthe four expressions in themetric.\n\t\t\tThe score is then stored to the column named in thescoreColumnproperty\n\t\t\tin themetric.",
      "Id - Step identification string.",
      "Report Columns - Input columns which are copied to the rows inserted to the report end point.",
      "When Condition - Expression that must be satisfied for this step to be executed."
    ]
  },
  {
    "Step Name": "DQ Rule Instance Results",
    "Step Details": "Detailed Description Returns results of DQ rule instances. It lives under catalogItem entity. Results include also explanations that provide invalid reasons.",
    "Step Properties": [
      "Attribute Id - .",
      "Catalog Item Id - .",
      "Rule Id - .",
      "Limit - .",
      "Time Range From - .",
      "Time Range To - .",
      "Server Name - Connection details for connecting theONE Web Application. (File\n            Explorer>Servers)",
      "Shadow Columns - TheShadow Columnsare\n            used by steps to define new columns of a specific type in the output format. The created columns can contain initial data as a result\n            ofDefault Expression.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "DQ Term Aggregation Results",
    "Step Details": "Detailed Description Returns aggregated result of DQ for whole term. It lives under term entity. It takes last (see at parameter) results from all attributes where is the term assigned and sums them together.",
    "Step Properties": [
      "Term Id - .",
      "At - .",
      "Limit - .",
      "Time Range From - .",
      "Time Range To - .",
      "Server Name - Connection details for connecting theONE Web Application. (File\n                Explorer>Servers)",
      "Shadow Columns - TheShadow Columnsare\n                used by steps to define new columns of a specific type in the output format. The created columns can contain initial data as a result\n                ofDefault Expression.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "DQ Term Attribute Aggregation Results",
    "Step Details": "Detailed Description Returns results of Term aggregation for attribute.",
    "Step Properties": [
      "Term Id - .",
      "Attribute Id - .",
      "Limit - .",
      "Time Range From - .",
      "Time Range To - .",
      "Server Name - Connection details for connecting theONE Web Application. (File\n                Explorer>Servers)",
      "Shadow Columns - TheShadow Columnsare\n                used by steps to define new columns of a specific type in the output format. The created columns can contain initial data as a result\n                ofDefault Expression.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Collibra Reader",
    "Step Details": "Detailed Description The step was created to read the Asset objects data from Collibra application. It allows the user to specify the Asset Type\n\tof the Assets which should be returned and further filtering it with Domain or Community name .\n\tFor each Asset returned by Collibra a record is created in a plan processing. Both meta-information (ID or name) of the Asset as well as its Attributes are returned. The data provided can be mapped to record columns.\n\tThe idea is that in the output the record representing the Asset will have the both the meta-information and Attributes in a specified column.\n\tThe step allows user to map following information: Asset ID Asset Name Asset Type Domain Id Any Attribute the Asset may have (defined on its Type) The step also allows you to read the Collibra Relationships information. In that case, the step will automatically\n\tfilter the Relationships and returns just those which are relevant to the returned Assets (The relationship where at least\n\tone of the returned Asset is either in Target or Source role). The Relationship data are returned in a separate relations Endpoint. The reason is to always provide all the data without any duplicates. For each Relationship the record is created and its data can be mapped to an appropriate columns. The step allows user to map the following Relationship related information. Relationship ID Id of the Asset which is in Source role for returned relationship Id of the Asset which is in Target role for returned relationship",
    "Step Properties": [
      "Url Resource - The reference of the runtime server resource which contains URL and Collibra credentials. The Generic server resource type\n\tshould be used.",
      "Asset Type - The name of the Type of the Assets which should be returned by the step.",
      "Community Name - The name of theCollibra Community. When specified, only Assets in thisCommunityare returned. In the\n\tmulti-leveled Community scenario. Always the name of the deepest level Community should be specified.",
      "Domain Name - The name of theCollibra Domain. When specified, only Assets in this Domain are returned.",
      "Asset Id Column - This property is incolumns tab.Thename of the columnwhere Id of the returned Asset will be stored.",
      "Asset Name Column - This property is incolumns tab.Thename of the columnwhere the Name of the Asset will be stored.",
      "Asset Display Name Column - This property is incolumns tab.Thename of the columnwhere the Display Name of the Asset will be stored.",
      "Asset Type Column - This property is incolumns tab.Thename of the columnwhere the Type name of the Asset will be stored.",
      "Domain Id Column - This property is incolumns tab.Thename of the columnwhere theDomain Idof the Asset will be stored.",
      "Asset Status Column - This property is incolumns tab.Thename of the columnwhere theStatusof the Asset will be stored.",
      "Last Update Column - This property is incolumns tab.Thename of the columnwhere the date and time of a last Asset update will be stored.",
      "Columns - The optional property allowing user to map values from Collibra Asset Attributes (Characteristics) to a certain output columns.",
      "Relation Binding - The optional property allowing user to return the relationship data for the returned assets.\n\t\t\t\tIf enabled, the user can choose columns to store IDs of related Assets. The data is written to a separate endpoint.",
      "Shadow Columns - Contains a set of additional columns that are not extracted from theXML data and are added to the record format to the columns defined byattributesproperty.Default expressions can access parent output end points using their names. They also can access input end point.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Collibra Writer",
    "Step Details": "Detailed Description The step specifically designed to allow user to import or update Assets, Attributes and Relations in Collibra. Step is utilizing Collibra Import API\n\tcreating Collibra import job based on the data on its input. For more information about Collibra Import API visit this documentation article . The step can work in three modes. Assets only: In this mode the step is importing just Assets and its Attributes. Each row in DQC processing is considered an Asset which\n\tshould be imported to Collibra. Each row has to contain all the important information for Asset to be imported in its columns. In order for record to be imported to Collibra it has to contain information necessary to identify the imported Asset.\n\tThe information necessary to identify the Asset might be: The Id of the Asset The combination of the Asset Name and Domain Name If both of these conditions are not satisfied, the record will be rejected and sent to the rejectedAsset endpoint. For every batch of 1000 not-rejected records one import job is created and sent to Collibra. Each Collibra import job created this way is tracked\n\tby the step and once finished the notification is logged containing the original message from Collibra describing the status of the job. The step can create multiple import jobs at the same time (per 1000 records processed). The issue is that Collibra has hard limit\n\tfor number of running import jobs in parallel which is 4. It is important to have this in mind and reducing the parallelism and not starting multiple\n\tCollibra Writer Steps in the same plan. Relations only: In this mode the step is importing just Relations attempting to add or update Relations in Collibra based on the input data. Each row in DQC processing\n\tis considered a Relation which should be imported. In this mode the both source and target Asset is identified in the same way as while importing Assets.\n\tHowever, the step is not checking any data consistency in the input. Also importing Relations requires a knowledge of Relation type Id from Collibra in\n\torder to work. Therefore using the step to import relations is considered advanced. The processing works the same as for the Assets mode and possible parallelism should be reduced. Mixed mode: In this mode one step can write both Assets and Relations. If this is the case, it will always first update\n\tthe Assets and only after that the Relations.",
    "Step Properties": [
      "Url Resource - The reference of the runtime server resource which contains URL and Collibra credentials. The Generic server resource type\n\tshould be used.",
      "Asset Writer Config - Configuration property which allows user to configure step to accept Asset related data and import it to Collibra.",
      "Relation Writer Config - Configuration property which allows user to configure step to accept Relations related data and import it to Collibra.Note:Due to the complexity of Relations in Collibra this part is for advanced usage only. It is recommended\n\t\t\t\tto use Collibra to create relations between Assets if it is possible.",
      "Import File Size - Maximal size of imported batch",
      "Parallelism - Parallelism",
      "Error Handler - Specify how to treat errors reported by collibra import jobs.",
      "Import File Size - Maximum size in records of one import job. Default = 1000.",
      "Import Job Parameters - Allows to set some parameters for import job API call.\n\t\t\t\tSeedocumentation.\n\t\t\t\tThe parameters are: sendNotification, deleteFile, simulation, continueOnError, relationsAction",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Column Assigner",
    "Step Details": "Detailed Description This step assigns the result of a defined expression to a specified column.",
    "Step Properties": [
      "Assignments - Tag associating individual assignments with defined expressions.",
      "When Condition - Expression that must be satisfied for this step to be executed.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Command line mode",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Complex Xml Assigner",
    "Step Details": "Detailed Description This step creates output XML object representation according to the template. Works the same way and uses the same configuration as Complex Xml Writer , with two differences: Instead of creating one XML file from all objects, it creates one XML representation for each object. These\n\t\t\t\t\tXML strings are added as a new column to the output, which also contains all columns of the top level object stream. For this reason, multiple top level objects are not allowed, exactly one is required.",
    "Step Properties": [
      "Output Column Name - Name of the new column with XML representation of the objects.",
      "Id - Step identification string.",
      "Objects - Top level streams of objects. Every new object adds a new input of the same name,\n\t\t\t\tfrom which these objects will be read - and possibly used for creating output XML file (depending on template).",
      "Template - XML template for the output file. The content of the output file will be this template (xml header will be added automatically),\n\t\t\t\texcept all elements in the template that correspond to a stream/object (by their names)\n\t\t\t\twill be entirely replaced by said stream. For that the stream/object has its own XML template.",
      "Encoding - File data encoding. The possible encodings are all encodings supported by the\n\t\t\t\ttarget Java platform. Some commonly used encodings are: ISO-8859-1,\n\t\t\t\tISO-8859-2, and UTF-8.",
      "Indenting - When this is set, the output XML will be in human readable form, indented multi-line. Uncheck this for smaller size of the file.",
      "Maximum Records In Memory - For each inner object, all its records are stored in the memory until this limit is reached; after that, they are stored\n\t\t\t\tin an external file, which is slower.",
      "Data Format Parameters - Contains data formatting information for the whole step, unless there is another such attribute specified at certain\n\t\t\t\tobject stream, in which case the latter will be used for that object stream and all its substreams. For details refer to theDataFormatParameterssection."
    ]
  },
  {
    "Step Name": "Complex Xml Writer",
    "Step Details": "Detailed Description This step creates output XML file according to the template. Without any advanced configuration,\n\t\t\tthe template is simply written to the output file. For filling it dynamically with some data, we will\n\t\t\tneed objects. An object is a stream of records, and the columns of the records are attributes\n\t\t\tof this object. When a new object is added, a new input of the same name is automatically created for reading\n\t\t\tthese records. When we do have an object and we connect a data stream to its input, we can use it in the template.\n\t\t\tThis is simply done by including an element with the same name as the object. That causes this element to be\n\t\t\tentirely replaced by the XML representation of the object - actually, by one representation for every record read from the input The XML representation of the object is done in very similar way - by its own XML template. But here, we are inside the object,\n\t\t\tand we can use its attributes - columns of the original records - as text values of nodes or attributes of elements. Attribute is\n\t\t\treferenced by its name in curly brackets. The default behavior for handling null values is omitting the entire XML element for all column types except of STRING, which will\n\t\t\tinclude the element with empty content. This behavior can be changed by putting additional flag after the column name inside the\n\t\t\tcurly brackets. This can be either a question mark (?) or an exclamation mark (!). The former removes the entire XML element for\n\t\t\ta null value of the column. The latter forces the XML element with empty content for null value. XML template of objects is not a standalone XML and is not limited to single root element. Any object can have subobjects - more complex attributes that are read from their own input streams and are joined with parent\n\t\t\tobject exactly the same way as in inner join of tables. The attributes/columns for join are specified in the inner objects, in\n\t\t\tthe parameter Column Binding . Inner objects are objects as well and can also have subobjects, etc. This step supports encodings supported by Java, including Unicode formats. Available encodings are listed in step configuration.",
    "Step Properties": [
      "File Name - Name of the output XML file.",
      "Id - Step identification string.",
      "Objects - Top level streams of objects. Every new object adds a new input of the same name,\n\t\t\t\tfrom which these objects will be read - and possibly used for creating output XML file (depending on template).",
      "Template - XML template for the output file. The content of the output file will be this template (xml header will be added automatically),\n\t\t\t\texcept all elements in the template that correspond to a stream/object (by their names)\n\t\t\t\twill be entirely replaced by said stream. For that the stream/object has its own XML template.",
      "Encoding - File data encoding. The possible encodings are all encodings supported by the\n\t\t\t\ttarget Java platform. Some commonly used encodings are: ISO-8859-1,\n\t\t\t\tISO-8859-2, and UTF-8.",
      "Indenting - When this is set, the output XML will be in human readable form, indented multi-line. Uncheck this for smaller size of the file.",
      "Maximum Records In Memory - For each inner object, all its records are stored in the memory until this limit is reached; after that, they are stored\n\t\t\t\tin an external file, which is slower.",
      "Data Format Parameters - Contains data formatting information for the whole step, unless there is another such attribute specified at certain\n\t\t\t\tobject stream, in which case the latter will be used for that object stream and all its substreams. For details refer to theDataFormatParameterssection."
    ]
  },
  {
    "Step Name": "Component Steps",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Compute General Ledger",
    "Step Details": "Detailed Description Algorithm processes input records where it binds columns defined as properties Account Number , Name , Credit Value and Debit Value .\n    \t\tIt then sums values from Credit Value and Debit Value columns for each account \n    \t\tidentified in the input by Account Number defined in the account hierarchy (see property Accounts ). \n \t\t\tAccounts not defined in the Accounts property are ignored. The Accounts property represents \n \t\t\ttree structure of accounts identified by account number. After all records are processed, \n \t\t\tthe summary information is written to the output out_computed endpoint. \n \t\t\tIt consist of records in the following format: accountNumber : Long accountName : String cumulatedCredit : Float. It is account's credit + sum of all children-accounts credits cumulatedDebit : Float. It is account's debit + sum of all children-accounts debits See also steps Reconcile and Mapped Reconcile .",
    "Step Properties": [
      "Accounts - Definition of account hierarchy.",
      "Debit Value - Defines column, which contains value to be added tocumulatedDebitcolumn.",
      "Credit Value - Defines column, which contains value to be added tocumulatedCreditcolumn.",
      "Gl Account - Defines column, which contains account number.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Condition",
    "Step Details": "Detailed Description Filters data flow across multiple branches as determined by the defined\n\t\tconditional expression. There are two main branches: satisfied condition and not-satisfied\n\t\tcondition, which are labeled \"out_true\" and \"out_false\", respectively. For a detailed description of the construction of expressions, please refer to the section expressions .",
    "Step Properties": [
      "Id - Step identification string.",
      "Condition - Logical expression that controls branching."
    ]
  },
  {
    "Step Name": "Convert Phone Numbers",
    "Step Details": "Detailed Description Transforms phone numbers from the old Czech numbering system to the new one which was\n\t\t\testablished in September, 2002. Input of the step consists of the area code and the actual phone number.\n\t\t\tOnly strings which do not contain letters are accepted as valid input, special characters are removed. The input file contains the columns 'original area code', 'original phone number'\n\t\t\tand new 'national number'. Phone numbers (both original and national)\n\t\t\tend with a letter mask to specify the format of the whole number.\n\t\t\tThe number is therefore given as a prefix and its mask. The length of the\n\t\t\tmask for particular prefixes and national numbers is given by a\n\t\t\trenumbering plan. The schema is currently usable only in the Czech Republic,\n\t\t\twe are not aware of any other country where it might be applicable. The resulting dictionary is an indexed table with columns containing the \n\t\t\tprefix of the original number including the leading zero (source prefix), the length of the mask\n\t\t\tin characters and the prefix of the new national number. The table is\n\t\t\tindexed by the source prefix.",
    "Step Properties": [
      "Id - Step identification string.",
      "When Condition - Expression that must be satisfied for this step to be executed.",
      "Area Code - Column that contains input area codes.",
      "Conversion Table File Name - Name of the file containing the conversion table, seeDictionary filesfor the file format description.",
      "National Number - Column that stores the resulting phone numbers.",
      "Omit If Not Converted - Flag specifying whether invalid input phone numbers should be written to the \n\t\t\t\toutput (flag set tofalse) or not (flag set totrue).\n\t\t\t\tIf the flag is set tofalseand the input number cannot be converted\n\t\t\t\t(either not found in thedictionary fileor of incorrect length)\n\t\t\t\tthen the input phone number is sent to the output. Connection of the area code and the phone number is stored\n\t\t\t\tin the output propertyNational Number.\n\t\t\t\tDefault value:True(not to send to output).",
      "Phone Number - Column that contains input phone numbers.",
      "Scorer - Element which stores basic scoring settings."
    ]
  },
  {
    "Step Name": "Create Postal Address CZ",
    "Step Details": "Detailed Description Based on the input data (address components or address id) this step creates a \n\t\t\tuniform address to be printed on an envelope.\n\t\t\t\n\t\t\tThere are two ways of specifying the address components: either component\n\t\t\tvalues given directly as columns or address id to be queried in \n\t\t\tthe address reference data for components.\n\t\t\t \n\t\t\tIf some fields are missing, this situation is reported by means of NULL flags, although\n\t\t\tthis reporting is not generally applied to all components. There\n\t\t\tare several components that are required for generating addresses,\n\t\t\tso these must be non-empty, regardless of how they are defined.\n\t\t\tThe other components may or may not be reported as missing, depending\n\t\t\ton the way how they were acquired. If the address is to be generated\n\t\t\tfrom the reference data, these are not reported as missing since it is assumed \n\t\t\tthat this is the correct situation, whereas if defined directly, they\n\t\t\tare reported as missing.\n\t\t\t\n\t\t\tAdditionally, if any of the required components are missing, the step will not\n\t\t\tgenerate an address .\t\t\t\n\t\t\t\n\t\t\tThe required components are: ZIP code and post office Street, if SN is defined Land registry number Locality Locality part Components that can be missing are: SN City part If the address cannot be generated, the CPA_CANNOT_GENERATE flag is set on.",
    "Step Properties": [
      "When Condition - Expression that must be satisfied for this step to be executed.",
      "Id - Step identification string.",
      "Mode - Defines the method.Possible values:UIR-ADRandRUIAN.\n\t\t\t\tUIR-ADR defines legacy method, whereas RUIAN stands for the method defined by notice\n\t\t\t\tof Czech Office for Surveying, Mapping and Cadastre from November 24th, 2011.\n\t\t\t\tDefault value: UIR-ADR.",
      "Address Id - Identification number of an address point (address ID).\n\t\t\t\tIf this property is linked to an existing output column (i.e., property and its column are defined),\n\t\t\t\tthe address is determined via this value. The rest of bindings MUST NOT be linked to any column.",
      "Locality - Column that contains the input locality (city name).\n\t\t\t\tRequired if a column for addressId is not specified.",
      "Locality Part - Column that contains the input locality part.\n\t\t\t\tRequired if a column for addressId is not specified.",
      "Street - Column that contains the input street name.",
      "Land Registry Number - Column that contains the input land registry number.\n\t\t\t\tRequired if a column for addressId is not specified.",
      "Postal Code - Column that contains the input postal code.\n\t\t\t\tRequired if a column for addressId is not specified.",
      "Post Office - Column that contains the input post office.\n\t\t\t\tRequired if a column for addressId is not specified.",
      "Street Number - Column that contains the input street number.",
      "City Part - Column that contains the input city part.",
      "District Of Prague - Column that contains the input district of Prague (\"mestsky obvod\", Praha 1 - 10).\n\t\t\t\tRelevant only when mode is set toRUIAN.",
      "Cadastral Area - Column that contains the input cadastral area. This value is used\n\t\t\t\tonly for addresses at area of Prague.\n\t\t\t\tRelevant only when mode is set toRUIAN.",
      "Top Row - Column that stores the first row of the address to be printed.",
      "Middle Row - Column that stores the second row of the address to be printed.",
      "Bottom Row - Column that stores the third row of the address to be printed.",
      "Data Folder - Path to the UIR-ADR reference data.",
      "Scorer - Element which stores basic scoring settings."
    ]
  },
  {
    "Step Name": "CSV services",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Custom Web Console Pages Component",
    "Step Details": "Detailed Description Adds Custom Pages section to Admin Center and enables using and creating custom pages within this section. The following page classes are available: FileBrowserPage – displays a list of files in a directory including size and last modification date. FileTablePage – sorts files in a directory into one or more columns; each column has it own regular expression filter. DatabaseQueryPage – displays results of one or more SQL queries, supports linking to other pages.",
    "Step Properties": [
      "Custom Categories - Each custom category corresponds to a section in the Admin Center navigation panel.",
      "Disabled - Specifies whether component should be disabled."
    ]
  },
  {
    "Step Name": "Czech Stemmer",
    "Step Details": "Detailed Description Step uses algorithm by Ljiljana Dolamic and Jacques Savoy. The algorithm was originally published in http://dx.doi.org/10.1016/j.ipm.2009.06.001. Stemming a word is conducted in several steps. After each step, the current word form is searched in user defined lookup. If the form is found in lookup, the stemming process stops. When no lookup file is supplied, all stemming steps are performed. Optional tokenizer may be configured to process composed records. The tokenization is done only for internal use. A record is tokenized, tokens are stemmed separately and then reassembled to one record. All non-alphabetic characters are passed through without any change. Algorithm can run in two modes: aggressive and light. The light version only removes case endings. The aggressive version tries to remove possessives endings, comparative endings, diminutive endings, augmentative suffixes and derivational suffixes.",
    "Step Properties": [
      "Id - Step identification string.",
      "Aggressive Stemming - Checked for aggressive version, unchecked for light version.",
      "Input Column - Column that contains the input text. To properly stem multiple words in a record, configurable tokenizer should be used.",
      "Output Column - Column for stemmed output.",
      "Lookup File - Lookup file used for checking partial stems.",
      "Tokenizer Config - Configurable Tokenizer configuration.",
      "When Condition - Expression that must be satisfied for this step to be executed."
    ]
  },
  {
    "Step Name": "Data Quality Indicator",
    "Step Details": "Detailed Description The step evaluates a set of named boolean expressions (rules) and writes the result to an output column and also creates a summary report.\n\t\t\tIt contains rules which are types of coded boolean conditions. The other fields are not required for step functionality.\n\t\t\tThe step works as follows: Computes all rule conditions for each row from the input. If at least one of the rules was evaluated to false then the code is\n\t\t\t\t    appended to the explainedColumn . Finally, it creates a report with as many rows as there are rules with the following\n\t\t\t\t\tcolumns: Name - name of the rule Code - code from the rule Short Description - short description of the rule Description - description of the rule Success Count - number of input rows where all rules were evaluated to true Total - number of input rows",
    "Step Properties": [
      "Rules - An array of rules.",
      "Id - Step identification string.",
      "Explained Column - Codes of rules which were evaluated to false are appended to this column.\n\t\t\t\tIf there was some non-null value in the input record in this column then\n\t\t\t\tthe codes are appended to this value.",
      "Out Records Filter Type - Specification of which records are sent to the output."
    ]
  },
  {
    "Step Name": "Data Sampler",
    "Step Details": "Detailed Description This step is used to obtain a representative data sample from input data.\n\t\t\tIt computes sizes of groups of records with the same keys. For each cluster of same-sized groups\n\t\t\tit selects a number of groups based on the percentage given. The sample groups are selected\n\t\t\tuniformly from the cluster in order to obtain a uniform data distribution across the whole\n\t\t\tinput data source. At least one entire group is selected even the percentage\n\t\t\tis too low to cover at least one whole group.\n\t\t\tMore grouping rules can be defined to be applied to the records.\n\t\t\tSame-sized groups within a cluster can optionally be sorted.",
    "Step Properties": [
      "Id - Step identification string.",
      "Groups - A list of group sorting/filtering properties."
    ]
  },
  {
    "Step Name": "Data types",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Dbf File Reader",
    "Step Details": "Detailed Description This step reads data from an input file representing a DBF (database) data source.",
    "Step Properties": [
      "Id - Step identification string.",
      "Columns - Contains definitions of columns which will be read from the DBF file.",
      "Encoding - Encoding of the input file.",
      "File Name - Name of the.dbffile.",
      "Shadow Columns - Contains a set of columns that are not present in the input data, but should be\n\t\t\t\tcreated in the output (so that they are then available for further use as any\n\t\t\t\tother \"real\" column read from a file)."
    ]
  },
  {
    "Step Name": "Dictionary Lookup Builder",
    "Step Details": "Detailed Description Generates reference data from a single input stream. NOTE: it removes the old content \n\t\t\tof the reference data folder.",
    "Step Properties": [
      "Id - Step identification string.",
      "Reference Data - Specification of the reference data.",
      "Component Mappings - Mapping of reference data entity components to columns."
    ]
  },
  {
    "Step Name": "Dictionary Lookup Identifier",
    "Step Details": "Detailed Description Identifier that uses dictionary based lookups. Identifies the input record possibly down to \n\t\t\tits primary key. It writes address proposals determined during the processing\n\t\t\tinto a dedicated output endpoint (out_proposals).\n\t\t\t\n\t\t\tThe step proceeds as follows: Examines input stream - it searches for occurrences of known values\n\t\t\t\tof the reference data component. Uses dictionaries defined in the reference data. Matches the found values with supporting vectors and performs\n\t\t\t\tlookups into indices for proposals. Note that each vector might \n\t\t\t\tmatch input values more than once - there exist \"variants\" of\n\t\t\t\tmatching input to the vector. Each vector from step 2 possibly returns a bulk of proposals. Each bulk is\n\t\t\t\tprocessed independently - each proposal is compared with input\n\t\t\t\tand scored by means of user defined scoring and the proposal with the best\n\t\t\t\tscore is selected as the best proposal. If there is a proposal with a \n\t\t\t\tscore less than or equal to the predefined value (see SupportingVectorDefinition),\n\t\t\t\tit is selected as the result and no more vectors are processed.\n\t\t\t\tWhen comparing proposal components with input text approximately,\n\t\t\t\tspaces occurring between two letters of different types (such as\n\t\t\t\tbetween a dot and a letter) can be missing in the input text and such\n\t\t\t\tcases are NOT considered as an error (no scoring case can be triggered by this\n\t\t\t\tsituation). If required, reduces proposals to be rated (and then processed in the subsequent steps). Calculates rate of the input. Writes results to output. If required, limits number of proposals written to the out_proposals end point. Note: each vector may map onto the input string more than once. See\n\t\t\tthe description of SupportingVectorCase.",
    "Step Properties": [
      "Reference Data - Name of the folder where reference data are located. Reference data metadata\n\t\t\t\tare being read from the .metadata file located directly in that\n\t\t\t\tfolder. The folder must exist.",
      "Out Pattern - Name of the column (type STRING) where the pattern corresponding\n\t        \tto the best matching input record should be written.",
      "Out Component Pattern - Name of the column (type STRING) where the pattern consisting \n\t        \tonly of components corresponding to the best matching input record should be written.",
      "Out Supporting Vector Name - Name of the column (type STRING) where the name of the supporting\n\t        \tvector that has been used for finding the address/proposal should be written.",
      "Component Output Mapping - Array of component mappings from identified address to record columns.",
      "Components - Definition of component usage.",
      "Abbreviations Filename - Name of the file holding the string lookup dictionary with common abbreviations.\n\t        \tThe file name is NOT relative to the reference data folder.",
      "Out Proposal Count - Name of the column (type INTEGER) where the number of proposals found\n\t        \tand written out for the input address should be written. This number will always\n\t        \tbe the same as number of proposals written to theout_proposalsendpoint.",
      "Input Layout - Definition of input elements forming data to identify.",
      "Score - Name of the column (type INTEGER) where the score of the input entity should be written.",
      "Explanation - Name of the column (type STRING) where the explanation of the score for the input entity should be written.",
      "Scoring - Definition of scoring rules.",
      "Supporting Vector Cases - Defines which vectors to use and under which conditions.",
      "Max Score To Accept Vector - Defines the maximum value of the score for the best proposal of the currently \n\t\t\t\tprocessed supporting vector case to accept this result and stop\n\t\t\t\tfurther processing of vector cases.\n\t\t\t\tThis is default for all vector cases.\n\t\t\t\tDefault value: 0.",
      "Expert Settings - Expert settings of the step. Here, \"internal\" properties such as input search method, \n\t\t\t\tproposal evaluation method, etc. can be modified.",
      "Record Descriptor Column - Name of the column of type string to store the identification of record in group. The identifier\n\t\t\t\thas the form<group_id>:<record_count>:<record_number>.",
      "Limit Output Proposals - Definition of limitation expressions for reducing proposal count written intoout_proposalsendpoint.\n\t\t\t\tThese elements are scanned one by one possibly stopping at the first\n\t\t\t\tone whosewhencondition is evaluated to TRUE. Itslimitexpression is then used as maximum number\n\t\t\t\tof proposals that can be written toout_proposalsendpoint.",
      "Limit Rated Proposals - Definition of limitation expressions for reducing number of proposals to be rated and then sent to output \n\t\t\t\t(howeverlimitOutputProposalis still effective).",
      "Ratings - Definition of rating cases. All cases are evaluated.",
      "Id - Step identification string.",
      "When Condition - Expression that must be satisfied for this step to be executed."
    ]
  },
  {
    "Step Name": "Dictionary Lookup Reader",
    "Step Details": "Detailed Description This step is intended for reading data from the dictionary for which source data are not\n\t\t\tavailable or to check that the dictionary has been generated as requested.",
    "Step Properties": [
      "Dictionary Folder - Folder containing the dictionary.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Diff",
    "Step Details": "Detailed Description This step joins two separate input data flows into a single output data flow based on corresponding input data flow keys. The data entry points (endpoints) are named respectively in_left and in_right . All data input flows ( in_left and in_right ) must contain columns with key values (primary keys), which are defined as column properties leftKey and rightKey . The keys govern how the data records are paired up across the data input flows. All basic data types can be used as a key. The step is optimized to identify differences in 2 similar data flows (i.e., the data has to be more or less ordered). Using Diff step in its use case (ordered data) brings the following advantages over the Join step: Efficiency. The step looks for a match in the window defined by the buffer property, not in the whole data set. Specificity. Diff step sends each record to the output exactly once, even when multiple records match. Join step in this case outputs Cartesian product of all matching records. Diff step is non-blocking. To join the data flows, Join step needs to read all the records; Diff starts outputting data immediately. If it finds a match, it will output it straightaway. If it does not, it will output the record after reading buffer number of records. Note: Make sure the data is ordered, otherwise the step does not find matches. The step behavior: 1. Read buffer number of records from left and right input data flows. 2. Compare the records: use leftKey and rightKey values to determine whether left and right records are the same: If the records match, put them on the output and return to step 1. If the records do not match, compare the orderColumns : loop up through Buffer records on the side of the record with higher orderColumn value and compare them with the other record: If match is found, put it on output and remove the used record from the buffer. If match is not found, put the record with lower order column on the output (the one from the side that was not looped through) and remove it from the buffer. 3. Return to step 1. If the data input flow contains multiple records with the same key values, the data output flow contains the matched records (not a Cartesian product of data input records as the Join step does). Records having a null key value are not joined with any other (null-keyed) records and are processed as unpaired. Note: Only data defined in the expression property of the columnDefinition properties are written to the output. If the property columnDefinitions is empty or missing, then the output remains empty.",
    "Step Properties": [
      "Buffer Size - Buffer is used to set the size of the window (number of records) in which the step will search for a match.",
      "Column Definitions - Defines output data. Contains a set ofcolumnDefinitionproperties which define operations executed on the conjuncted input data. The content of the output data is the result of expressions operated on the conjuncted input data.",
      "Id - Step identification string.",
      "Left Key - Column that contains key values of the left (first) input flow. The keys are used to decide whether the left and right records are the same.The key can be defined as a composite key.",
      "Left Order Column - Values in this column are used to order the records within the buffer window. Typically a timestamp, or an order of the records.",
      "Right Key - Column that contains key values of the left (first) input flow. The keys are used to decide whether the left and right records are the same.The key can be defined as a composite key.",
      "Right Order Column - Values in this column are used to order the records within the buffer window. Typically a timestamp, or an order of the records."
    ]
  },
  {
    "Step Name": "DmmDqPlanPreviewWriter",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "DmmReader",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "DmmWriter",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "ONE Basics",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "DqDmmWriter",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "DQI SK",
    "Step Details": "Detailed Description The step evaluates a set of named boolean expressions ( Rules ) \n\t\t\tand writes the result to an output column and also creates a summary report. \n\t\t\tIt contains rules which are types of coded boolean conditions. The other fields are not \n\t\t\trequired for step functionality. \n\t\t\tThe step works as follows: Computes all rule conditions for each row from the input. Sends each row to the 'out' endpoint and adds boolean columns which are named\n\t\t\t\t\taccording to the code of rules to the output format. This implies that no code \n\t\t\t\t\tcan be the same as any of the input column names. \n\t\t\t\t\tThen it stores the results of the conditions into these added columns. Finally, it creates a report with as many slots as there are rules. \n\t\t\t\t\tEach slot can contain one or more rows depending on how grouping has been defined.\n\t\t\t\t\tWhen no grouping is used, then each slot consists of just one row. When the grouping\n\t\t\t\t\tis employed, each slot contains as many rows as there are groups in the input data.\n\t\t\t\t\tData in each slot are calculated from data matching that rule (when condition)\n\t\t\t\t\tand particular group.\n\t\t\t\t\tGenerally, each row in a slot contains the rule with all its fields as columns and the following\n\t\t\t\t\tadded columns: Success_count - how many times the condition of this rule returned a true value Total - count of input rows matching any rule. Total_Per_Rule - count of input rows matching this rule Total_Exposure_Value - total exposure value over all records matching rhis rule Group_By - grouping key",
    "Step Properties": [
      "Rules - An array of rules.",
      "Id - Step identification string.",
      "Default Error Assignments - Default error assignments to be performed when the record is not valid for this rule.",
      "Group By - Definition of grouping.",
      "Output All Records - Indicates to output records valid for rules. Records thar are not valid for any rule\n\t\t\t\tare always sent to output. On contrary, records that don't match any rule (when condition\n\t\t\t\tis false for all rules) are always discarded.",
      "Code Column - Name of the column where to write code of the applied rule.",
      "Table Name Column - Name of the column where to write table name of the applied rule."
    ]
  },
  {
    "Step Name": "Dynamic Expression Assigner",
    "Step Details": "",
    "Step Properties": [
      "Assignments - List of assignments.",
      "Cache Size - Size of cache storing compiled expressions.Default = 10.",
      "Id - Step identification string.",
      "When Condition - Expression that must be satisfied for this step to be executed."
    ]
  },
  {
    "Step Name": "One Hot Encoder",
    "Step Details": "Detailed Description Uses previously created model to encode input data.",
    "Step Properties": [
      "Feature Configs - Configuration of features used to preform encoding of the data based on the model file loaded.",
      "Model File - Name of file with trained model that will be used for encoding.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "One Hot Encoder Trainer",
    "Step Details": "Detailed Description Trains One Hot Encoding model based on selected normalization type and input data.\n                If result column is filled, use trained model to encode training data and output the encoder to output column.",
    "Step Properties": [
      "Feature Configs - Configuration of features used to train the model and perform encoding on training input data.",
      "Output Model File - Name of the model output file.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "English Stemmer",
    "Step Details": "Detailed Description Step uses Porter algorithm, originally published in Porter, 1980, An algorithm for suffix stripping, Program, Vol. 14, no. 3, pp 130-137, Publicly available at: http://tartarus.org/martin/PorterStemmer/def.txt Step uses adapted implementation of Porter algorithm available at: http://tartarus.org/martin/PorterStemmer/java.txt Stemming a word is conducted in several steps. After each step, the current word form is searched in user defined lookup. If the form is found in lookup, the stemming process stops. When no lookup file is supplied, all stemming steps are performed. Optional tokenizer may be configured to process composed records. The tokenization is done only for internal use. A record is tokenized, tokens are stemmed seprarately and then reassembled to one record. All non-alphabetic characters are passed through without any change.",
    "Step Properties": [
      "Id - Step identification string.",
      "Input Column - Column that contains the input text. To properly stem multiple words in a record, configurable tokenizer should be used.",
      "Output Column - Column for stemmed output.",
      "Lookup File - Lookup file used for checking partial stems.",
      "Tokenizer Config - Configurable Tokenizer configuration.",
      "When Condition - Expression that must be satisfied for this step to be executed."
    ]
  },
  {
    "Step Name": "Task Count Lines In Text File",
    "Step Details": "Detailed Description The task counts number of lines in the specified text file.\n\t\t\tThe task will store its output into 3 task variables: TOTAL_LINE_COUNT . Number of all lines in the file IGNORED_LINE_COUNT . Number of lines ignored by ignoredRowRegEx condition DATA_LINE_COUNT . Number of lines that contain data. It is calculated via the following formula: DATA_LINE_COUNT = totalLineCount - ignoredLineCount - numberOfLinesInHeader - numberOfLinesInFooter",
    "Step Properties": [
      "Encoding - File data encoding. The possible encodings are all encodings supported by the\n\t\t\t\ttarget Java platform. Some commonly used encodings are: ISO-8859-1,\n\t\t\t\tISO-8859-2, and UTF-8.",
      "Ignored Row Reg Ex - A regular expression that is compared to lines in the input file. If a line (read as a string) matches the\n\t\t\t\tregular expression, the line will be ignored.",
      "Line Max Read Length - Specified maximum number of characters per line to be read and processed.\n\t\t\t\tThe behaviour for that event is configured byfailOnLongLineproperty.",
      "Fail On Long Line - If the option is set to true, fail processing if line is longer than the limit.If the option is set to false, instead of failing it throws a warning into log file.\n\t\t\t\tThe warning is logged only for first 100 lines.\n\t\t\t\tIf the number of lines which size exceeds allowed, the total number is put into logs.",
      "Line Separator - Specifies the string to be recognized as a line delimiter. The line delimiter\n\t\t\t\tcan be any sequence of letters, although a specific symbol is often used\n\t\t\t\twhich usually depends on the operating system where the file originated from.\n\t\t\t\tThe special symbols are:  \\r = CR, \\l = LF, \\n = LF.Escaped string property.",
      "Number Of Lines In Footer - Specifies the number of lines from the end of the file that will be excluded from\n\t\t\t\tprocessing (footer lines).",
      "Number Of Lines In Header - Specifies the number of lines from the beginning of the file that will be excluded from\n\t\t\t\tprocessing (header lines, comments, etc).",
      "Progress Interval - If a value for progress interval is defined, the task will store its progress into task log.",
      "Source File - Name of the file containing the input data.",
      "String Qualifier - String enclosing a text string. Not defined by default.Escaped string property.",
      "String Qualifier Escape - Escape character escaping theString Qualifierin the original meaning of\n\t\t\t\tthis character. Not defined by default.Escaped string property."
    ]
  },
  {
    "Step Name": "Task Run DPM Job",
    "Step Details": "Detailed Description Runs a DQC plan or component at remote DPM/DPE instance: it is configurable to wait until the process finishes. Logs of the process are stored in the task resources folder. Supported runtime.properties values to tune connections: _name_ is either \"sfs\" indicating connection to Shared file system or keycloak for connection to Keycloak server.\n\t\t\t\n\t\t\t\tataccama.client.connection._name_.http.properties.connectTimeout\n\t\t\t\tataccama.client.connection._name_.http.properties.readTimeout\n\t\t\t\tataccama.client.connection._name_.http.properties.writeTimeout\n\t\t\t\tataccama.client.grpc.properties.max-message-size",
    "Step Properties": [
      "Arguments - Arguments for DQC processing",
      "Async - Indicates whether to run the DPM job asynchronously or wait for its completion (default value: false)",
      "Force Cluster Launch - Indicates that this job is intended to be running on a cluster",
      "Cluster Name - Name of the cluster to run the job at.",
      "Cluster Password - Password for cluster authentication",
      "Cluster Password File - File with password for cluster authentication",
      "Cluster User - User name for cluster authentication",
      "Files - Files necessary for processing",
      "Links - Links to content necessary for processing",
      "Main Plan - Main DQC plan or component",
      "Main Plan Path - Path to the main plan at remote side",
      "Mount Drivers - Drivers to mount at remote side",
      "One Platform - Name of Ataccama ONE platform - this references the remote processing side",
      "Path Vars - Path variables",
      "Priority - Job priority",
      "Root Dir - Root directory",
      "Runtime Configuration - Optional runtime configuration. If not defined, runtime configuration from the environment is used",
      "Working Dir - Working directory of the job. By default it is the",
      "Zip - Indicates whether to download results in packed format"
    ]
  },
  {
    "Step Name": "Task Run DQC Process",
    "Step Details": "Detailed Description Runs a DQC plan or component in a new Java process: the task waits until the process finishes. This allows to use specific Java version and runtime parameters (such as memory settings etc.). The plan is validated before the run: if the plan is invalid, the task will fail with FINISHED_FAILURE state and log details into a log. Logs of the process are stored in the task resources folder. The stdout log also contains a copy of the process command. Note: This task requires the DQC_HOME system or user environment variable to be properly set, otherwise the task may fail. Ensure that DQC_HOME is set for the user running the online server (especially when the server is started as a Windows service).",
    "Step Properties": [
      "Java Dir - Defines the path of the Java JRE or JDK directory to use. The defined directory must contain eitherbin/javaorbin/java.exeexecutable.If this attribute is not specified, the JRE is autodetected using Java'sjava.homeproperty of the current Java process. If this value cannot be resolved, the OS propertyJAVA_HOMEis used instead. If none of the properties is defined, the task fails during validation.",
      "Java Options - Space separated Java options to pass to the Java Virtual Machine.",
      "Parameters - Set of parameters to pass to the DQC component.",
      "Path Variables - Set of local path variables to use with the current task.",
      "Plan File - Relative (to the workflow file) or absolute path to the plan or component to run.",
      "Parallelism Level - Parallelism level for this DQC execution. Default value: 1."
    ]
  },
  {
    "Step Name": "Task Run DQC",
    "Step Details": "Detailed Description Runs a DQC plan or component. The plan is validated before: if the plan is invalid, the task will fail with FINISHED_FAILURE state and log details into a log. Former versions of this task had the runtimeFile parameter, this attribute has been removed. Runtime Configuration is now an argument of the <DQC_HOME>/bin/runewf.[bat|sh] script (for batch jobs). When workflows are started from Admin Center, runtime configuration is read from Server Configuration.",
    "Step Properties": [
      "Parameters - Set of parameters to pass to the DQC component.",
      "Path Variables - Set of local path variables to use with the current task.",
      "Plan File - Relative (to the workflow file) or absolute path to the plan or component to run.",
      "Parallelism Level - Parallelism level for this DQC execution. Default value: 1."
    ]
  },
  {
    "Step Name": "Task Parameters",
    "Step Details": "Detailed Description Parameters",
    "Step Properties": [
      "Description - Arbitrary description of the workflow, e.g., of its purpose.",
      "Name - Human-friendly name of the workflow.",
      "Multiplicity - Multiplicity. Defines the maximum number of simultaneously running instances of this workflow. If 0 is defined or multiplicity is not defined, the number of instances of a given workflow is not limited. The default value is 0. See Workflow Resource Management for more information.",
      "Continue On Failure - Continue on failure. Defines whether the workflow supports error endpoints. If unchecked, the workflow is stopped immediately after any of the tasks fails.",
      "Role - Defines the role that is required to run the workflow. If undefined, anyone can run the workflow.",
      "Final State - Final state expression.",
      "Variables - This is where you set upInputandDerivedvariables for the workflow.",
      "Groups - Groups are used by visualizers to display similar tasks together in the same \"group.\""
    ]
  },
  {
    "Step Name": "Task Operate On File",
    "Step Details": "Detailed Description Provides 10 potential file operations: Copy, Delete, Exists, Not_exists, List, Mkdir, Move, Info, Unzip, Zip, which are defined in the Operation parameter. Some parameters support the following wildcards ? and * at any path level: ? : 1 character, except file separators '/' and '\\' * : 0…N characters, except file separators '/' and '\\' The task can use remote resources (accessible with resource://<resourceName>/<path>/<inputFile> syntax): Amazon S3 server HDFS (if your product package contains Big Data Engine) Note: The task evaluates both /data/tmp and /data/tmp/ paths as a tmp folder, so to operate folder's content without the folder itself, using /data/tmp/?* mask is recommended. All operations naturally respect permissions on the filesystem (i.e. in case of insufficient permissions on the specified source/target, the task fails).",
    "Step Properties": [
      "Operation - Defines a file operation. Possible implementations: Copy, Delete, Exists, Not_exists, Mkdir, Move, Info, Unzip, Zip."
    ]
  },
  {
    "Step Name": "Task Iterate",
    "Step Details": "Detailed Description The task is used to implement a for-each cycle. It sequentially runs child workflows for each input value (e.g., a file or DB row). \n\t\t\tThe task finishes with result OK, unless any of the child workflows does not start correctly or finishes with a wrong result. Break on error has no impact on the task result. When the iterator has no values, the task finishes with result OK.",
    "Step Properties": [
      "Break On Error - If an error occurs, the iteration is either terminated immediately (true) or continues (false).",
      "Iterable - Implementation and configuration of additional iterators. Possible implementations: File Iterator, Set Iterator, SQL Row Iterator.",
      "Iteration Type - SERIAL or PARALLEL run.",
      "Workflow Id - Id of a workflow to run. Workflow Id specification depends on the workflow execution context:\n\t\t\t\n\t\t\t\tIn the server context, use the<sourceId>:<workflowName>.ewfnotation, e.g., WF02:02_03b_Executor.ewf. ThesourceIdprefix is defined in Workflow Server Component.\n\t\t\t\tOutside the server (from IDE or viarunewf.[bat|sh]), you can use two mutually exclusive notations:<sourceId>:<workflowName>.ewf; in this case, the<sourceId>:prefix is ignored and the specified workflow from the current folder is run. This is useful when you want to develop and test workflows locally in the IDE and deploy them to server later on.<path>/<workflowName>.ewf, e.g., ../02_Run_Plans/02_03b_Executor.ewf. Thepathis optional and is relative to the current workflow."
    ]
  },
  {
    "Step Name": "Task HTTP Get",
    "Step Details": "Detailed Description Downloads resource via HTTP(S) protocol. URL resource or URL must be defined. If URL resource is used, then credentials for authorization can be defined in Url Resource configuration. If URL is used, then credentials for authorization can be defined in User Name and Password properties. Task will send requests until it receives HTTP 200 OK status code, or until it exceeds the number of requests specified in Retry property. Note: To use the HTTPS protocol, make sure you import the certificate to the Java truststore file: <JAVA_HOME>/lib/security/cacerts .",
    "Step Properties": [
      "File Name - Path to file where the result of the request is saved. The file can be local, on Amazon S3 server and on HDFS (if your product package contains Big Data Engine).",
      "Http Headers - Set of user defined parameters to pass to request header.",
      "Overwrite Flag - Permission to overwrite target file. The task fails whenOverwrite Flagis false andFile Namealready exist.",
      "Polling Interval - The interval to wait between two requests (ms). Value >0 required.",
      "Post Params - Set of user defined parameters to pass to POST request.",
      "Retry - Specifies how many times a request is re-sent before the task fails.",
      "Url Resource - Implementation and configuration of URL Resource. Possible implementations: URL Expression, URL Resource Bean.",
      "Use Post Method - When enabled, the task uses HTTP POST method. Additional parameters can be defined inPost Params."
    ]
  },
  {
    "Step Name": "Task Link",
    "Step Details": "Detailed Description Link",
    "Step Properties": [
      "Condition - Condition of a link. The target task of the link will be run only if the condition on the link is satisfied."
    ]
  },
  {
    "Step Name": "Task Read File Variables",
    "Step Details": "Detailed Description Source file format and its mapping to variables depends on selected strategy (see File Variable Strategy). In case the limit set in maxLines is exceeded or the value cannot be converted to requested input data type, a warning is issued. Any lines beyond limit are ignored. Once the data is loaded, you can reuse the values across the workflow using the following syntax: ${ewfGetTaskVariable(\"task_name\", \"key\")}",
    "Step Properties": [
      "Encoding - The encoding used in the source file.",
      "File Variable Strategy - Defines how the variables should be read from the source file.",
      "Max Lines - The maximum number of lines that the task reads from file.",
      "Source File - The file that should be read. Default extension:.ewfv."
    ]
  },
  {
    "Step Name": "Task Read SQL Result",
    "Step Details": "Detailed Description Reads data provided by a query to the variables defined by the mapping. If the query returns no result set or the result set contains no rows, then the task fails. Otherwise, the first row of the returned result set is mapped to the variables using defined mappings. Therefore it is recommended to specify the query the way it returns only one specific row or use order by clause in order to retrieve rows in an intended order.",
    "Step Properties": [
      "Connection Name - Name of the database connection.",
      "Mapping - Set of mappings describing mapping of the columns of the result set to the variables. Individual result rows are mapped to the defined mappings in the order they are defined.",
      "Query - Query to be executed to get the data; supports variable mappings."
    ]
  },
  {
    "Step Name": "Task Reload Versioned File System",
    "Step Details": "Detailed Description Reloads the \"versioned filesystem\" for lookups (lookup refresh), including service restart.",
    "Step Properties": []
  },
  {
    "Step Name": "Task Run DQC On Cluster",
    "Step Details": "Detailed Description Starts a MapReduce or Spark processing on a cluster. Runs a DQC plan or component on a cluster in a new Java process: the task waits until the process finishes. This allows to use specific Java version and runtime parameters (such as memory settings etc.). The plan is validated before: if the plan is invalid, the task will fail with FINISHED_FAILURE state and log details into a log. Logs of the process are stored in the task resources folder. The stdout log also contains a copy of the process command. This task currently demands drivers of all database connections existing in the IDE to be placed in <DQC_home>/runtime/lib/. Note: The task can start a processing on a cluster that interacts with BDE via native Hadoop Java API. To trigger a processing on a cluster via REST API, run the runbde.sh script with the Run Shell Script task.",
    "Step Properties": [
      "Parameters - Set of parameters to pass to the DQC component.",
      "Path Variables - Set of local path variables to use with the current task.",
      "Hadoop Source - Name of an existing cluster connection.",
      "Execution Engine - Selects the data processing engine (MapReduce or Spark) to run plans on a cluster.",
      "Java Dir - Defines the path of the Java JRE or JDK directory to use. The defined directory must contain eitherbin/javaorbin/java.exeexecutable.If this attribute is not specified, the JRE is autodetected using Java'sjava.homeproperty of the current Java process. If this value cannot be resolved, the OS propertyJAVA_HOMEis used instead. If none of the properties is defined, the task fails during validation.",
      "Java Options - Space separated Java options to pass to the Java Virtual Machine.",
      "Plan Path - Relative (to the workflow file) or absolute path to the plan or component to run on a cluster."
    ]
  },
  {
    "Step Name": "Task Send Mail",
    "Step Details": "Detailed Description The task sends an email to multiple email addresses. It can also send local attachments (e.g., log files). Note: Result state of this task indicates only that the email was sent successfully to a mail server. It does not indicate whether the email was received by recipients. If you are planning to send emails when a previous task fails, make sure to set the Continue on Failure global property to true. For detailed information, see Global Workflow Properties.",
    "Step Properties": [
      "Attach Files - Path to the attachment relative to the workflow file.",
      "Body - Text message of the email.",
      "Charset - Encoding of the message. Default value = utf8.",
      "Email Subject - Subject of the email.",
      "From - Email address of the sender.",
      "Mail Type - Format of the email (plain text or HTML).",
      "Smtp Server - Name of a previously defined SMTP server. See Creating a New Server Connection.",
      "To - Recipient of the email: a valid email address."
    ]
  },
  {
    "Step Name": "Task Run Shell Script",
    "Step Details": "Detailed Description Runs linux-like shell command: use it specifically for unix-like OS tasks. Warning: To prevent collisions between unix shell variables format and workflow expression format, this task uses a different expression markup: $%expression()% instead of the regular ${expression()} markup.",
    "Step Properties": [
      "Command - Command or file name to execute.\tProcessing of the command: 1. The possible variable mapping replacements are performed. 2. Whole command value is copied to the temporary script file (located in task's resources directory). 3. Temporary script file is executed using specified interpreter. 4. Read return value is returned.",
      "Expected Return Codes - Comma separated list of integer values representing valid return codes of the command.",
      "Interpreter - Defines interpreter to use to process the command value. Default: /bin/sh.",
      "Log Start Stop - If set to true, the task writes start/stop marks to the standard output log (stdout).",
      "Wait For - Defines whether the task should wait until the job finishes. Wait for modes:waitFor=true:Task waits until the run command is done. Returned code is then compared against Expected Return Codes. If expected codes do not contain the returned code, the task fails.waitFor=false:Task only starts up the command and then terminates immediately. That means that there is no valuable return code to compare against Expected Return Codes, therefore this comparison is skipped. Task finishes in OK state and the result of the task is set to -1 (\"unknown\" value).",
      "Working Dir - Working directory.  Default value: current java-process directory (usually value of \"usr.dir\" property)."
    ]
  },
  {
    "Step Name": "Task Sleep",
    "Step Details": "Detailed Description Waits for a specified amount of time.",
    "Step Properties": [
      "Sleep Time - The time to sleep (ms)."
    ]
  },
  {
    "Step Name": "Task Execute SQL",
    "Step Details": "Detailed Description Executes an SQL Query. The task does not process the possibly returned result in any way, therefore it is suitable for commands returning no result, such as inserts, updates, etc.",
    "Step Properties": [
      "Connection Name - Connection name to use for query invocation. Connection names are taken from Runtime Configuration.",
      "Query - Query to execute."
    ]
  },
  {
    "Step Name": "Task Synchronize Tasks",
    "Step Details": "Detailed Description In larger workflow configurations consisting of many workflow tasks, Synchronize Tasks can be used to unite several tasks so that they are processed as a single item, which can serve multiple purposes: To ensure a task is not skipped if one of its dependencies fails. To help with the general workflow structure and enable easier visualization. To facilitate an easy replacement of tasks.",
    "Step Properties": []
  },
  {
    "Step Name": "Task Task",
    "Step Details": "Detailed Description Task",
    "Step Properties": [
      "Id - The unique ID of the step, which is displayed under the task on the canvas.",
      "Name - A human-readable name of the task.",
      "Priority - Task priority. Defines the execution priority for unlinked tasks. The default value is 0. See Workflow Resource Management for more information.",
      "Description - Task description. An optional description of what exactly the task is supposed to do.",
      "Executable - Task configuration.",
      "Resources - Task resource requirements.",
      "Accept Mode - Task accept mode. If there are several links leading to a task and some of them are not accepted (because the source task fails or the condition on the link is not met), the accept mode determines whether the task will be run. There are two accept modes:ALL_VALID – all incoming links must be accepted.AT_LEAST_ONE – at least one incoming link must be accepted.The task will not be run until all preceding tasks (and all other prerequisites) are finished."
    ]
  },
  {
    "Step Name": "Task Trigger Workflow",
    "Step Details": "Detailed Description If synchronous=false, the task quits immediately after triggering (it does not wait until the invoked workflow is finished); otherwise, the task waits until the invoked workflow is finished. In the synchronous mode, task result is equal to invoked workflow result.",
    "Step Properties": [
      "Parameters - Set of parameters to pass to the workflow as global variable values.",
      "Synchronous - Invocation type.",
      "Workflow Id - Id of a workflow to run. Workflow Id specification depends on the workflow execution context:\n\t\t\t\n\t\t\t\tIn the server context, use the<sourceId>:<workflowName>.ewfnotation, e.g., WF02:02_03b_Executor.ewf. ThesourceIdprefix is defined in Workflow Server Component.\n\t\t\t\tOutside the server (from IDE or viarunewf.[bat|sh]), you can use two mutually exclusive notations:<sourceId>:<workflowName>.ewf; in this case, the<sourceId>:prefix is ignored and the specified workflow from the current folder is run. This is useful when you want to develop and test workflows locally in the IDE and deploy them to server later on.<path>/<workflowName>.ewf, e.g., ../02_Run_Plans/02_03b_Executor.ewf. Thepathis optional and is relative to the current workflow."
    ]
  },
  {
    "Step Name": "Task Wait For File",
    "Step Details": "Detailed Description Waits for a file to appear (or disappear) with a defined timeout. If the file does not appear (disappear) in a given period of time, the task fails. Otherwise it succeeds and returns FINISHED_OK state. The task can use remote resources (accessible with resource://<resourceName>/<path>/<inputFile> syntax): Amazon S3 server HDFS (if your product package contains Big Data Engine)",
    "Step Properties": [
      "File Name - Path to the file(s) to appear (disappear).Supports wildcards.",
      "Polling Interval - The interval to wait between two checks of the condition (ms). Value >0 required.",
      "Timeout - The maximum amount of time the task waits for the condition to be true (ms). After this time, the task fails (it does not wait anymore even if the condition is just being checked). Value >0 required.",
      "Wait For - If the value is set to \"appear\", the task waits for a file appearance. If the value is \"disappear\", the task waits for the file removal (i.e. some lock file)."
    ]
  },
  {
    "Step Name": "Task Wait For SQL Row",
    "Step Details": "Detailed Description Task waits (at maximum given amount of time) until specified query returns result containing at least one row. If the query returns no result set, the task fails immediately. If the result set contains no row, the task keeps waiting.",
    "Step Properties": [
      "Connection Name - Name of the database connection.",
      "Polling Interval - The interval to wait between two checks of the condition (ms). Value >0 required.",
      "Query - Query to execute over the database; supports variable mappings.",
      "Timeout - The maximum amount of time the task waits for the condition to be true (ms). After this time, the task fails (it does not wait anymore even if the condition is just being checked). Value >0 required."
    ]
  },
  {
    "Step Name": "Task Wait For SQL Value",
    "Step Details": "Detailed Description Waits (at maximum given amount of time) until the specified condition is evaluated to true. If the query passed to the task does not return any result (i.e. it does not return any result set containing rows and it does not contain any output parameters), the task fails immediately. If the query returns a result set, only the first row is taken into account: therefore it is recommended to specify the query the way it returns only one specific row or use \"order by\" clause in order to retrieve rows in an intended order. Values read from the query are not stored to the task's variables until the condition is evaluated as true (but still are available in the condition). All objects read from mappings are converted to strings before storing on context (evaluation in conditions, storing to task variables) because workflow does not support data types. When using mapped values in condition, re-typing to the desired type is required (eg. toInteger). When deserializing Date values, use default US Date format \"yyyy-MM-dd HH:mm:ss.\" Since values from the mappings are mapped as global variables in the Condition property, the mapping with the same name as a global variable overrides this global variable's value in the Condition evaluation.",
    "Step Properties": [
      "Condition - Condition determining whether the task can quit. It must evaluate to boolean: if the result is TRUE, the task quits and mapped output values are stored to the task's variables. If the condition's result is FALSE, the task continues to wait/check.",
      "Connection Name - Name of the database connection.",
      "Mapping - Defines how to map the output parameters and values read from the result set to the variables. Each mapping must have name which is used either in the condition or later for setting up the value to the task variable.",
      "Polling Interval - The interval to wait between two checks of the condition (ms). Value >0 required.",
      "Query - Defines SQL query to execute. The query uses the following syntax for passing parameters:${name} – variable mapping, value of the variable name will be passed to the query$+{name} – output parameter mapping, output variable name will be registered to the SQL statement at this place. Mapping property must containOutput Parameterelement with this name (it specifies data type of the output parameter)$#{name} – string replace, value of the variable name will be placed to the SQL query at given position",
      "Timeout - The maximum amount of time the task waits for the condition to be true (ms). After this time, the task fails (it does not wait anymore even if the condition is just being checked). Value >0 required."
    ]
  },
  {
    "Step Name": "Task Run Windows Command",
    "Step Details": "Detailed Description Executes a Windows command: use it only for Windows operations. For Linux operations, use Run Shell Script.",
    "Step Properties": [
      "Command - Command to execute. Processing of the command: 1. The possible variable mapping replacements are performed. 2. EXIT %ERRORLEVEL% instruction is added to the end of the command. 3. Whole command value is copied to the temporary script file. 4. Temporary script file is executed using cmd.exe (you can find the temporary script in the task's resources directory). Utilizing intermediate temporary script file allows the user to define more sophisticated interpreter-related logic.Note: When you call other scripts from your command, please remember to invoke them using call syntax (e.g. call somescript.bat). Otherwise the called script will not return control to the caller once it is done and will not store return value to the OS ERRORLEVEL property. Both of these situations prevent correct result code setup.",
      "Expected Return Codes - Comma separated list of integer values representing valid return codes of the command.",
      "Log Start Stop - If set to true the task writes start/stop marks to the standard output log (stdout).",
      "Wait For - Defines whether the task should wait until the job finishes. Wait for modes:waitFor=true:Task waits until the run command is done. Returned code is then compared againstExpected Return Codes. If expected codes do not contain the returned code, the task fails.waitFor=false:Task starts up the command and then terminates immediately. This means that there is no valuable return code to compare againstExpected Return Codes, therefore this comparison is skipped. Task finishes in OK state and the result of the task is set to -1 (\"unknown\" value).",
      "Working Dir - Working directory. Default value: current java-process directory (usually value of \"usr.dir\" property)"
    ]
  },
  {
    "Step Name": "Task Write File Variables",
    "Step Details": "Detailed Description Mapping of variables to target file depends on selected strategy (see File Variable Strategy).",
    "Step Properties": [
      "Data Format Parameters - Specifies how values of different types should be formatted.",
      "Encoding - The encoding used in the target file.",
      "File Variable Strategy - Defines how the values should be written to the target file.",
      "Target File - The file to write to."
    ]
  },
  {
    "Step Name": "Exact Address Identifier",
    "Step Details": "Detailed Description The algorithm parses input text given as an expression and tries to \n\t\t\tfind the address defined in the reference data. It uses components exactly as parsed\n\t\t\t(i.e. it performs no approximations on input values or no heuristics\n\t\t\tbased on component relation) and searches for address id in internal\n\t\t\tindex lookups. Patterns are organized into groups and the algorithm parses them one by one\n\t\t\tpotentionally stopping when (for a group) either only one address \n\t\t\tor several different addresses are found. In other cases the algorithm\n\t\t\tcontinues with the next group. In ideal case, the algorithm returns address id, pattern name, group name, \n\t\t\treference address components and parsed address components. \n\t\t\tIf there are multiple address ids available, it returns only common \n\t\t\tvalue (the same for all addresses or null) for each component \n\t\t\t(for both, parsed and clean components). \n\t\t \tPattern name is returned only if there is exactly one address, whereas\n\t\t \tgroup name is returned only if the address can be determined or is\n\t\t \tambiguous (since processing stops after parsing particular group). Note that the algorithm always needs value of postal code\n\t\t \tand one or both numbers (LRN and/or SN) for successful identification.\n\t\t \tAccording to data it might need to have other components (locality part, \n\t\t \tlocality and street). It doesn't use municipality part (CITY_PART - městská část),\n\t\t \tdistrict (DISTRICT - okres) nor post office to find address id. The identification process needn't to use all pased components, however,\n\t\t \tall of them are used in verification that the address contains them.",
    "Step Properties": [
      "Address Expression - Expression evaluating into string that is to be examined. Must evaluate\n\t\t\t\tinto a value of typeSTRING.",
      "Reference Folder - Path to the reference data folder containing compiled \"UIR-ADR\" \n\t\t\t\tfiles extended with this algorithm's specific indices. Reference data\n\t\t\t\tcan be generated withUirAdrGeneratoralgorithm \n\t\t\t\t(propertygenerateExactIndicesmust be turned on).",
      "Replacements - Partial and full replacement files for particular components if required.\n\t\t\t\tThere replacements will be applied on parsed component values for both,\n\t\t\t\tstandardization with reference data and \"typing errors\" correction.",
      "Pattern Group Name - Name of the column, where to store name of the group. Expected column\n\t\t\t\tof typeSTRING.",
      "Parser Rule Name - Name of the column, where to store name of the pattern. Expected column\n\t\t\t\tof typeSTRING.",
      "Output Components - Definition of output columns where to store result.",
      "Pattern Groups - Definition of pattern groups.",
      "Components - Definition of user components that can be used in patterns - these\n\t\t\t\tcomponents match text against reference data only - the data\n\t\t\t\tis not processed later in searching for address id.",
      "Id - Step identification string.",
      "When Condition - Expression that must be satisfied for this step to be executed.",
      "Scorer - Element which stores basic scoring settings."
    ]
  },
  {
    "Step Name": "Excel File Reader",
    "Step Details": "Detailed Description Reads rows of table from one sheet of an Excel spreadsheet file. The step is able to read from new Office 2007 file format (OOXML format, extension .xlsx).",
    "Step Properties": [
      "Id - Step identification string.",
      "File Name - Name of the input file.",
      "Sheet - Number of the sheet in the file from which data will be read\n\t\t\t\t(numbering is zero-based, so the first sheet is numbered 0).Ignored ifSheet Nameis specified.Default value: 0.",
      "Sheet Name - Name of sheet in the file from which data will be read.Default value: the parameterSheetis used.",
      "Start Cell - Specification of the first row and column of the input portion of the sheet.\n\t\t\t\tUse the traditional cell reference form [column letter][row number].Default value: 'A1'.",
      "Columns - Contains definitions of the columns that will be read from the input file.",
      "Shadow Columns - Contains a set of columns that are not present in the input data, but should be\n\t\t\t\tcreated in the output (so that they are then available for further use as any\n\t\t\t\tother \"real\" column read from a file).",
      "Skip Empty Rows - Indicates whether rows which contain no data should be ignored.Default value: false (all rows including empty will be sent to output).",
      "Use standard API - Reading large file may cause memory leak when using underlying model API for it.\n\t\t\t\tWhen this is unchecked, efficient reading method instead model API is used."
    ]
  },
  {
    "Step Name": "Excel File Writer",
    "Step Details": "Detailed Description Creates a new, or writes to an existing, Excel file and fills the specified sheet with rows of data. The step is able to create or write into new Office 2007 file format (see property Excel 2007 ),\n\t\t\t\tbut the condition described in ExcelFileReader has to be satisfied.",
    "Step Properties": [
      "Id - Step identification string.",
      "File Name - Name of the output file.",
      "Template File - Name of input file. The content of this file is copied (and possibly changed by input) into output file.\n\t\t\t\tThis allows to have prefilled some sheets, charts, calculations etc\n\t\t\t\twhich will be then applied to written data.",
      "Template Formula Recalculate - The formulas contained in template will have been recalculated before saving of output file.Default = true.",
      "Inputs - List of sheets and related inputs. It is possible to simultaneously write into more sheets.",
      "Date Format - Specifies an optional format for date values stored in the sheet.\n\t\t\t\tUse the format according tothis specification(see the paragraph \"Custom Number Formats\". For example, \"d.m.yyyy\".)Default: the implicit format is used.",
      "Excel 2007 - Specifies the new Office 2007 file format should be created.Default = false."
    ]
  },
  {
    "Step Name": "Expert Server Components",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Expressions",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Extended Repository Reader",
    "Step Details": "Detailed Description This step reads all records from database repository used by extended unification step.\n\t\t\tThe input timestamp, last update timestamp and unification keys can be read, too. Note: fetching of unification keys together with records slows performance, use\n\t\t\tit only when really needed.",
    "Step Properties": [
      "Data Source - Name of the data source for connection to the database repository.",
      "Repository Name - Name of the repository determining the table names in the database.",
      "Pk Column - Column name that stores the primary key value.",
      "Key Column - Name of string column for storing the unification key values. This column usually\n\t\t\t\tcontains all keys needed for fetching candidate groups of all unify operations\n\t\t\t\t(generally all operations using grouping), then the keys are specially serialized:\n\t\t\t\tKeys are separated by \"/\" character, this separator contained in key value itself\n\t\t\t\tis replaced by \"\\-\". For example, two keys \"0:1:840514/1457\" and \"2:0:John Smith\\London\"\n\t\t\t\tcontaining these specials will be converted to string\n\t\t\t\t\"0:1:840514\\-1457/2:0:John Smith\\\\London\".",
      "Input Timestamp Column - Name of datetime column for storing timestamp of record, when used.",
      "Update Timestamp Column - Name of datetime column for storing last update timestamp.",
      "Columns - Contains a set of repository columns that conforms output record.",
      "Shadow Columns - Contains a set of columns that are not present in the input data, but should be\n\t\t\t\tcreated in the output (so that they are then available for further use as any\n\t\t\t\tother \"real\" column read from a file).",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Extended Unification",
    "Step Details": "Detailed Description Some operations in [branding:product.name.abbreviation] are based on certain record groupings. These operations\n\t\t\tthen need to hold complete groups of records. This involves the need to preserve all historical\n\t\t\tdata to allow the collection of whole groups when only part of them comes to\n\t\t\tthe input. The storage for the historical data is called the Repository . This step retrieves historical records and forms groups \n\t\t\tof them from the repository, as well as storing them back. Moreover, this service manages\n\t\t\tcoordination between several process running in parallel and ensures logical integrity\n\t\t\tof record groups concurrently updated in the repository. Supported operations are: Unification, collecting groups of matching records Classification of groups Creating of representatives of groups Regrouping groups (adding next grouping level) General expression evaluating and assigning\n\t\t\t\t\t(not technically a group operation, but can be usable in cooperation with others)",
    "Step Properties": [
      "Id - Step identification string.",
      "Primary Key Column - Column containing a unique primary key of records.Not used whenProcessing ModeisNONINCREMENTALor(QUICK)IDENTIFY.",
      "Minimum Id To Assign - Minimal value for newly assigned group ids. This parameter is related to all unification operations, which\n\t\t\t\tmay assign new unique ids to candidate or  groups during processing. The real last assigned id is\n\t\t\t\talways stored in the repository and this value is only the lower limit for newly assigned ids.\n\t\t\t\tThe minimal value is 1.Default value: 1.",
      "Direct Update - Specifies that repository will be updated regardless to relationship of records. This improves performance but doesn't ensure\n\t\t\t\tconsistency of repository in case of crash. Suitable especially for initial full-load. Implies exclusive access to repository (exclusiveMode=true)Default value: False.",
      "Exclusive Mode - Specifies that only one batch can be processed at a time. Suitable for unification running as batch.\n\t\t\t\tWhen false, fully concurrent processing of (online) requests will be enabled.Default value: True.",
      "Processing Mode - Specifies processing mode.",
      "Data Source - Name of the data source for connection to the database repository.Mandatory whenProcessing Modeis notNONINCREMENTAL.",
      "Repository Name - Name of the repository determining the table names in the database.Mandatory whenProcessing Modeis notNONINCREMENTAL.",
      "Read Only - Specify that the repository will not be updated.Default value: False.",
      "Delete Flag Column - Boolean column or expression indicating whether the record must be removed from the repository.",
      "Timestamp Column - Datetime column containing an optional last record update time. The time comes from source external system.",
      "Processing Status Column - String column for storing a flag indicating if and how the record has changed in the repository.The flag can have the following values:A- input record added to the repositoryU- input record updated in the repositoryD- input record deleted from the repositoryR- reloaded record changed in the repositoryN- reloaded record which has not changedI- identified input record (when processionMode=IDENTIFY)X- input record which is older than its repository image and has been discardedY- input record which has an empty or duplicate primary key and has been discardedIf not specified, discarded older records which would obtain the flag \"X\", are not sent to the output.",
      "Processing Timestamp Column - Datetime column for storing the time stamp when the record was written to the repository. The time can be used to synchronize target external system.",
      "Operations - List of operations.",
      "Output Strategy - Detailed specification of manner how the records will be sent to output.",
      "Map All Columns - Specifies that all columns of input record format will be stored in the repository.\n\t\t\t\tPropertyRepository Columnsmust be empty when true.Default value: True.",
      "Repository Columns - List of columns stored in the repository. At least all columns needed for unification\n\t\t\t\tmust be present."
    ]
  },
  {
    "Step Name": "Extract Filter",
    "Step Details": "Detailed Description This step extracts records from the data flow. The output endpoint out_extract contains records satisfying the specified Condition .\n\t\tThe output endpoint out contains all records.",
    "Step Properties": [
      "Id - Step identification string.",
      "Condition - Condition that filters records for output \"out_extract\".\n\n\t\t\tFor a detailed description of the construction of expressions, please refer to the sectionexpressions."
    ]
  },
  {
    "Step Name": "File Provider Component",
    "Step Details": "Detailed Description Enables access to the files in a server directory through HTTP. If a file specified in the request exists, its content is sent in a response. The response depends on the file type:\n\t\t\t\n\t\t\t\tfor recognizable files (e.g., .txt or .xml ), the browser displays the file content as a text\n\t\t\t\tfor non-recognizable files (e.g., .profile ), the browser offers to download the file For example, if HTTP Dispatcher contains definition of the default listener on the port 8888, setting the File Provider Component Listeners to default and Location to files exposes the files at http://myserver:8888/files. To enable access to files located in the directory folder, set Files Directory to directory : then, a request to http://myserver:8888/files/subdirectory/file.txt returns the content of file.txt from the directory/subdirectory folder.",
    "Step Properties": [
      "Disabled - Specifies whether component should be disabled.",
      "Files Directory - Relative (to the server configuration file) or absolute path to the root directory of the exposed filesystem. You can access any file located in theFiles Directoryor its subfolders.",
      "Listeners - Comma-separated list of names of HTTP listeners where the service should be accessible.",
      "Location - Path to the location within the listener where the service will be accessible. The path has to start with slash, e.g.,/files."
    ]
  },
  {
    "Step Name": "FileReaderWrap",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Lookup Files",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "FileWriterWrap",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Filter",
    "Step Details": "Detailed Description Controls the processing of each data record and determines if the record can pass through\n\t\tin the data flow. Only data records that satisfy the given condition will be allowed to\n\t\tpass through. For a detailed description of the construction of expressions, please refer to the section expressions .",
    "Step Properties": [
      "Condition - Logical expression that controls record filtering.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Fixed Width File Reader",
    "Step Details": "Detailed Description A text file is expected as the input of this step, containing data presented line-by-line.\n\t\t\tEach line may contain several fields (columns) that are identified by their start\n\t\t\tposition and length. Values are not separated by any separator, the data start and stop\n\t\t\tpositions on the line are figured out from the current position (which depends on fields that\n\t\t\thave been read so far) and the column's attributes ( skip and width ). The step is capable of processing compressed files using either ZIP or GZIP format.\n\t\t\t\tWhen processing ZIP file, it searches for a file named after the archive itself (without the extension)\n\t\t\t\tor uses the only file present in the archive (regardless of its name). Similar to Text File Reader ,\n\t\t\tthis step supports all encodings\n\t\t\tsupported by Java, including Unicode formats (supported Unicodes are: UTF-8, UTF-16,\n\t\t\tUTF-16BE, UTF-16LE). Input data is processed the same way (via a Unicode aware reader), allowing\n\t\t\tcorrect processing of all files including those ones with a Byte Order Mark (BOM) signature.\n\t\t\tMore detailed information about supported encodings can be found in the Text File Reader step. The line structure is described by individual column definitions.\n\t\t\t\tEach column must define: How many characters should be skipped from the previous field stop\n\t\t\t\t\t\t(those characters are considered as the separator) - this value is stored in the parameter skip how many characters define the value (attribute width ) type of the data (attribute type ) and possibly more configuration data for parsing ( DataFormatParameters ) Values specified here are relative values from the end of the previous (last) field.\n\t\t\t\tThe step computes absolute values from the sequence of fields, therefore the fields must be\n\t\t\t\tdefined in an order matching the data record column order. Input data are processed utilizing parameters specified in the Data Format Parameters element. For a detailed description refer to DataFormatParameters . This step may produce the following errors: SHORT_LINE , INVALID_DATE , UNPARSABLE_FIELD , LONG_LINE , EXTRA_DATA , PROCESSING_ERROR . If a SHORT_LINE error occurs, the value given to further processing\n\t\t\t\tdepends on the SHORT_LINE instruction's action. If the action is NULL_VALUE then null is passed to further parsing. If\n\t\t\t\tthe action is READ_POSSIBLE then the returned value represents the value\n\t\t\t\tread from the field: it means either null if there were no data for the\n\t\t\t\tgiven field or a non-null value if data were present, but insufficient, in the\n\t\t\t\tfield. Error management is defined by the element Error Handling Strategy .\n\t\t\t\tThe error handling strategy allows the step to exclude incorrect records from processing, and such\n\t\t\t\trecords can be sent to the \"rejected\" output file. For more information about error handling\n\t\t\t\tconsult data parsing strategies . When creating a reject file, the following rules are observed: The initial name for the reject file is rejected.txt . The encoding defined for the input data file is used as the encoding of the rejected file. The line separator defined for the input data file is used as the line separator. Every input row is written to the reject file at most once. So,\n\t\t\t\t\teven if there are more error fields in the same row whose instructions\n\t\t\t\t\trequire writing to the reject file the row is written there only once. Empty reject files are not created. A reject file is created when\n\t\t\t\t\tan instruction requires writing to this file. The following example assumes that the input line consists of the following fields: DATE , DATETIME and STRING - configured as in the\n\t\t\t\texample below. Consider the following input data: 11.12.2001xxx12-10-2000 12:51+GMT0----|||***This is a test*** The processing ouput is (assuming that DATETIME uses the yyyy-MM-dd HH:mm:ss format, DATE uses yyyy-MM-dd formatting and the delimiter ';'): 2001-12-11;2000-10-12 12:51:00;This is a test",
    "Step Properties": [
      "Columns - Contains definitions of columns which will be read from the input file.",
      "Data Format Parameters - Contains data formatting information for the whole step. For details refer to theDataFormatParamaterssection.",
      "Encoding - Specifies encoding used in the input file.",
      "Error Handling Strategy - Definitions of error state management. Defines how to handle each error which\n\t\t\t\tcan occur during processing. For a detailed description of error strategies seeError Handling Strategies.",
      "File Name - Name of the input file to process.",
      "Compression - Defines compression type of the input file.Default value: NONE.",
      "Ignored Row Reg Ex - A regular expression that is compared to lines in the input file. If a line (read as a string) matches the\n\t\t\t\tregular expression, the line will be ignored.",
      "Number Of Lines In Header - Specifies the number of lines from the beginning of the file that will be excluded from\n\t\t\t\tprocessing (header lines, comments, etc).",
      "Number Of Lines In Footer - Specifies the number of lines from the end of the file that will be excluded from\n\t\t\t\tprocessing (footer lines).",
      "Line Max Read Length - Specifies the maximum number of characters per line to be read and processed.\n\t\t\t\tIf the length of the line exceeds this value, an error occurs.",
      "Line Separator - Specifies the string to be recognized as a line delimiter. The line delimiter\n\t\t\t\tcan be any sequence of letters, although a specific symbol is often used\n\t\t\t\twhich usually depends on the operating system where the file originated from.\n\t\t\t\tThe special symbols are:  \\r = CR, \\l = LF, \\n = LF.This parameter is not needed for files with a fixed line length.Escaped string property.",
      "Shadow Columns - Contains a set of columns that are not present in the input data, but should be\n\t\t\t\tcreated in the output (so that they are then available for further use as any\n\t\t\t\tother \"real\" column read from a file).",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Frequency Analysis",
    "Step Details": "Detailed Description Generates frequency analysis (a histogram), computes value counts\n\t\t\t\t\tin a given data set, either using all discrete values or using determined intervals.\n\t\t\t\t\tAll data types supported by [branding:product.name.abbreviation] can be used. This step can perform frequency analysis over multiple columns of input data in a single pass. This step has two outputs: statistical and data set. Statistical Output - output format of String\n\t\t\t\t\t[stat_name], String\n\t\t\t\t\t[stat_value/classification_label], String\n\t\t\t\t\t[record_count]. The output data set has the same number of columns and records as the input data set.",
    "Step Properties": [
      "Id - Step identification string.",
      "Analyses - The root node for analyses.",
      "Default Locale - Locale definition representing a specific geographical, political, or cultural region,\n\t\t\twith respect to data parsing and comparison as performed by the step."
    ]
  },
  {
    "Step Name": "GBAddressesBuildLookupData",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Address Identifier GB",
    "Step Details": "",
    "Step Properties": [
      "Id - Step identification string.",
      "In End Point - Input endpoint column mappings.",
      "Out End Point - Output endpoint column mappings.",
      "Data Dir - Directory with lookup files for English addresses. Must point to directory named 'data'\n                in the standard folder hierarchy (<project>/data/ext/lkp/gb_address_identifier)."
    ]
  },
  {
    "Step Name": "Generic Data Reader",
    "Step Details": "Detailed Description Reads a generic binary file. The format of the file has to be specified by the hierarchical configuration\n\t\t\tof this step - structures, fields, iterations etc. Each structured member (not a field) can\n\t\t\tbe drawn out as an endpoint, enabling access to all its fields as columns (except filler types). It can also\n\t\t\thave additional shadow columns that are capable of referencing the data of parent structures by specifying\n\t\t\ttheir name as prefix (for instance \"parent.column\"). This hierarchical streams concept is similar to that of a XML Reader step .",
    "Step Properties": [
      "File Name - Input file.",
      "Iterative - Controls if the input file is to be treated as a sequence of repeating data.\n\t\t\t\tDefault value is: false.",
      "Streams - Root of the hierarchical format structure.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "GetRuntimeProp",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Glossary",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Group Aggregator",
    "Step Details": "Detailed Description Produces output similar to using SELECT ... GROUP BY ... SQL statements.\n\t\t\tEach grouping is defined in the Aggregation Sets element which defines\n\t\t\tgrouping key components and a set of aggregating expressions.\n\t\t\tComponent values and aggregated results are collected into new output\n\t\t\trecords which are sent to the out_results endpoint.",
    "Step Properties": [
      "Id - Step identification string.",
      "Aggregation Sets - Set of definitions of aggregation.",
      "When - Condition for performing all aggregation calculations.\n\t\t\t\tInput record is ignored when evaluated asfalse.Default value =true."
    ]
  },
  {
    "Step Name": "Group Assigner",
    "Step Details": "Detailed Description",
    "Step Properties": [
      "Record Descriptor Column - Name of string input column in which special recordDescriptor is stored.\n\t\t\t\tThe descriptor contans group id, group size and record number in group as\n\t\t\t\tthree numbers separated by colon. For example4152:3:2.",
      "Assignments - List of assignments that will be performed on each group record.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Group Selector",
    "Step Details": "Detailed Description This step creates groups of records based on specified key definitions.\n\t\t\t\tThe step will select records from each group based on specified criteria.\n\t\t\t\tRecords can be selected multiple times based on different criteria. The resulting\n\t\t\t\trecords of selection criteria for each group may be dependent on sort order. For example when\n\t\t\t\tselecting the first N records, the sort order of the group determines which records will be selected. Selection/Grouping Methods: all Select all records of a group. min Select the first N ( count ) records of a group. max Select the last N ( count ) records of a group. median Select the middle N ( count ) records of a group. proportional Select N ( count ) uniformly distributed count records of a group. mincard Select all group records where each group has at least N ( count ) records. maxcard Select all group records where each group has at most N ( count ) records.",
    "Step Properties": [
      "Group Name - Field storing the group name of the grouping criteria/rules.",
      "Id - Step identification string.",
      "Selection Name - Field storing the name of the selection criteria/rules.",
      "Results - Default record selection criteria. These are applied for every group (defined bygroup rule).",
      "Default Locale - Locale represents a specific geographical, political, or cultural region, \n\t\t\twith respect to data parsing and comparison as performed by the step.",
      "Groups - List or rules governing group creation."
    ]
  },
  {
    "Step Name": "GroupSubGroupsAssigner",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Guess Name Surname",
    "Step Details": "Detailed Description The step identifies a first name and a last name from specified data input. This identification and parsing is\n\t\t\tdependent on dictionaries that contain a list of known first names and last names (see the properties). It is also possible to specify that in case diacritics (accents) within the found first name or last name are different from the source\n\t\t\tvalue, then the original diacritics are retained (preserved). This step uses a parser to examine the input string. For more details about\n\t\t\tit please see the description of Pattern Parser .\n\t\t\tBesides standard components there are the following predefined ones available.\n\t\t\tThese components are verified against corresponding dictionaries and can be configured by Word Definition , Multi Word Definition and Interlaced Word Definition properties. FIRST_NAME - syntactically {WORD}. LAST_NAME - syntactically {WORD}. FIRST_NAME_INTERLACED - syntactically {INTERLACED_WORD}. LAST_NAME_INTERLACED - syntactically {INTERLACED_WORD}. MULTI_FIRST_NAME - syntactically {MULTIWORD}, word separator defined in wordSeparators . MULTI_LAST_NAME - syntactically {MULTIWORD}, word separator defined in wordSeparators . Components MULTI_FIRST_NAME and MULTI_LAST_NAME consider following \n\t\t\tword separators (parameter wordSeparators): -'`\"~ . For a multiplicative version of the step see Multiplicative Guess Name Surname",
    "Step Properties": [
      "In - Column that contains both input first name and last name.",
      "First Name - Column that stores the final output first name (corresponding to the dictionary value).",
      "Last Name - Column that stores the final output last name (corresponding to the dictionary value).",
      "First Name Orig - Column that stores the original (input) first name.",
      "Last Name Orig - Column that stores the original (input) last name.",
      "Pattern Name - Column that stores the name of the applied parsing pattern.",
      "Hint Name - Column that stores the name of the applied parsing pattern hint.",
      "Trash - Column that stores the trash information. Trash information is the part\n\t\t\t\tof the input text which was not recognized\n\t\t\t\tas a known component or, when theFull Trash Scopeflag is true,\n\t\t\t\ttext parsed by component not having its own output columnStore Parsed Into.\n\t\t\t\tThe binding is not mandatory, thus the propertyFull Trash Scopeis effective only when this binding is set.",
      "Components - List of user defined components.",
      "Full Trash Scope - Specifies whether text parsed by components which don't define theStore Parsed Intoparameter is\n\t\t\t\tstored in the column defined by the bindingTrash. Text not parsed by any component is stored in any case.\n\t\t\t\tThis parameter has an effect only if the bindingTrashis defined.Default value:False.",
      "First Name Lookup File Name - Dictionary file that contains known first names. This dictionary\n\t\t\t\tcontainssingle-wordfirst names.\n\t\t\t\tFor more information about the dictionary seethe detailed description here.",
      "Hints - Contains a list of definitions of parsing hints. A parsing hint defines how to parse input data when more than\n\t\t\t\tone parsing pattern is found which can be applied to the input string. It assists the step in picking\n\t\t\t\tthe preferred parsing pattern in case of such ambiguity. In particular, hints are defined using propertyhint.",
      "Last Name Lookup File Name - Dictionary file that contains known last names. This dictionary\tcontainssingle-wordlast names.\n\t\t\t\tFor more information about the dictionary seethe detailed description here.",
      "Multi First Name Lookup File Name - Dictionary file that contains known first names. This dictionary\n\t\t\t\tcontainsmulti-wordfirst names (multi-word components work with this dictionary).\n\t\t\t\tFor more information about the dictionary seethe detailed description here.",
      "Multi Last Name Lookup File Name - Dictionary file that contains known last names. This dictionary\n\t\t\t\tcontainsmulti-wordlast names (multi-word components work with this dictionary).\n\t\t\t\tFor more information about the dictionary seethe detailed description here.",
      "Pattern Groups - Tag associating the group of patternspatternGroup.",
      "Preserve If Differs - Defines whether the original value of the input firstname, lastname or both \n\t\t\t\tshould be retained if the final standardized value \n\t\t\t\t(the selected dictionary value which conforms most closely\n\t\t\t\twith the original value) differs from the original value.\n\t\t\t\tThe word 'differs' here means that the values are different\n\t\t\t\twhen compared for equality ignoring case but are the same when\n\t\t\t\ttransformed by matching value generator defined by appropriate\n\t\t\t\tdictionary.Possible values:PRESERVE_FIRSTNAME,PRESERVE_LASTNAME,PRESERVE_BOTHandPRESERVE_NONEDefault value:PRESERVE_NONE.",
      "Word Definition - Default value: {WORD}",
      "Interlaced Word Definition - Default value: {INTERLACED_WORD}",
      "Multi Word Definition - Default value: {MULTIWORD:wordSeparators=\"-'`\"\"~\"}",
      "Tokenizer Config - Tokenizer Definition.",
      "Proposal Selection Strategy - Defines strategy to select from matched patterns.Possible values are:DISTANCE_BASED selects only those patterns whose distance calculated over all matched names is the losest.FIRST_MATCH selects only the first matched pattern regardless of others.Default value:DISTANCE_BASED.",
      "Scorer - Element which stores basic scoring settings.",
      "Id - Step identification string.",
      "When Condition - Expression that must be satisfied for this step to be executed."
    ]
  },
  {
    "Step Name": "HBaseDelete",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "HBaseGet",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "HBasePut",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "HBaseScan",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "HBaseScanWrap",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Hive Reader",
    "Step Details": "Detailed Description Reads data from a Hive table. In local mode, it is using HCatalog ReaderWriter interface. In MapReduce mode, it is using HCatalog InputOutput interface. In Spark mode, it is using Apache Spark SQL interface. Note Usage of the Hortonworks Hive 3 via Spark engine is supported. When using Hive version of Hadoop it is not allowed to use upper case in column name. When using Databricks version you can use both upper and lower cases in column names.",
    "Step Properties": [
      "Cluster - Name of the Hadoop Cluster with Hive service to be used.",
      "Database Name - Name of the source database (leave empty for default database).",
      "Table Name - Name of the table to be read.",
      "Filter - In Local and MapReduce mode, the filter condition can be used to read specific partitions via theHCatalog operations.\n                In Spark mode, it may contain SQL functions which will be used after \"where\" condition in SQL Query.",
      "Columns - Definition of HCatalog table columns (corresponding by order, names and types).Note:When usingHiveversion of Hadoop it is not allowed to use upper case in column name.When usingDatabricksversion you can use both upper and lower cases in column names.",
      "Shadow Columns - Definition of shadow columns (additional to those ones read from HCatalog).",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Hive Writer",
    "Step Details": "Detailed Description Writes the incoming data to a Hive table. The target table with appropriate schema must already exist before writing the data. In local mode, it is using HCatalog ReaderWriter interface. In MapReduce mode, it is using HCatalog InputOutput interface. In Spark mode, it is using Apache Spark SQL interface. Use button Create / Alter table... to create new or adapt an existing table for writing. After pushing this button you will be prompted to confirm the changes. If you are altering partitions of existing table the original data in this table will be deleted. Due to Hive partition concept the order of columns marked as partition is important. Note Usage of the Hortonworks Hive 3 via Spark engine is supported. When using Hive version of Hadoop it is not allowed to use upper case in column name. When using Databricks version you can use both upper and lower cases in column names. Known issues It is not possible to write into existing partitions repeatedly when running via MapReduce engine. This functionality is supported only in Local and Spark modes. Reported for local and MapReduce modes.\n\t\t\t\t\t\t\tHive throws NullPointerException in case if it encounters nulls in partition values.\n\t\t\t\t\t\t\tShould be fixed since Hive 1.2.2 and Hive 2+. HIVE-11470 When using partitioning, please make sure that the values in partition do not contain empty strings. Local run is affected by this bug HIVE-10809 . When running plan on local computer with Windows operating system, spaces characters in partition values are escaped whereas on other OS they aren't. It causes a problem when you want to write into one partitioned table repeatedly from two \t\tdifferent computers and one runs on Windows.",
    "Step Properties": [
      "Cluster - Name of the Hadoop Cluster with Hive service to be used.",
      "Database Name - Name of the target database (leave empty for default database).",
      "Table Name - Name of the table to be written to.",
      "Columns - Definition of HCatalog table columns (corresponding by order, names and types).Note:When usingHiveversion of Hadoop it is not allowed to use upper case in column name.When usingDatabricksversion you can use both upper and lower cases in column names.",
      "Batch Size - The size of one batch to be written at once. Each partition is counted separately.\n                This only applies to local mode - in MapReduce and Spark contexts,\n                the writing does not happen in batches.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Health State Logger",
    "Step Details": "Detailed Description Logs all sensor status changes to the standardly configured logger (i.e. to the configured file or stdout).",
    "Step Properties": [
      "Disabled - Specifies whether component should be disabled."
    ]
  },
  {
    "Step Name": "Health State Providers",
    "Step Details": "Detailed Description Adds several sensors that report health status of the server parts such as the path variables and database connections.",
    "Step Properties": [
      "Data Source Refresh Rate - Frequency (s) of the database connections health state refresh.",
      "Disabled - Specifies whether component should be disabled.",
      "Path Variable Refresh Rate - Frequency (s) of the path variables health state refresh."
    ]
  },
  {
    "Step Name": "Health State Web Console",
    "Step Details": "Detailed Description Adds Server Health Status section to the Admin Center to display all server's health sensor statuses.",
    "Step Properties": [
      "Disabled - Specifies whether component should be disabled."
    ]
  },
  {
    "Step Name": "Http Dispatcher",
    "Step Details": "Detailed Description Basic component that enables communication via HTTP protocol and responds to HTTP requests. It receives all HTTP requests, distributes them for processing to deployed services, and initiates request role resolution. The incoming HTTP request is analyzed, and if it contains an Authorization header, the username and password is extracted. Then the dispatcher contacts the Authentication Service Component on the server and resolves user roles assigned according to the provided username, password and IP address of the incoming request. Resolved roles are stored into the request context and may be used later in the Online Services Component to check if access to the service should be allowed or not. The component is required by many other components which register themselves into\n\t\t\t\tHttp Dispatcher on the specified URL path so that the HttpDispatcher can redirect handling of\n\t\t\t\trequests.",
    "Step Properties": [
      "Disabled - Specifies whether component should be disabled.",
      "Filters - Defines the actions that have to be done for each incoming request. One example of a filter is logging of requests and responses, another is encryption of requests. Filters are applied on the incoming HTTP requests before they invoke the service itself.Every filter has its own parameters in theMappingssubsection with subelements that define conditions when the filter will be activated, such as which listener is used to accept the request, url prefix of the request and set of tests that must all succeed. If theMappingsare not specified, the filter will be mapped to all listeners and all requests.If the request conforms to several patterns, the filters will be applied in the order of appearance in the filters section.",
      "Listeners - Defines listener threads that receive requests on the specified TCP ports and specifies if the listener should communicate via SSL or not. It is possible to define several listeners, each of them accepting requests on a different TCP/IP port.",
      "Servlet Filters - Filters which can be used to filter traffic, requests and responses to and from Java servlets.",
      "Worker Queues - Defines dedicated thread pools to some services or requests. TheWorker QueueshasMappingssubsection with subelements that define conditions when the worker queue is activated. All requests that meet the URL pattern and listener defined in theMappingssection are handled by the queue defined. If theMappingsare not specified, the Worker Queue will be mapped to all listeners and all requests."
    ]
  },
  {
    "Step Name": "Imputer",
    "Step Details": "Detailed Description Imputing servers for filling in missing data. These missing data are computed based on the type of imputer selcted.\n                This step loads imputers from model located in file specified by field \"model file\".\n                After loading the model, it applies the specified features on selected columns.",
    "Step Properties": [
      "Feature Configs - Configuration of features used to impute the data based on the model file loaded.",
      "Model File - Name of file with trained model that will be used for imputing.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Imputer Trainer",
    "Step Details": "Detailed Description Imputing servers for filling in missing data. These missing data are computed based on the type of imputer selcted.\n                Based on specified features, this steps trains imputing model and saves the trained model to the specified file.\n                Each feature can have different type of imputing defined. Multiple imputers can be defined for single feature.",
    "Step Properties": [
      "Feature Configs - Configuration of features used to train the model and perform imputing on training input data.",
      "Output Model File - Name of the model output file.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Incremental Manual Override Builder",
    "Step Details": "Detailed Description This step manages a set of rules for the purpose of manual overriding against the existing rules within the repository.\n\t\t\tThis step differs from Manual Override Builder by operating\n\t\t\ton a single rule level and therefore facilitating add/change/delete operations of a single rule.",
    "Step Properties": [
      "Id - Step identification string.",
      "Type Column - String column that contains and eventually stores the rule type. See themanual overridesection.",
      "Parent Record Pk Column - String column that contains and eventually stores the primary key of the parent record (not applicable to R->C rule).",
      "Child Record Pk Column - String column that contains and eventually stores the primary key of the child record.",
      "Delete Column - Boolean column or expression that specifies whether a given rule should be deleted.",
      "Repository - Repository properties."
    ]
  },
  {
    "Step Name": "Product Help",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Integration Input",
    "Step Details": "Detailed Description This step can be used instead of other, more specialized, input steps, such as Text File Reader ,\n\t\t\tto create a Plan file that is usable as either a component or an online service . An input step in a Plan file in a component step\n\t\t\tcreates an endpoint named by the input step. Then data inserted into the endpoint are sent to the output endpoint of this input step.\n\t\t\tA similar situation is when a Plan file with an input step is used as an online service . In this case the input steps can be used as \n\t\t\treceivers of data from the online service and the received data are sent to the output endpoint of this input step. The \"opposite\" step is Integration Output .",
    "Step Properties": [
      "Id - Step identification string.",
      "Columns - These columns define the format of data being sent into this step either from a component step or from an online service.",
      "Shadow Columns - These columns, together with the columns property, define the record format of the output endpoint. The shadow\n\t\t\t\tcolumns are not visible outside of the Plan file where the input step is used."
    ]
  },
  {
    "Step Name": "InputStepWrap",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Installation and system requirements",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Introduction",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Introduction to writing functions in Java",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Introduction to writing steps in Java",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Issue Importer",
    "Step Details": "Detailed Description This step processes the data flow containing the DQ issues and transfers the issues to the DQIT data storage.\n\t  This can be either an XML file storage or a database storage, depending on the DQIT configuration.\n\t  The data flow coming into the step should contain all the data columns defined in the selected Issue Type metadata and the necessary technical “issue_” columns.\n\t  For the list of all “issue_” columns for each Issue Type, see the description of the individual Issue Types in the DQIT Administration Guide | Issue Types and Classes . When issues are imported in the Upsert mode, the issues currently present  in DQIT (if there are any) are left intact, and all the issues in the load are added to the issue database.\n\t  If the Full mode is used, all the issues currently present in the DQIT and not present in the load are closed. Only the issues that are present in the load will be open. When\n\t  issues are imported in the Close mode, matched issues are closed.",
    "Step Properties": [
      "Server - Name of the server that runs the DQIT web application.",
      "Assignee - DQIT user to whom the imported issues are assigned.",
      "Comment - Additional information describing import action.",
      "Id - Step name.",
      "Import Mode - Import mode:Full,UpsertorClose.",
      "Issue Type - Name of the issue type, with the following naming convention:[entity_name]_[name of the issue type].\n          The name of the issue should be completely in lowercase.",
      "Connection Timeout - Connection timeout in milliseconds."
    ]
  },
  {
    "Step Name": "Issue Reader",
    "Step Details": "Detailed Description This step reads the current issues from DQIT Web Application.\n\t  Issues can be specified by four optional filters: Status , Entity , System and Assignee fields.",
    "Step Properties": [
      "Server - Name of the server that runs the DQIT web application.",
      "Assignee - DQIT user to whom the imported issues are assigned.",
      "Entity - Entity (named according to metadata configuration) to which the issue data belongs.",
      "Id - Step identification string.",
      "Issue Columns - A user-specified set of columns to be used for further processing after theIssue Readerstep",
      "Issue Type - Name of the issue type, with the following naming convention:[entity_name]_[name of the issue type].\n          \t\t\t\t\t\t\t\t\t\t\t\t\t\t\tThe name of the issue should be completely in lowercase.",
      "Shadow Columns - Additional columns, which are not present in the input data\n           but which can be specified by the user to be used in the steps followingIssue Reader.",
      "Status - Value of theissue_statusfield, which stores the current workflow status of an issue",
      "System - System in which the issue data originates.",
      "Connection Timeout - Connection timeout in milliseconds."
    ]
  },
  {
    "Step Name": "Jdbc Reader",
    "Step Details": "Detailed Description This step reads data records from the results of a specified query executed on a specified JDBC database table. Usage notes Placeholders can be inserted in any part of the SQL query and are replaced before the query executed. This allows you to define custom schema or table names, etc. A warning is shown if a placeholder is provided in the query but not defined in the table. Specifying two placeholders with the same name results in an error.",
    "Step Properties": [
      "Id - Step identification string.",
      "After Script - SQL instructions representing the script to be executed in the database after reading is done.",
      "Before Script - SQL instructions representing the script to be executed in the database before reading.",
      "Columns - Contains definitions of columns which will be constructed from the JDBC query result set.",
      "Data Source Name - Name of the DataSource. DataSource groups together information regarding access to the database, such as: URL, driver name, username and password. See theData Source descriptionfor more details.",
      "Query File Encoding - Encoding of the query file if such a file is used.",
      "Query File Name - When defined, it represents the name of the file that contains the query to execute. When this attribute is filled in, the attributeQuery Stringmust not contain any value, otherwise a query conflict error is reported.",
      "Query String - SQL query to execute on the data source to obtain input data records. Only selection operations are supported.",
      "Shadow Columns - Contains a set of columns that are not present in the input data, but should be created in the output (so that they are then available for further use as any other \"real\" column read from a file).",
      "Placeholder Begin Mark - The string marking the beginning of column placeholder.",
      "Placeholder End Mark - The string marking the end of column placeholder."
    ]
  },
  {
    "Step Name": "Jdbc Writer",
    "Step Details": "Detailed Description The standard output for this step are data records written to a database table.\n\t\t\tSaving to the database is done utilizing the SQL INSERT command. All data is written in\n\t\t\tbatches to speed up the process. The following databases have been tested with the step:\n\t\t\tOracle, Sybase, MS SQL, DB2, Excel file (via ODBC), mySQL and postgreSQL. For better error management, the parameter Error Handler handles\n\t\t\t\terror situations when writing to the DB. If this property is defined then the\n\t\t\t\tstep has one mandatory output, err_out . Otherwise it has no\n\t\t\t\toutput. Important notes: It is strongly recommended not to use the JDBC-ODBC driver, due to several problems: ODBC writes are always slower (sometimes dramatically) the driver implements only the JDBC 2.0 specification,\n\t\t\t\t\t  \twhich is missing some important functionality for complex exception handling (so\n\t\t\t\t\t  \tyou may experience problems when writing to the DB using the JDBC-ODBC driver\n\t\t\t\t\t  \tunder some circumstances, e.g., when a DB writing error occurs). the connection may not support Unicode encodings such as UTF-8 in dependency on the data source it may be impossible to set autocommit=off ,\n\t\t\t\t\t\twhich, again, causes problems with exception handling Therefore, it is recommended to use direct JDBC drivers for the specified database\n\t\t\t\t\t  system. When the Error Handler property is defined, the step tries to handle\n\t\t\t\t\t  writing problems with the database. When it is not possible to handle them (for\n\t\t\t\t\t  example, when the driver used does not support all required functionality) it falls\n\t\t\t\t\t  back to processing as without Error Handler defined. This usually\n\t\t\t\t\t  ends up with the exception in case of writing problems, while data written to\n\t\t\t\t\t  the database depends on the error strategy used. Consult the Error Handler property description for more detail on this. In order to fully support exception handling, the driver is required to\n\t\t\t\t\t  implement JDBC 3.0 (or at least the savepoints specification section).\n\t\t\t\t\t  Only the JDBC-ODBC driver does not meet this requirement for all tested drivers. The ROLLBACK and ROLLBACK_AND_STOP strategies should be used only with Commit Size set\tto 0 to ensure data integrity. In that case are all\n\t\t\t\t\tdata are rolled back (since all data comes in a single commit) when an error occurs or\n\t\t\t\t\tall data are written to the database when no error has occurred. The autocommit property from previous versions was removed.\n\t\t\t\t\tThe step now tests the connection on its own and sets autocommit=on only if it is not possible to use the autocommit=off option, which is much more suitable for exception\n\t\t\t\t\thandling. For now, the only case when autocommit is set to on is\n\t\t\t\t\tan ODBC connection to a simple file-based source such as an Excel file. If there are many invalid entries in the input and Error Handler is defined, writing may take significantly more time (although it is heavily dependent on the database used).\n\t\t\t\t\tWhen writing data without errors to the DB, there should not be a significant time difference between errorHandler-defined and\n       \t\t\t\terrorHandler-disabled configurations.",
    "Step Properties": [
      "After Script - SQL instructions representing the script to be executed in the database after writing is done.",
      "Batch Size - Number of elements to be used in the batch. The minimum size for a batch is 0 (no batching).\n\t\t\t\tThe preferred maximum batch size is currently 10000 rows. When the commit size is smaller\n\t\t\t\tthanBatch SizethenBatch Sizeis set to the value ofCommit Size.",
      "Before Script - SQL instructions representing the script to be executed in the database before writing.",
      "Columns - A list of columns to be written to the database. Note that column names should be defined\n\t\t\t\tin an \"unquoted\" form, even if they contain special characters or represents SQL reserved word.\n\t\t\t\tContrary to theTable Nameproperty, the column names are quoted automatically\n\t\t\t\tas needed.",
      "Commit Size - Gives the number of elements after which the commit should be performed. The currently opened\n\t\t\t\tbatch is executed and the whole current transaction is committed. The commit size is\n\t\t\t\tnot limited. When set to 0, the whole writing process will be sent as single-commit\n\t\t\t\ttransaction.",
      "Data Source Name - Name of the DataSource. The DataSource groups together information regarding\n\t\t\t\taccess to the database, such as: URL, driver name, username and password.\n\t\t\t\tSee theData Source descriptionfor more details.",
      "Error Handler - Error handler which defines the behavior of the step in case of writing problems.\n\t\t\t   When this property is set the step has one mandatory output,out_err,\n\t\t\t   where the invalid rows are sent to. The format of this output is the same as that of the input with one\n\t\t\t   additional textual column named as specified by the propertyerrorFieldName, where\n\t\t\t   the cause of problem as reported by the database are written.",
      "Table Name - Name of the table to write data to. This table must exist in the given database and must\n\t\t\t\tbe writable for the given user.Notethat it is necessary to enclose individual table name parts with quotes when:- given part contains special character(s) (according to the SQL specification)- given part matches SQL reserved word (according to the SQL specification)It's because table name can represent structured value following the catalog.schema.table\n\t\t\t\tpattern and therefore it's on the user to specify and quote individual parts when needed.Example:having catalog 'test:catalog' with schema 'User' and table 'table.1', the correct\n\t\t\t\tvalue is\"test:catalog\".\"User\".\"table.1\"(because both 'table.1' and 'test:catalog'\n\t\t\t\tcontains special characters and 'User' is an SQL reserved word).Warning:quoted parts should exactly represent DB object name since many DB's are case sensitive\n\t\t\t\twhen dealing with quoted names.",
      "Quote All Identifiers - If selected, all column names are quoted.\n\t\t\t\tThis means they are treated as case-sensitive if the database supports this option.  \n\t\t\t\tThe typical use case is when a table is created with quoted column names.",
      "Write All Columns - Specifies whether to write out all columns as defined in the input format. This\n\t\t\t\tattribute is exclusive to column definitions. If this value is set to on, then there\n\t\t\t\tmust be no columns defined in the columns element, otherwise an error is reported.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Jms Provider Component",
    "Step Details": "Detailed Description Selects active connections from the JMS configuration pool and configures JMS loggers.",
    "Step Properties": [
      "Connection Pool Size - Sets the number of threads that may concurrently process JMS requests received from all JMS connections defined.The level of parallelization may be decreased by the settings of the service that consumes messages from JMS queues/topics (e.g., minPoolSize/maxPoolSize settings in the.onlinefiles). If theConnection Pool Sizeis missing, the default value of 5 is used. The max size should be set reasonably with respect to the particular HW configuration and solution load. Keep in mind that a higher number might not yield higher throughput. Please do not use a number higher than 1000.",
      "Disabled - Specifies whether component should be disabled.",
      "Filters - Filter definitions and their mapping to connections and input or output destinations. The following filters are available:Jms Logging Filter - logs content of the request/response to the fileJms Response Time Logger - logs the time of the request processing to the logger",
      "Jms Resources - Contains references tojmsConnectionnodes\n\t\t\t\tdefined in runtime configuration file. Only these connections are available to the Ataccama Server."
    ]
  },
  {
    "Step Name": "Jms Writer",
    "Step Details": "Detailed Description Format of the message is configured by an expression template. Supports JMS message properties. Optional output passes processed records. Scorer may be configured to mark possible message transfer errors. It is necessary to copy proper JMS MQ implementation JAR to DQC classpath, e.g. [DQC_HOME]\\runtime\\lib\\",
    "Step Properties": [
      "Jms Resource - Reference to JMS Server Resource inruntime configuration.JMS properties specific for Message Broker are specified there, e.g. Connection Factory class name,\n\t\t\tJNDIContextFactoryInitial, JNDIProviderURL.",
      "Input Template - Template defining structure of body of message.",
      "Text Message - Determines whether the JMS message type is TextMessage or BytesMessage. By default it is enabled and TextMessage is used. Disable to use BytesMessage.",
      "Message Encoding - Encoding used for the message.",
      "Message Properties - Properties of the JMS message.",
      "Request Destination - JMS destination which will receive messages.",
      "Request Destination Type - Type of the destination. Possible values:QUEUE/TOPIC.",
      "Response Destination - Value of \"ReplyTo\" JMS header. Defines destination for responses\n\t\t\t\tto this particular message. This information only tentative,\n\t\t\t\tthe receiving counterparty may ignore it completely.",
      "Scorer - Standard DQC scorer.",
      "Template Begin Mark - Symbol marking the beginning of column placeholder.",
      "Template End Mark - Symbol marking the end of column placeholder.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Join",
    "Step Details": "Detailed Description This step joins two separate input data flows into a single output data flow based on corresponding\n\t\t\t\tinput data flow keys.\n\n\t\t\t\tThe data entry points (endpoints) are named respectively in_left and in_right .\n\n\t\t\t\tAll data input flows ( in_left and in_right ) must contain columns with key values (primary keys),\n\t\t\t\twhich are defined as column properties leftKey and rightKey .\n\n\t\t\t\tIt is the keys that govern how the data records are paired up across the data input flows.\n\t\t\t\tAll basic data types can be used as a key.\n\n\t\t\t\tThe join operation can be defined as a standard SQL join operation with join types such as: inner , outer , left ,\n\t\t\t\tand right join.\n\n\t\t\t\tThe join type is specified by the property joinType , which must contain one of the supported join types. If the data input flow contains multiple records with the same key values, the data output flow\n\t\t\t\twill contain a Cartesian Product of data input records. Records having a null key value are not joined with any other (null keyed) records. Those records are processed as\n\t\t\t\tunpaired, depending on joinType . The content of the output data is the result of expressions operated on the conjuncted input data.\n\t\t\t\tIn expressions, references to the particular columns must be made by dot notation ( in_left.* , in_right.* ).\n\n\t\t\t\tThe step first groups input records by key values and computes the Cartesian Product for all records for each group.\n\t\t\t\tThen, the expression sub-properties defined in the columnDefinition properties are evaluated\n\t\t\t\tto determine the result records of the Cartesian Product.\n\n\t\t\t\tResults of these operations are stored in the column defined by the sub-property name of the property columnDefinition .\n\n\t\t\t\tIn case of the join operations left , right or outer , for which a record from an input flow\n\t\t\t\tmight be empty, input column values of such empty records are transformed to null values before expression evaluation. NOTE: • Only data that is defined in the sub-properties expressions of the columnDefinition properties are written\n\t\t\t\tto the output. If the property columnDefinitions is empty or missing, then the output remains empty.\n\t\t\t\t(Though the previous versions of the step generated a default set of columnDefinitions , the current version requires explicit definition of all of the columnDefinitions .) • Not only string type is supported for values in Left Key and Right Key .  Alternatively you can use any data type based on the input column or apply an expression transformation.",
    "Step Properties": [
      "Id - Step identification string.",
      "Left Key - Column that contains key values of the left (first) input flow. The keys are used in conjunction. Alternatively you can useexpressionsin order to put condition on joining key values.",
      "Right Key - Column that contains key values of the right (second) input flow. The keys are used in conjunction. Alternatively you can useexpressionsin order to put condition on joining key values.",
      "Column Definitions - Set ofcolumnDefinitionproperties which define operations executed on the conjuncted input data, also the output data.",
      "Join Type - Specifies which type of conjunction to perform on the input data.",
      "Match Strategy - Specifies technique used for records pairing."
    ]
  },
  {
    "Step Name": "JoinRec",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Json Call",
    "Step Details": "Detailed Description The step calls online service installed on the Url . For POST and PUT methods, the request body is built using Input Template . In both cases, nested DQC expressions (marked with begin and end\n\t\t\tstrings specified) are evaluated\n\t\t\tfor each record. The response is\n\t\t\texpected to be in JSON format and\n\t\t\ttherefore the values can be\n\t\t\textracted\n\t\t\tusing JSON Reader approach. For details on JSON Reader refer\n\t\t\tto the documentation .\n\t\t\tBesides the reader's endpoints, the endpoint \"out\" is available to\n\t\t\tcopy\n\t\t\tinput stream of data, allowing scoring and additional processing. Support for Hadoop Kerberos-based Authorization You can connect to server with Kerberos authorization using jsonCall.kerberos.principal and jsonCall.kerberos.keytab Java arguments.\n\t\t\t\t\tWhen arguments mentioned above are applied authorization of the server that Json Call Step links to will be ignored. Kerberos will be used. Support for Hadoop Knox SSO Authorization When you are using Json Call step in ONE IDE accessing cluster that is secured by Knox SSO authentication you need to define the following JVM argument in Java. -DjsonCall.knox.sso.url=https://<knox_host>:8443/gateway/knoxsso/api/v1/websso Support for Mutual TLS (mTLS) Authentication To enable using mutual TLS authentication, where client certificates are verified in addition to server certificates, the following Java arguments need to be provided: javax.net.ssl.keyStore : Points to the keystore containing the client certificate and the corresponding private key. This can also be a .p12 file. javax.net.ssl.keyStorePassword : The password for the keystore. Troubleshooting Execution of JSON Call Step When JSON Call step returns no response but the debug request files is created correctly, check SSL settings whether both Root and Sub CA certificates are added. You can add those using the Keytool in Command Line. Add the following lines to Run Configurations | Runtimes | VM Arguments to: Check whether this issue is related to certificates, add [-Dcom.sun.net|http://dcom.sun.net/]{{.ssl.checkRevocation=false}} Debug SSL settings, add [-Djavax.net|http://djavax.net/].debug=ssl,handshake,trustmanager{{}}[~accountid:nnnnn:accountid]",
    "Step Properties": [
      "Url - The target URL\n\t\t\t\tof the service to be called. Is a DQC template - can use nested DQC\n\t\t\t\texpressions that will be\n\t\t\t\tevaluated for each record.\n\t\t\t\tIf URL resource is specified, this URL will be appended to URL defined\n\t\t\t\tin URL resource.",
      "Url Resource - Name of the URL resource to be used.",
      "Method - The HTTP method to be used.",
      "Input Template - The JSON\n\t\t\t\tmessage that will be sent for POST, PUT, PATCH and DELETE methods (no message is\n\t\t\t\tsent for GET method). Is a DQC template - can use nested DQC\n\t\t\t\texpressions that will be\n\t\t\t\tevaluated for each record.",
      "Encoding - The encoding in\n\t\t\t\twhich the JSON message will be sent.",
      "Headers - Additional HTTP headers that will be set for the request.",
      "Timeout (ms) - If the call of the service takes longer time than the Timeout (ms) then the call is aborted.",
      "Data Format Parameters - Data formats for formatting nested DQC expressions to strings. For\n\t\t\t\tdetails refer to theDataFormatParameterssection.",
      "Url Begin Mark - The string that\n\t\t\t\tmarks begin of nested DQC expression in URL template.",
      "Url End Mark - The string that\n\t\t\t\tmarks end of nested DQC expression in URL template.",
      "Template Begin Mark - The string that\n\t\t\t\tmarks begin of nested DQC expression in input template.",
      "Template End Mark - The string that\n\t\t\t\tmarks end of nested DQC expression in input template.",
      "Response Debug File - The responses can be written into the file. If the file name contains a percent character\n\t\t\t\tthen it is replaced by the number of the record for which the response was generated.\n\t\t\t\tIf a file with the same name exists then it is rewritten.",
      "Request Debug File - The same asResponse Debug Filebut for the request.",
      "Reader - The reader used for parsing the JSON responses. For details refer to\n\t\t\t\ttheJSON Readersection.",
      "Delay Between Requests (ms) - Sometimes when calling an outside service it is needed to make some pauses\n\t\t\t\tbetween requests in order not to get banned for DOS attack. The value is in milliseconds.",
      "Response Code Column - Column where the HTTP status code of the response should be stored.",
      "Scorer - Element which stores basic scoring settings.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Json Output Profiling",
    "Step Details": "Detailed Description This profiling step is used in statistical analysis of data.\n\t\t\t\t\tFor each data column this step will compute\n\t\t\t\t\tstatistics (values) such as minimum, maximum, standard and error values. The profiling step is capable of multiple analytical operations\n\t\t\t\t\tin a single pass over multiple columns of input data. All date types supported by [branding:product.name.abbreviation] can be specified as long as they correspond\n\t\t\t\t\tto the applied date operations. INTEGER, LONG DAY, DATETIME BOOLEAN STRING Data Count yes yes yes yes Null Count yes yes yes yes Not Null Count yes yes yes yes Different Value Count yes yes yes yes Unique Value Count yes yes yes yes Sum yes - yes - Variance Definition .\n\t\t\t\t\t\t\t\tResult for DAY/DATETIME values are in squared days. yes yes - - Standard Deviation Definition .\n\t\t\t\t\t\t\t\tResult for DAY/DATETIME values are in days. yes yes - - Average yes yes yes - Median yes yes yes yes Quantile yes yes yes yes Maximum yes yes yes yes Minimum yes yes yes yes First X Values yes yes yes yes Last X Values yes yes yes yes",
    "Step Properties": [
      "Id - Step identification string.",
      "Inputs - List of input sources on which the profiling is performed. Each element needs an \n\t\t\tappropriate input endpoint.",
      "Fk Analysis - List of definitions of foreign key analyses.",
      "Default Locale - Locale represents a specific geographical, political, or cultural region, with respect\n\t\t\tto data parsing and comparison as performed by the step.Default value:en_US",
      "Output File - Filename of the result file where the profile results will be stored.",
      "Export File - XML or JSON file where the profile results will be exported.",
      "Output Limit - Specifies the maximum number of frequency records being stored into the output.\n\t\t\tThe parameter is applied on both most frequent values\n\t\t\tand least frequent values.\n\t\t\tThe amount of data is thus at most twice as this value.\n\t\t\tThe value is applicable only to results of frequency and group size analysis\n\t\t\tand is applied to all profiled data separately.\n\t\t\tIf the value is set to zero, number of stored records is unlimited.Default value:1000",
      "Data Source - Data source name of database for storing drill-through data.\n\t\t\tRequired when at least oneinputhas specifiedDrill-through.",
      "Table Name Prefix - Prefix for names of database tables to which drill-through data will be stored.\n\t\t\tRequired when at least oneinputhas specifiedDrill-through.",
      "Masks - List of mask definitions.",
      "Domains - Specifies which and how domain analysis will be performed.",
      "User Metadata - Additional metadata appended to result file.",
      "Threaded - Specifies whether the step will perform calculations in single thread (one after one)\n\t\t\tor in multiple-threads (parallel processing).",
      "Export To Endpoint - If checked, the step exports its output in JSON format to an endpoint calledout."
    ]
  },
  {
    "Step Name": "Json Parser",
    "Step Details": "Detailed Description This step is similar to the JSON Reader step .\n\t\tIt can create several output streams from input stream containing JSON strings. Rows of one output stream can be\n\t\tlogically children to rows from another stream of rows. For example let's assume input JSON contains set\n\t\tof clients and each client has several addresses. Such record can be read as a stream of clients\n\t\tand another stream of addresses, where each address can have an id taken from the client stream (for\n\t\tfurther identification of the corresponding client). Each stream can also use shadow columns and referencing\n\t\tvalues from other columns of the input stream for further identification of particular records.\n\n\t\tThe input stream passes through to the default output \"out\", with the possibility to specify a column Parsing Error Column that will\n\t\tbe filled with explanation in case of an error parsing the associated JSON.",
    "Step Properties": [
      "Input Column - The input column containing JSON strings to be parsed.",
      "Parsing Error Column - Column for detecting JSON parsing errors. This column is available in the default output \"out\".",
      "Data Streams - Output end points defined on the root level.",
      "Data Format Parameters - Data formats for parsing string JSON fields into DQC data types. For details refer to theDataFormatParameterssection.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Json Reader",
    "Step Details": "Detailed Description",
    "Step Properties": [
      "File Name - The input file.",
      "Encoding - The encoding of the input file.",
      "Data Streams - Output end points defined on the root level.",
      "Data Format Parameters - Data formats for parsing string JSON fields into DQC data types. For details refer to theDataFormatParameterssection.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Json Writer",
    "Step Details": "Detailed Description This step creates output JSON file according to the templates. Without any advanced configuration,\n\t\t\tthe template is simply written to the output file. For filling it dynamically with some data, we will\n\t\t\tneed objects. An object is a stream of records, and the columns of the records are attributes\n\t\t\tof this object. When a new object is added, a new input of the same name is automatically created for reading\n\t\t\tthese records. When we do have an object and we connect a data stream to its input, we can use it in the template.\n\t\t\tThis is simply done by including an property surrounded by \"<\" and \">\" with the same name as the object. That causes this property to be\n\t\t\tentirely replaced by the JSON representation of the object - actually, by array (one object = one record read from input) or single JSON object read from the input. The JSON representation of the object is done in very similar way - by its own JSON template. But here, we are inside the object,\n\t\t\tand we can use its attributes - columns of the original records - as text values. Attribute is\n\t\t\treferenced by its name in surrounded by \"%\" (e.g. \"%value%\"). It is also possible to reference subobject's properties. Syntax is the same as usual property with substream prefix (e.g. \"%stream.property%\"). Sometimes unquoted value of attribute is needed, adding extra equals sign at the beginning and end will discard quotes (e.g. \"%=value=%\" or \"%=stream.property=%\").\n\t\t\tIn that case formatting rules do not apply as valid JSON must be generated. JSON template of objects is a standalone JSON and that's why it is validated as a standard JSON structure. Any object can have subobjects - more complex attributes that are read from their own input streams and are joined with parent\n\t\t\tobject exactly the same way as in inner join of tables. The attributes/columns for join are specified in the inner objects, in\n\t\t\tthe parameter Column Binding . Inner objects are objects as well and can also have subobjects, etc. This step supports encodings supported by Java, including Unicode formats. Available encodings are listed in step configuration.",
    "Step Properties": [
      "File Name - Name of the output JSON file.",
      "Encoding - File data encoding. The possible encodings are all encodings supported by the\n\t\t\t\ttarget Java platform. Some commonly used encodings are: ISO-8859-1,\n\t\t\t\tISO-8859-2, and UTF-8.",
      "Root Object - Root object for the final JSON structure. Each record from input stream will create single root object in the final JSON file.",
      "Indenting - When this is set, the output JSON will be in human readable form, indented multi-line. Uncheck this for smaller size of the file.",
      "Include Null - When this is set, the output JSON will include properties even with null values. When unchecked, properties with null values will be ignored in the output.",
      "Maximum Records In Memory - For each inner object, all its records are stored in the memory until this limit is reached; after that, they are stored\n\t\t\t\tin an external file, which is slower.",
      "Data Format Parameters - Contains data formatting information for the whole step, unless there is another such attribute specified at certain\n\t\t\t\tobject stream, in which case the latter will be used for that object stream and all its substreams. For details refer to theDataFormatParameterssection.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "JSON services",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Kafka Provider Component",
    "Step Details": "Detailed Description Kafka provider component enables using Kafka as a source and target for online service components.",
    "Step Properties": [
      "Disabled - Specifies whether component should be disabled.",
      "Kafka Resources - References to the defined Kafka server connections."
    ]
  },
  {
    "Step Name": "Kafka Reader",
    "Step Details": "Detailed Description Kafka Reader step consumes messages from the specified Kafka topics. Consumer is identified by a Group Id property. Distributed consuming is possible using the same Group Id . By default, Kafka Reader has enable.auto.commit property set to true and commits read messages over time. \n\t\t\t\tAfter it has finished reading the messages, Kafka Reader step ensures that all the messages were committed.\n\t\t\t\tTo guarantee \"At Least Once\" semantics it is possible to use DQC Transactional Model.\n\t\t\t\tIn this case commit of the read messages will be done only after the whole processing has finished. \n\t\t\t\tTo enable DQC Transactional Model you will need to send the following Java property while starting plans containing Kafka Reader steps: -Dtransaction.coordinator=explicit",
    "Step Properties": [
      "Server - The name of the\tsource server which runs Kafka.",
      "Topics - The name of the Kafka topics (source of the messages), separated by comma.",
      "Group Id - ID of the Kafka consumer group.",
      "Timeout - The time, in milliseconds, spent waiting for records of the topic. It should not be too small, for instance at least 100 ms.\n\t\t\t\tMust not be negative. If 0, returns immediately with any records that are available currently in the buffer, else returns empty.",
      "Poll Once - This option causes reading records of the topics only within one poll request. It could be used if user does not want to wait for new messages.\n\t\t\t\tIf there are no new records with regard of Group Id it waits for them until timeout.\n\t\t\t\tIt is independent on timeout.",
      "Read From Beginning - Read all available records of the topic. Before the read, seek to the first offset for all of the currently assigned partitions. Could be used for development and debugging purposes to guarantee consistent reads.",
      "Encoding - Encoding of characters in the topic. You can use any encoding supported by the target Java platform. Some commonly used encoding sets are ISO-8859-1, ISO-8859-2 and UTF-8.",
      "Input Format - Input format of the consuming message.",
      "Properties - Kafka properties for consumer. List of all possible properties ishere.",
      "Properties Encrypted - Kafka encrypted properties for consumer.  Such as (ssl.key.password,ssl.keystore.password,ssl.truststore.password, etc).",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Kafka Writer",
    "Step Details": "Detailed Description Kafka Writer step publishes messages to a specified topic in Kafka. Format of the message is described by the expression template. If the topic does not exist yet, a new one is created with 1 partition and replication factor 1. Alternatively, it is possible to create a topic with multiple partitions or multiple replicas using the \"Create topic\" button. User can define a key for routing a message to the partitions. This key ( Partition Key ) is an expression. Partition is decided based on the hash of the\tcompiled key.",
    "Step Properties": [
      "Server - The name of the source server which runs Kafka.",
      "Topic - The name of the Kafka topic where the message should be published.",
      "Partition Key - Expression for key which is used for partitioning.",
      "Encoding - Encoding of characters sent to the topic. You can use any encoding supported by the target Java platform. Some commonly used encodings are ISO-8859-1, ISO-8859-2 and UTF-8.",
      "Output Format - Output format specification of produced messages.",
      "Properties - Kafka properties for connection. List of all possible properties ishere.",
      "Properties Encrypted - Kafka encrypted properties for consumer.  Such as (ssl.key.password,ssl.keystore.password,ssl.truststore.password, etc).",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Regressor",
    "Step Details": "Detailed Description Based on specified features, this step predicts value based on previously trained model. All features must be the same\n                as when training the model. Transformations used in model training are applied automatically before making prediction. Note: Transformations are done in memory.",
    "Step Properties": [
      "Feature Configs - Configuration of features used to predict the value.",
      "Out Regression Column - Column, that is used to output the predicted value.",
      "Out Vector Column - Column, that is used to output the final feature vector created from features after all transformations. This vector is used for prediction.",
      "Model File - File, that contains previously trained model.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Regressor Trainer",
    "Step Details": "Detailed Description Based on specified features (measurable properties), this step trains regression model and saves trained model to the specified file.\n                Each feature can have some transformations defined. Order of the features is not important, after all transformations,\n                the features are ordered alphabetically.\n                The model can be than used in Regression step to do predictions.\n                We can for example train model to predict house value based on it's longitude, latitude, distance from coast, and density\n                of housing in the area. Note: Everything (transformations and training) is done in memory.",
    "Step Properties": [
      "Trainer Configuration - Type of regression model.",
      "Feature Configs - Configuration of features used to fit the regression model.",
      "Target Column - Column, that contains target used to fit the regression model.",
      "Out Vector Column - Column, that is used to output the final feature vector created from features after all transformations. This vector is used to train the model.",
      "Output Model File - File, that is used to output the fitted regression model.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Logging",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Logging Component",
    "Step Details": "Detailed Description Enables customization of the logging settings in the server context. In addition, it lets you update the logging configuration from the Admin Center without the need to restart the server. Logging component is used to inject the logging configuration to the server. The logging configuration is processed with the following priority: Config File defined in the Logging Component. Logging configuration specified in the Runtime Configuration. Default logging setup.",
    "Step Properties": [
      "Config File - Relative (to the server configuration file) or absolute path to the logging configuration file.If no logging configuration file is supplied, the default configuration is used: log records with the severity level INFO and higher are written to the console (standard output).",
      "Disabled - Specifies whether component should be disabled."
    ]
  },
  {
    "Step Name": "Classifier",
    "Step Details": "Detailed Description Based on specified features, this step predicts class based on previously trained model. All features must be the same\n                as when training the model. Transformations used in model training are applied automatically before making prediction. Note: Transformations are done in memory.",
    "Step Properties": [
      "Feature Configs - Configuration of features used to predict the target.",
      "Out Classification Column - Column, that is used to output the prediction.",
      "Out Probability Column - Column, that is used to output probability of the prediction.",
      "Out Vector Column - Column, that is used to output the final feature vector created from features after all transformations. This vector is used for prediction.",
      "Model File - File, that contains previously trained model.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Classifier Trainer",
    "Step Details": "Detailed Description Based on specified features (measurable properties), this step trains classification model (predicting\n                categorical variable) and saves trained model to the specified file.\n                Each feature can have transformations defined. Order of the features is not important, after all transformations,\n                the features are ordered alphabetically.\n                We can for example train model to predict flower name based on leaf length and width and size of it's blossom. Note: Everything (transformations and training) is done in memory.",
    "Step Properties": [
      "Classification Type - Type of classification model.",
      "Feature Configs - Configuration of features used to fit the classification model.",
      "Target Column - Column, that contains target used to fit the classification model.",
      "Out Vector Column - Column, that is used to output the final feature vector created from features after all transformations. This vector is used to train the model.",
      "Output Model File - File, that is used to output fitted classification model.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Lookup",
    "Step Details": "Detailed Description Expressions in Match Condition , Select Best Match and Columns elements can use values from current input record, record found in lookup file\n\t\t\tand some information about search result. Column values of found record are accessible via dot-source lookup . Another source named query contains following information about search result: query.matchingKey The searched key from input, transformed by appropriate matching\n\t\t\t\t\tvalue generator query.lookupKey The real key found in lookup file. The value may be different\n\t\t\t\t\tfrom searched key query.difference (integer) Number of diffrences between searched and found keys. Always zero\n\t\t\t\t\tin case of non-aproximative search query.relativeDifference (float) Number of differences related to lenght of real key. query.records (integer) Number of records related to corresponding non-unique lookup key. For unique key it is 1.",
    "Step Properties": [
      "Key Lookup Value - Search key value expression.",
      "Table File Name - Generic table dictionary file name (seespecification).",
      "Match Condition - Boolean expression. Only table records satisfying this condition will be matched.The expression uses three record formats (dot-sources) for current record, recordlookupfrom file\n\t\t\t\tand specialquerysource, see detailed description.\n\t\t\t\tFor example, the expressionlevenshtein(src_first_name, lookup.fname) <= 2can be used.",
      "Columns - List of column assignments for populating record with data from table.\n\t\t\t\tThe expressions in each element uses three record formats as inMatch Condition.",
      "Max Difference - Maximal number of differences in approximative string search.\n\t\t\t\tEach difference means one of the following:changing one character at any position within the stringomitting one character in the searched keyinserting one character into the searched keyWhen set to0, approximative searching is disabled. In case of non-string\n\t\t\t\tkey types or in case the table doesn't contain a string index,\n\t\t\t\tthe approximative search is not available and this property must be set to0.Default value:0.",
      "Prefix - Specifies that keys from table having prefix equal to searched key will be found, too.\n\t\t\t\tPrefixed searching is allowed only for table with string approximative index.\n\t\t\t\tIt can be combined with approximative search, e.g. prefix search with\n\t\t\t\tmaxDifference 1 will found for input \"MARY\" both \"MARYLAND\" and \"MARRIED\".Default value: false.",
      "Select Best Match - Specifies priority conditions used for selecting the best of more matching records.The expressions in each element uses three record formats as inMatch Condition,\n\t\t\t\tso for example the expressionlevenshtein(src_first_name, lookup.fname)can be used to select the best\n\t\t\t\tmatching record.",
      "When Condition - Expression that must be satisfied for this step to be executed.",
      "Id - Step identification string.",
      "Scorer - Element which stores basic scoring settings."
    ]
  },
  {
    "Step Name": "Lookup Builder",
    "Step Details": "Detailed Description This step creates a table from specified input columns and a primary key.\n\t\tThe key can be of any supported data type and need not be unique.\n\t\tThe created table can contain index structures for finding records by their key,\n\t\tbut it is possible to have no index at all. String lookup index enables approximative\n\t\tsearching for keys of the type string. Hash lookup index can be used for faster exact searching for keys\n\t\tof any type. When both indexes are present exact searches are done using Hash Lookup and approximative\n\t\tsearches are done using String Lookup. The only drawback of having both indexes is slightly increased size of\n\t\tthe output file. If neither index is present the file can still be used in certain steps and can be read with\n\t\tLookup Reader. More information about dictionary files and steps which\n\t\tuse them can be found here .",
    "Step Properties": [
      "Id - Step identification string.",
      "Key - Primary key definition.",
      "Additional Columns - Column list used to construct the table rows.",
      "File Name - Dictionary filename.",
      "Approximative Index - Specifies that index enabling approximative string searching will be included in the output file.\n\t\t\tRequires key of type string.If not specified, hash index will be created. The hash index is applicable to any key type.Default = false.",
      "Bidirect Approximative Index - The approximative index will be enhanced by adding secondary tree for searching\n\t\t\t\tin backward direction. This should improve performance of approximative searching.",
      "Best Distance Index - Applicable for string key only.\n\t\t\tSpecifies that real key values will be stored into file\n\t\t\tand these values will be used to improve possibly ambiguous search result.\n\t\t\tSearching is limited by selecting only rows which are \"nearest to searched key\" -\n\t\t\tthe distance between searched real key and real key found is minimal.\n\t\t\tThe distance is counted as case insensitiveeditDistancerelative to (divided by) length of found key.Default = false.",
      "Compressed - Specifies storage compressed row data. In case of wide data\n\t\t\tcan save disk and memory space. Unsuitable for small data.Default = false.",
      "Matching Value - Specifies how matching key values are generated.",
      "Duplicates - Specifies handling of duplicate keys and whether the index created in the file can be non-unique.",
      "User Metadata - User specific meta data which will be stored in the file."
    ]
  },
  {
    "Step Name": "Lookup Reader",
    "Step Details": "Detailed Description",
    "Step Properties": [
      "File Name - Specify the name of the file with the data to read.",
      "Key Column Name - Specify the column name to store the output of the values from the \"Table key\" column.",
      "Columns - Contains a list of columns that should be read from lookup file.\n\t\t\t\tThose definitions must comply with column definitions in the file.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Loqate",
    "Step Details": "Detailed Description",
    "Step Properties": [
      "Data Folder - The folder contains configuration locate.ini file, Loqate license file and reference data files (data packs).\n                The step expects data files with .lfs format (extracted from the original .lfz archive).",
      "Input Elements - List of input address elements source.",
      "Output Elements - List of output address elements mapping.",
      "Server Options - List of additional server options (advanced use).",
      "Default Country - Default country, used when country identification is not found in input elements.",
      "Geocoding - Use Geo location process.",
      "Id - Step identification string.",
      "When Condition - Expression that must be satisfied for this step to be executed."
    ]
  },
  {
    "Step Name": "Manual Override Builder",
    "Step Details": "Detailed Description This step manages a set of rules for manual overriding against the existing rules within the repository.",
    "Step Properties": [
      "Id - Step identification string.",
      "Type Column - String column or expression that contains a rule type. See themanual overridesection.",
      "Parent Record Pk Column - String column or expression that contains the primary key of the parent record (not applicable to R->C rule).",
      "Child Record Pk Column - String column or expression that contains the primary key of the child record.",
      "Repository - Repository properties."
    ]
  },
  {
    "Step Name": "Mapped Reconcile",
    "Step Details": "Detailed Description The algorithm calculates aggregated accounting records based on mapping from a composite key\n    \t\tto an account numbers for both account balance values and accrued amount values.\n    \t\tA new accounting record is created only if the value is not null or zero.\n\t\t\tNegative values are stored in debit column,\n\t\t\twhereas positive values are stored in the credit column.\n\t\t\tAccount number of each new record is defined by Account Balance or Accrued Amount property of the rule respectively. The algorithm sends all records to its out endpoint and\n    \t\tproduces a new record for each distinct account number with aggregated debit\n    \t\tand a new record for each distinct account number with aggregated credit  \n    \t\tto outAccountingRecords endpoint. The records at this endpoint have the following format: account : String (account number) debit : Float credit : Float So the records at outAccountingRecords endpoint are aggregated\n    \t\tin the same way as in Reconcile algorithm. Note, that account records with the same account numbers (and relevant debit or credit value) are aggregated together\n    \t\twithout distinction of the origin of values (i.e. values from Account Balance and Accrued Amount can be aggregated together). See also steps Reconcile and Compute General Ledger .",
    "Step Properties": [
      "Product Type - Defines column whose value is to be matched toProduct Typesubkey value\n\t\t\t\tfrom a mapping rule.",
      "Asset Class - Defines column whose value is to be matched toAsset Classsubkey value\n\t\t\t\tfrom a mapping rule.",
      "Group Account Type - Defines column whose value is to be matched toGroup Account Typesubkey value\n\t\t\t\tfrom a mapping rule.",
      "Currency - Defines column whose value is to be matched toCurrencysubkey value\n\t\t\t\tfrom a mapping rule.",
      "Accrued Amount - Defines column whose value is to be stored in debit or credit column in accounting record.\n\t\t\t\tFor this value, an account number fromAccrued Amountproperty of the appropriate\n\t\t\t\trule is stored asaccountcolumn in accounting record.\n\t\t\t\tThe type of the column must befloat.",
      "Account Balance - Defines column whose value is to be stored in debit or credit column in accounting record.\n\t\t\t\tFor this value, an account number fromAccount Balanceproperty of the appropriate\n\t\t\t\trule is stored asaccountcolumn in accounting record.\n\t\t\t\tThe type of the column must befloat.",
      "Rules - Mapping rules.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Matching Values",
    "Step Details": "Detailed Description The step creates matching values of specified expressions and stores them in output columns.",
    "Step Properties": [
      "When Condition - Expression that must be satisfied for this step to be executed.",
      "Id - Step identification string.",
      "Columns - Set of matching value source expressions and destination columns."
    ]
  },
  {
    "Step Name": "MDM Identify",
    "Step Details": "Detailed Description This step should be used in online service to create compound services with complex logic not achievable using just native MDM services. For every input record, step asks MDM to execute identification process. This process takes input data, cleanses them and matches them by matching rules with data in MDM.\n                If input record is matched, its would-be master_id and other matching information is added to output. Moreover values of its would-be master record are added to output. Step reads data directly via internal Java interface (not via HTTP call) and works only if NmeServerComponent is in the same JVM. If there are multiple steps in one plan, they share the same transaction when accessing data, providing consistent data.\n                For example in a plan with MDM Read and MDM Identify steps, record is first processed by MDM Read, data is changed, record is processed by MDM Identify\n                - MDM Identify will see the same data as MDM Read because the change happened in another isolated transaction.\n                This applies for both MDM Read and MDM Identify step - both of them share the same transaction.\n                The level of consistency depends on used persistence and underlying database.\n                For read-only transactions, VldbPersistence does not track read operations, potentially allowing obsolete record versions to be retained by LTC, even if needed by the transaction.\n                For read-write transactions, VldbPersistence keeps old record versions and ensures repeatable read using MVCC.\n                To ensure consistency, you need to change the underlying database isolation level from READ COMMITTED (default for most databases) to REPEATABLE READ.",
    "Step Properties": [
      "Model Config File - MDM model configuration file, usually in folder Files/etc and named nme-model.gen.xml.",
      "Master View - Entity master view.",
      "Entity - Entity name.",
      "Origin - An expression whose result is value of origin. Origin determines source system and is sometimes used in identification process (depends on MDM configuration).",
      "Input Columns - Input values.",
      "Output Match Info - If true (default), matching information columns are added to output, e.g. master_id, match_rule_name.",
      "Output Columns - List of columns to be added to output.",
      "Use fake identity to bypass security - If set to true, the step overrides the identity provided by the plan and uses its own identity which has all roles.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "MDM Read",
    "Step Details": "Detailed Description This step should be used in an online service to create compound services with complex logic not achievable using just native MDM services. For every input record, the step asks MDM to search for records of configured entity that satisfies the search criteria.\n                If multiple records are found the input record will be multiplied. Use Record Descriptor column to work with multiplied groups. The number of returned records is limited by the shared NME Runtime Property . The nme.services.range.maxCount which is used also for NME Native Services. The default value is 100 . Step reads data directly via internal Java interface (not via HTTP call) and works only if NmeServerComponent is in the same JVM. If there are multiple steps in one plan, they share the same transaction when accessing data, providing consistent data.\n                For example in a plan with MDM Read and MDM Identify steps, record is first processed by MDM Read, data is changed, record is processed by MDM Identify\n                - MDM Identify will see the same data as MDM Read because the change happened in another isolated transaction.\n                This applies for both MDM Read and MDM Identify step - both of them share the same transaction.\n                The level of consistency depends on used persistence and underlying database.\n                For read-only transactions, VldbPersistence does not track read operations, potentially allowing obsolete record versions to be retained by LTC, even if needed by the transaction.\n                For read-write transactions, VldbPersistence keeps old record versions and ensures repeatable read using MVCC.\n                To ensure consistency, you need to change the underlying database isolation level from READ COMMITTED (default for most databases) to REPEATABLE READ.",
    "Step Properties": [
      "Model Config File - MDM model configuration file, usually in folder Files/etc and named nme-model.gen.xml.",
      "Layer - Entity layer - INSTANCE or MASTER.",
      "Master View - Entity master view, required if MASTER layer is chosen.",
      "Entity - Entity name.",
      "Record Descriptor Column - Name of the column of type string to store the identification of record in group. The identifier\n                has the form<group_id>:<record_count>:<record_number>.",
      "Search - Search conditions.",
      "Output Columns - List of columns to be added to output.",
      "Use fake identity to bypass security - If set to true, the step overrides the identity provided by the plan and uses its own identity which has all roles.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "MDM Remote Read",
    "Step Details": "Detailed Description This step is still in PoC phase of development. As such it should not be a part of any customer demos. Note You need to add plugin in MDM in order to allow gRPC Server. Currently there is no security or any authorization in place. You need to fill manually column Name and data Type (Values) properly according to what is stated in MDM.",
    "Step Properties": [
      "Id - Step identification string.",
      "MDM gRPC server - Connection details for connecting to MDM gRPC Server. (File Explorer>Servers)",
      "Entity Type - Entity Type. Entity layer -INSTANCEorMASTER.",
      "Master Layer Name - Master Layer Name, e. g.,<masterLayer name=\"masters\">.",
      "Entity Name - Entity Name. Specific table that should be read from MDM.",
      "Columns - Specific columns that should be read from MDM.",
      "Shadow Columns - Contains a set of columns that are not present in the input data, but should be created in the output (so that they are then\n                available for further use as any other \"real\" column read from a file)."
    ]
  },
  {
    "Step Name": "Matching",
    "Step Details": "Detailed Description The step supports the MDM use case, where records which are once grouped together, won't be split again or merged\n\t\twith another group of records, unless a user manually chooses to or unless input data are flagged for rematching. Step uses MDM repository as its storage of records, server with running MDM instance is required for\n\t\tincremental matching.",
    "Step Properties": [
      "Id - Step identification string.",
      "Keeper Selection Rule - Default criterion for selecting of \"id keeper record\" for partitions which haven't specified its own",
      "Match Functions - List of Match Functions.",
      "Matching Measures - List of Matching Measures.",
      "Partitions - List of Partitions.",
      "Standalone Bindings - Additional bindings used in standalone mode"
    ]
  },
  {
    "Step Name": "ONE Metadata Reader",
    "Step Details": "Detailed Description Implementation Examples: You are using an external Business Intelligence tool (Tableau, Cognos, etc.) to display DQ Rules results on your data. DQ Evaluation project (rules with results) serves as a source of data for that Business Intelligence tool. In order to produce customized reports, you need to enrich each rule with its additional attributes (description, owner, custom attributes, etc.) that will be displayed in the report. You would like to create a DQC plan and you would like to drag and drop into your plan a rule that you would like to search based on its name and input attributes. You would like to create a DQC plans with data transformation, using the data sources from the ONE 2.0 Metadata Server. For your data catalog, you would like to create HTML documentation combining information about the physical object and profiling results.",
    "Step Properties": [
      "Subtypes - When you select a subtype, its properties will be available in Columns. Subtypes also function in embedded entity streams.",
      "Columns - Definitions of columns that will be retrieved from the ONE Web Application based on properties of selected Entity Type.",
      "Data Format Parameters - Data format parameters are parameters used for data formatting when an internal/external data format conversion is required. This task typically occurs whenONE IDEneeds to load data from an external file or needs to store data to an external file. Data format parameters then describe how to convert data to the desired form based on the reading and writing file data formats. Processing steps supporting DataFormatParameters (DFP) can define DFP at the top level of a step as well as on the \"local\" level of each column. If there are noDFP defined on the local level, the global DFP are used. If global data format parameters are not defined, the default values are assumed. When a column defines its own DFP,this DFP must contain all attributes needed for successful parsing (those attributes must be assigned valid values). The only exception to this rule is Thousands Separator.In contrast with the rest of DFP attributes that must always have some value, Thousands Separator may remain empty (meaning that no thousands separator is used).",
      "Embedded Entity Streams - Allows you to create separate reader that will be reading selected attributes of catalog items. There will be a separate output endpoint for those attributes.\n\t\t\tFor instance you can read connection attributes stored under data source catalog items, etc.",
      "Entity Column Name - Entities have subtypes. Define a column to store the concrete subtype (i.e. businessTerm is a subtype of term).",
      "Entity Type - Entity type that will be retrieved from theONE Web Application(catalog item, term, job, configuration, etc.).",
      "Id - Step identification string.",
      "Id Column Name - ID of the entity instance.",
      "Filter Query - Template defining structure of body of message. Enter a desired query using AQL, e.g.:$id = \"0e40b980-d671-4021-876f-a5dbb3d2052c\"For more information see,Searching in ONE Web Application.",
      "Parent Id Column Name - ID of the entity instance that is parent to the retrieved entity. For example, if you are reading an attribute, parent entity ID would be the ID of the catalogItem instance this attribute belongs to.",
      "Server Name - Connection details for connecting theONE Web Application.  (File Explorer>Servers)",
      "Shadow Columns - TheShadow Columnsare used by steps to define new columns of a specific type in the output format. The created columns can contain initial data as a result ofDefault Expression.",
      "Expression Begin Mark - Symbol marking the beginning of column placeholder.",
      "Expression End Mark - Symbol marking the end of column placeholder.",
      "Workflow State - State of the entity instance thatONE Metadata Readerstep will read.draft: the latest version available.published: the latest approved version available."
    ]
  },
  {
    "Step Name": "ONE Metadata Reference Array Writer",
    "Step Details": "",
    "Step Properties": [
      "Id - Step identification string.",
      "Server Name - Connection details for connecting theONE Web Application.  (File Explorer>Servers)",
      "Entity Type - Entity type that you will write into on theONE Web Application(catalog item, term, job, configuration, etc.).",
      "Workflow State - State of the entity instance that the step will write to.draft: the latest version available.published: the latest approved version available.",
      "Id Column Name - Column with ID of modified entity.",
      "Property - Name of property (array of references) to which to write.",
      "Operation - Operation to perform on the property.",
      "Target Id Column Name - Name of the column containing the ID of entity to add or remove from the property.",
      "Write Status Column Name - Name of the column where you want to add information whether a record was written successfully.",
      "Failure Description Column Name - Name of the column where you want to add details of possible failures when writing.",
      "Workflow State Column Name - Specify a column of a Imported Entities File to store the information, in which workflow state was the entity written intoONE Web Application(important when an error occurs during the import)",
      "Error Handling - Set the desired error handling strategy.",
      "Log File Name - Name of the log file. CSV and comma separated text files are supported.",
      "Log Into File - Enable to log records into a file instead of a database."
    ]
  },
  {
    "Step Name": "ONE Metadata Writer",
    "Step Details": "Detailed Description Implementation Examples: One time migration: The ONE Web Application will be used for management of metadata import. Integration: External tool is used for management of metadata entities. The ONE Web Application will be just a consumer. It will provide: Scheduling. Version controlling. Potential changes in MMD. Mapping of some attributes. Creating elements as a result of some data processing: When the processing detects a lot of similar data issues (or something interesting in the profiling), it automatically creates a rule that should prevent those problems in a future (a template, that has to be finished manually). This behavior would the user configure as a plan (the ONE Metadata Writer step in the data processing plan or in a component triggered after the data processing)",
    "Step Properties": [
      "Subtypes - Select entity subtypes you want to include. If you select a subtype, its properties will be available inColumnsto write in.",
      "Columns - Definitions of columns that will be written into the ONE Web Application based on properties of selectedEntity Type.",
      "Created Id Column Name - Output column. In case Output Endpoint is connected to other step it can provide information under which ID this node has been written toONE Web Application.",
      "Entity Column Name - Entities have subtypes (i.e. businessTerm is subtype of term). Specify a column with the concrete subtype filled to write instances of these subtypes. Leave empty when the selected entity has no subtypes.",
      "Entity Type - Entity type that you will write into on theONE Web Application(catalog item, term, job, configuration, etc.).",
      "Error Handling - Set the desired error handling strategy.",
      "Id - Step identification string.",
      "Id Column Name - When updating or deleting existing entity instances, specify a column with ID of these instances. Leave empty when creating new entities.",
      "Log File Name - Name of the log file. CSV and comma separated text files are supported.",
      "Log Into File - Enable to log records into a file instead of a database.",
      "Write Status Column Name - Name of the column where you want to add information whether a record was written successfully.",
      "Failure Description Column Name - Name of the column where you want to add details of possible failures when writing.",
      "Parent Id Column Name - Specify a column with ID of the parent entity instance, in which the created entity instance should be written into (i.e. Id of the concrete catalogItem when writing it's attributes). Leave empty when the parent type ismetadata.",
      "Parent Property - Property name of the entity, in which will the created/updated entity be embedded.",
      "Parent Type - Type of the parent entity (metadata, metadataRule, metadataTermInstance, metadataRuleInstance, etc.).",
      "Log File Name - Name of the file with list of imported entities together with assigned IDs.",
      "Workflow State Column Name - Specify a column of a Imported Entities File to store the information, in which workflow state was the entity written intoONE Web Application(important when an error occurs during the import)",
      "Server Name - Connection details for connecting theONE Web Application.  (File Explorer>Servers)",
      "Workflow State - State of the entity instance thatONE Metadata Writerstep will write to.draft: the latest version available.published: the latest approved version available.deleted: deleted node."
    ]
  },
  {
    "Step Name": "ModelWrapStep",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Monitoring",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "DQ Monitoring Project Aggregation Results",
    "Step Details": "Detailed Description Provides results of certain aggregations",
    "Step Properties": [
      "Monitoring Project Id - .",
      "Catalog Item Id - .",
      "Aggregation Id - .",
      "Limit - .",
      "Time Range From - .",
      "Time Range To - .",
      "Server Name - Connection details for connecting theONE Web Application. (File\n                Explorer>Servers)",
      "Shadow Columns - TheShadow Columnsare\n                used by steps to define new columns of a specific type in the output format. The created columns can contain initial data as a result\n                ofDefault Expression.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "DQ Monitoring Project Check Results",
    "Step Details": "Detailed Description Returns results of certain DQ checks. Validity checks include also explanations that provide invalidy reasons.",
    "Step Properties": [
      "Monitoring Project Id - .",
      "Catalog Item Id - .",
      "Dq Check Id - .",
      "Limit - .",
      "Time Range From - .",
      "Time Range To - .",
      "Server Name - Connection details for connecting theONE Web Application. (File\n                Explorer>Servers)",
      "Shadow Columns - TheShadow Columnsare\n                used by steps to define new columns of a specific type in the output format. The created columns can contain initial data as a result\n                ofDefault Expression.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "DQ Monitoring Project Results",
    "Step Details": "Detailed Description Returns project validity results over time. The results are aggregated over all catalog items in MP.",
    "Step Properties": [
      "Monitoring Project Id - .",
      "Limit - .",
      "Time Range From - .",
      "Time Range To - .",
      "Server Name - Connection details for connecting theONE Web Application. (File\n                Explorer>Servers)",
      "Shadow Columns - TheShadow Columnsare\n                used by steps to define new columns of a specific type in the output format. The created columns can contain initial data as a result\n                ofDefault Expression.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "DQ Monitoring Project Filter Values",
    "Step Details": "Detailed Description Provides data values of filter attributes. It lives under monitoringProjectProcessing entity - these values can change in each processing.",
    "Step Properties": [
      "Monitoring Project Processing Id - .",
      "Catalog Item Id - .",
      "Server Name - Connection details for connecting theONE Web Application. (File\n            Explorer>Servers)",
      "Shadow Columns - TheShadow Columnsare\n            used by steps to define new columns of a specific type in the output format. The created columns can contain initial data as a result\n            ofDefault Expression.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Multiplicative Address Doctor",
    "Step Details": "Detailed Description This step, which represents the Fast Completion processing mode, \n\t\tallows to obtain up to 100 results per line of input in case the address is not specified correctly. \n\t\tThe maximum number of suggestions can be set up by changing the MaxResultCount parameter to \"100\" \n\t\t(the default \"20\") in SetConfig.xml. If you want the step to create a detailed log of actions performed during the plan run, click on\n\t\t\tthe dropdown triangle next to the Run button and select Run Configurations . In the Run Configurations dialogue go to the Runtimes tab and in the VM Arguments field write -DaddressDoctor.debugLog=debug.xml . This specification will\n\t\t\tsave debug.xml into the folder with the plan. Alternatively, you can specify an arbitrary\n\t\t\tfull path.",
    "Step Properties": [
      "Data Folder - The folder containing configuration file(s) SetConfig.xml and optionally Parameters.xml.",
      "Parameters File - The path to specific parameters configuration file.",
      "Input Elements - List of input address elements source.",
      "Output Elements - List of output address elements mapping.",
      "Result Parameters - List of parsing result output mapping.",
      "Process Status - Name of column to store parsing process status.",
      "Record Descriptor Column - Name of the column of type string to store the identification of record in group. The identifier\n\t\t\t\thas the form<group_id>:<record_count>:<record_number>.",
      "Element Scorer - Element which stores scoring settings for address elements.",
      "Scorer - Element which stores basic scoring settings.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Multiplicative Guess Name Surname",
    "Step Details": "Detailed Description The step identifies a first name and a last name from specified data input\n\t\t\tin the same way the Guess Name Surname does but in addition returns all matches (see Bests Only property) of\n\t\t\tinput text to defined patterns in first matched pattern group.\n\t\t\tThis identification and parsing is dependent on dictionaries that contain \n\t\t\ta list of known first names and last names (see the properties). The step returns all matches as separate records (i.e. not only the best match but for each\n\t\t\tmatched pattern returns evaluated result). Each group of records can\n\t\t\tbe assigned an identifier of the form <group_id>:<record_count> . To illustrate its functionality let's assume: two patterns: A: {FIRST_NAME!} {LAST_NAME!} B: {LAST_NAME!} {FIRST_NAME!} dictionaries with names 'Vítek' and 'Vitek' respectively (first and last name) input row with text: 'Vítek Vitek'. When property bestsOnly is set to false, the step returns two records: input first name last name pattern Vítek Vitek Vítek Vitek A Vítek Vitek Vitek Vítek B where names are swapped respectively (see diacritics). When the property is set to true, only a single record is returned: input first name last name pattern Vítek Vitek Vítek Vitek A since the omitted record would have worse quality due to changes in diacritics. For a detailed description and other properties see Guess Name Surname .",
    "Step Properties": [
      "In - Column that contains both input first name and last name.",
      "First Name - Column that stores the final output first name (corresponding to the dictionary value).",
      "Last Name - Column that stores the final output last name (corresponding to the dictionary value).",
      "First Name Orig - Column that stores the original (input) first name.",
      "Last Name Orig - Column that stores the original (input) last name.",
      "Pattern Name - Column that stores the name of the applied parsing pattern.",
      "Hint Name - Column that stores the name of the applied parsing pattern hint.",
      "Trash - Column that stores the trash information. Trash information is the part\n\t\t\t\tof the input text which was not recognized\n\t\t\t\tas a known component or, when theFull Trash Scopeflag is true,\n\t\t\t\ttext parsed by component not having its own output columnStore Parsed Into.\n\t\t\t\tThe binding is not mandatory, thus the propertyFull Trash Scopeis effective only when this binding is set.",
      "Components - List of user defined components.",
      "Full Trash Scope - Specifies whether text parsed byComponentswhich don't define theStore Parsed Intoparameter is\n\t\t\t\tstored in the column defined by the bindingTrash. Text not parsed by any component is stored in any case.\n\t\t\t\tThis parameter has an effect only if the bindingTrashis defined.Default value:False.",
      "First Name Lookup File Name - Dictionary file that contains known first names. This dictionary\n\t\t\t\tcontainssingle-wordfirst names.\n\t\t\t\tFor more information about the dictionary seethe detailed description here.",
      "Hints - Contains a list of definitions of parsing hints. A parsing hint defines how to parse input data when more than\n\t\t\t\tone parsing pattern is found which can be applied to the input string. It assists the step in picking\n\t\t\t\tthe preferred parsing pattern in case of such ambiguity. In particular, hints are defined using propertyhint.",
      "Last Name Lookup File Name - Dictionary file that contains known last names. This dictionary\tcontainssingle-wordlast names.\n\t\t\t\tFor more information about the dictionary seethe detailed description here.",
      "Multi First Name Lookup File Name - Dictionary file that contains known first names. This dictionary\n\t\t\t\tcontainsmulti-wordfirst names (multi-word components work with this dictionary).\n\t\t\t\tFor more information about the dictionary seethe detailed description here.",
      "Multi Last Name Lookup File Name - Dictionary file that contains known last names. This dictionary\n\t\t\t\tcontainsmulti-wordlast names (multi-word components work with this dictionary).\n\t\t\t\tFor more information about the dictionary seethe detailed description here.",
      "Pattern Groups - Tag associating the group of patternspatternGroup.",
      "Preserve If Differs - Defines whether the original value of the input firstname, lastname or both \n\t\t\t\tshould be retained if the final standardized value \n\t\t\t\t(the selected dictionary value which conforms most closely\n\t\t\t\twith the original value) differs from the original value.\n\t\t\t\tThe word 'differs' here means that the values are different\n\t\t\t\twhen compared for equality ignoring case but are the same when\n\t\t\t\ttransformed by matching value generator defined by appropriate\n\t\t\t\tdictionary.Possible values:PRESERVE_FIRSTNAME,PRESERVE_LASTNAME,PRESERVE_BOTHandPRESERVE_NONEDefault value:PRESERVE_NONE.",
      "Word Definition - Default value: {WORD}",
      "Interlaced Word Definition - Default value: {INTERLACED_WORD}",
      "Multi Word Definition - Default value: {MULTIWORD:wordSeparators=\"-'`\"\"~\"}",
      "Multi Word Separators - Definition of possible word separators to be used by MULTI_FIRST_NAME and MULTI_LAST_NAME\n\t\t\t\tcomponents when parsing and verifying. Empty value causes the text be split into separate characters.\n                Default value: space.",
      "Tokenizer Config - Tokenizer Definition.",
      "Record Descriptor Column - Name of the column of type string to store the identification of record in group. The identifier\n\t\t\t\thas the form<group_id>:<record_count>:<record_number>.",
      "Bests Only - Indicates that only best matches will be put into the output. Best matches\n\t\t\t\tmeans those matches that are evaluated to be of the best quality\n\t\t\t\t(when compared input names with names from dictionaries etc).\n\t\t\t\tDefault value: false.",
      "Proposal Selection Strategy - Defines strategy to select from matched patterns.Possible values are:DISTANCE_BASED selects only those patterns whose distance calculated over all matched names is the losest.FIRST_MATCH selects only the first matched pattern regardless of others.Default value:DISTANCE_BASED.",
      "Scorer - Element which stores basic scoring settings.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Multiplicative Lookup",
    "Step Details": "Detailed Description The step returns all matches as separate records (i.e. not only the one best but more\n\t\t\tmatched record). Each group of records can\n\t\t\tbe assigned an identifier of the form <group id>:<record count>. This step is \"multiple record output\" variant of basic Lookup .",
    "Step Properties": [
      "Key Lookup Value - Search key value expression.",
      "Table File Name - Generic table dictionary file name (seespecification).",
      "Match Condition - Boolean expression. Only table records satisfying this condition will be matched.The expression uses three record formats (dot-sources) for current record, recordlookupfrom file\n\t\t\t\tand specialquerysource, see detailed description.\n\t\t\t\tFor example, the expressionlevenshtein(src_first_name, lookup.fname) <= 2can be used.",
      "Columns - List of column assignments for populating record with data from table.\n\t\t\t\tThe expressions in each element uses three record formats as inMatch Condition.",
      "Max Difference - Maximal number of differences in approximative string search.\n\t\t\t\tEach difference means one of the following:changing one character at any position within the stringomitting one character in the searched keyinserting one character into the searched keyWhen set to0, approximative searching is disabled. In case of non-string\n\t\t\t\tkey types or in case the table doesn't contain a string index,\n\t\t\t\tthe approximative search is not available and this property must be set to0.Default value:0.",
      "Select Best Match - Specifies priority conditions used for selecting the best of more matching records.The expressions in each element uses three record formats as inMatch Condition,\n\t\t\t\tso for example the expressionlevenshtein(src_first_name, lookup.fname)can be used to select the best\n\t\t\t\tmatching record.",
      "Bests Only - Indicates that only best matches will be put into the output.Default value: false.",
      "Record Descriptor Column - Name of the column of type string to store the identification of record in group. The identifier\n\t\t\t\thas the form<group_id>:<record_count>:<record_number>.",
      "Scorer - Element which stores basic scoring settings.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Multiplicative Loqate",
    "Step Details": "Detailed Description",
    "Step Properties": [
      "Data Folder - The folder contains configuration locate.ini file, Loqate license file and reference data files (data packs).\n                The step expects data files with .lfs format (extracted from the original .lfz archive).",
      "Input Elements - List of input address elements source.",
      "Output Elements - List of output address elements mapping.",
      "Server Options - List of additional server options (advanced use).",
      "Default Country - Default country, used when country identification is not found in input elements.",
      "Geocoding - Use Geo location process.",
      "Max Results - Maximal number of output proposals. When > 1 or not specified, usesSearchprocess of Loqate server.",
      "Record Descriptor Column - Name of the column of type string to store the identification of record in group. The identifier\n\t\t\t\thas the form<group_id>:<record_count>:<record_number>.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Multiplicative Naive Bayes Classifier",
    "Step Details": "Detailed Description Algorithm classifies input string based on trained model. More about algorithm http://en.wikipedia.org/wiki/Naive_Bayes_classifier . Step outputs classification for all classes. To obtain for the best match class only, use the NaiveBayesClassifier step. Step requires model generated by NaiveBayesTrainer step. Model is composed of lookup and index files.",
    "Step Properties": [
      "Accuracy - Output accuracy of classification.",
      "Classification - Output class name.",
      "Folder - Folder where model files are saved.",
      "Id - Step identification string.",
      "Input Document Tokenizer - Tokenizer of the input document.",
      "Record Descriptor Column - Record descriptor column.",
      "Columns - List of columns used for creating classification."
    ]
  },
  {
    "Step Name": "Multiplicative Pattern Parser",
    "Step Details": "Detailed Description This step performs parsing of the input string data based on defined parsing rules\n\t\t\tin the same way the Pattern Parser does\n\t\t\tbut in addition returns all matches of input text to defined patterns in first matching pattern group. The step returns all matches as separate records (i.e. not only the best match but for each\n\t\t\tmatched pattern returns evaluated result). Each group of records can\n\t\t\tbe assigned an identifier of the form <group_id>:<record_count> . For a detailed description and other properties see Pattern Parser .",
    "Step Properties": [
      "In - Column that contains the text be processed.",
      "Pattern Name - Column that contains the name of the pattern which was used to parse the input string.",
      "Trash - Column that stores the trash information. Trash information is the part\n\t\t\t\tof the input text which was not recognized\n\t\t\t\tas a known component or, when theFull Trash Scopeflag is true,\n\t\t\t\ttext parsed by a component not having its own output columnStore Parsed Into.\n\t\t\t\tThe binding is not mandatory, thus the propertyFull Trash Scopeis effective only when this binding is set.",
      "Full Trash Scope - Specifies whether text parsed by components which don't use theStore Parsed Intoparameter is\n\t\t\t\tstored into the column defined by the bindingTrash. Text not parsed by any component is stored in any case.\n\t\t\t\tThis parameter has effect only if the bindingTrashis defined.Default value:false.",
      "Parser Config - Configuration of the parser.",
      "Tokenizer Config - Configuration of the tokenizer.",
      "Record Descriptor Column - Name of the column of type string to store the identification of record in group. The identifier\n\t\t\t\thas the form<group_id>:<record_count>:<record_number>.",
      "Scorer - Element which stores basic scoring settings.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Multiplicative Regex Matching",
    "Step Details": "Detailed Description The step analyzes input string in the same way as Regex Matching does but in addition returns all matches of the input string to defined regular expressions.\n\t\t\tEexpressions that are defined as partial can match input text more than once and in such\n\t\t\tcases all matches can be return as well (see property multiplicative at Reg Expression ). The step returns all matches as separate records (i.e. not only the best match but for each\n\t\t\tmatched pattern returns evaluated result). Each group of records can\n\t\t\tbe assigned an identifier of the form <group_id>:<record_count> . To illustrate its functionality let's assume: three expressions: A: zip [0-9]{5}, defined as partial matching and multiplicative B: zip [0-9]{5}, defined as partial matching but not multiplicative C: city [a-z]+, defined as partial matching and not multiplicative each expression storing to zip, or city columns respectively input row with text: 'zip 18600, zip 46001, city Praha' The step returns four records: input zip city expression zip 18600, zip 46001, city Praha 18600 null A zip 18600, zip 46001, city Praha 46001 null A zip 18600, zip 46001, city Praha 18600 null B zip 18600, zip 46001, city Praha null Praha C The expression A is returned twice, since the expression is defined both as multiplicative and partial\n\t\t\tand it matches both, 18600 and 46001 substrings so each of them is returned as\n\t\t\ta separate record. Expression B matches both 18600 and 46001 as well but this\n\t\t\texpression is not defined as multiplicative so only the first match is returned. Null values in the records above are caused by single-column configuration of the expressions in this example. \n\t\t\tHence only one assignment is performed when the particular expression matches the input. For a detailed description and other properties see Regex Matching .",
    "Step Properties": [
      "Input - Expression applied to the input to get the input string, which will be matched with\n\t\t\t\tregular expressions.",
      "Regex Name Column - Name of the column where the name of the regular expression which has\n\t\t\t\tbeen matched to the input string will be stored. Column type must be string.",
      "Append Regex Name - If set to true then the name of the regular expression is appended to the\n\t\t\tvalue in theRegex Name Columnotherwise the value in theRegex Name Columnis rewritten.",
      "Regex Name Separator - If theAppend Regex Nameis set to true and then this property\n\t\t\tis used to separate the old value in theRegex Name Columnand the\n\t\t\tname of the regular expression.",
      "Regular Expressions - Contains definitions of regular expressions.",
      "No Match Columns - Contains a list of expression and a target column, which should be assigned when no match is found.\n\t\t\t\tThere are two cases when this element is used.If the input is NULL, then the column specified in these elements will be\n\t\t\t\t\tfilled with the NULL value (regardless of the expression).The input is not NULL, but it did not match to any of the regular expressions.\n\t\t\t\t\tIn this case values of expressions defined in this element will be sent to the output.",
      "Record Descriptor Column - Name of the column of type string to store the identification of record in group. The identifier\n\t\t\t\thas the form<group_id>:<record_count>:<record_number>.",
      "Scorer - Element which stores basic scoring settings.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Multiplicative Validate Phone Number",
    "Step Details": "Detailed Description This step verifies the validity of a phone number in the same way as the Validate Phone Number does but in addition it returns all matches of input text to defined patterns in first matched pattern group. \n\t\t\tThe matches returned never cross pattern group boundary (i.e. the same matches the parser \n\t\t\treturns to the simple variant step are returned from this step). The step returns all matches as separate records (i.e. not only the best match but for each\n\t\t\tmatched pattern returns evaluated result). Each group of records can\n\t\t\tbe assigned an identifier of the form <group_id>:<record_count> . For a detailed description and other properties see Validate Phone Number .",
    "Step Properties": [
      "Components - Definition of user components.",
      "Full Trash Scope - Specifies whether text parsed by components which don't the defineStore Parsed Intoparameter is\n\t\t\t\tstored into the column defined by the bindingTrash. Text not parsed by any component is stored in any case.\n\t\t\t\tThis parameter has effect only if the bindingTrashis defined.Default value:False.",
      "Ext Separator - A single character (only) used as a delimiter between\n\t\t\t\ttwo extension numbers in theOut Extoutput binding.\n\t\t\t\tDefault value:,(comma).",
      "Idc Lookup File Name - Dictionary file with known IDCs.\n\t\t\t\tFor more information about the dictionary seethe detailed description here.",
      "Idc Prefix - A string used for conversion of an international area code into a uniform output format.\n\t\t\t\tAllowed values are \"+\" or \"00\".Default value is an empty string.",
      "In - Column that contains the input phone number.",
      "Invalid Data Definitions - Collector tag for invalid data definitions",
      "Number Part Length - Length of a part of the phone number into which the number is split in the output format.Default value: 3.",
      "Number Separator - Separator used for phone number parsing in the output format.",
      "Out Ext - Column that stores is identified extension-line.",
      "Out Ext Orig - Column that stores is original extension-line.",
      "Out Idc - Column that stores is identified international area code.",
      "Out Idc Orig - Column that stores is original international area code.",
      "Out Phone Number - Column that stores is final standardized (output) phone number.",
      "Out Phone Number Orig - Column that stores is original phone number.",
      "Out Prov - Column that stores is identified phone provider number.",
      "Out Prov Orig - Column that stores is original phone provider number.",
      "Out Rule Name - Column that stores is used parsing rule that best describes the input structure.",
      "Tokenizer Config - Definition of the tokenizer.",
      "Pattern Groups - This section associates individualPatternGrouptags representing one specified group of patterns.",
      "Prefix - String attached to the beginning of the phone number in the output.Default value is an empty string.",
      "Prov Lookup File Name - Dictionary file with known phone providers.\n\t\t\t\tFor more information about the dictionary seethe detailed description here.",
      "Trash - Column that stores trash information. Trash information is the part\n\t\t\t\tof the input text which was not recognized\n\t\t\t\tas a known component or, when theFull Trash Scopeflag is true,\n\t\t\t\ttext parsed by the component not having its own output columnStore Parsed Into.\n\t\t\t\tThe binding is not required, thus the propertyFull Trash Scopeis effective only when this binding is set.",
      "Record Descriptor Column - Name of the column of type string to store the identification of record in group. The identifier\n\t\t\t\thas the form<group_id>:<record_count>:<record_number>.",
      "Scorer - Element which stores basic scoring settings.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Multiplicator",
    "Step Details": "Detailed Description This step duplicates input data and sends them into several output flows.",
    "Step Properties": [
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Naive Bayes Classifier",
    "Step Details": "Detailed Description Algorithm classifies input string based on trained model. More about algorithm http://en.wikipedia.org/wiki/Naive_Bayes_classifier . Step outputs only the best match class. To obtain classification for all classes, use the MultiplicativeNaiveBayesClassifier step. Step requires model generated by NaiveBayesTrainer step. Model is composed of lookup and index files.",
    "Step Properties": [
      "Accuracy - Output accuracy of classification.",
      "Classification - Output class name.",
      "Folder - Folder where model files are saved.",
      "Id - Step identification string.",
      "Input Document Tokenizer - Tokenizer of the input document.",
      "Columns - List of columns used for creating classification."
    ]
  },
  {
    "Step Name": "Naive Bayes Trainer",
    "Step Details": "Detailed Description Step creates lookup and index files for NaiveBayesClassifier and MultiplicativeNaiveBayesClassifier steps. Model is composed of lookup files and indexes, depending on the column type. File names are based on Folder and name of the column. Calculated probabilities are stored as logarithms of probability values except the new version of document, where other transformations are made.",
    "Step Properties": [
      "Classifications - Input classification(s) column.",
      "Classifications Delimiter - Classifications delimiter character(s).",
      "Id - Step identification string.",
      "Folder - Folder for saving model files.",
      "Input Document Tokenizer - Tokenizer of the input document.",
      "Matching Value - Matching value used for creating lookup files.",
      "Columns - List of columns used for creating model."
    ]
  },
  {
    "Step Name": "Name Finder",
    "Step Details": "Detailed Description Step creates a new record for each proper name found in the input string. It can also\n                identify category of the proper name (e.g. person, company). For optimal performance, one input record should be one sentence. Prepare large documents\n                with SentenceTokenizer . OpenNLP model files are necessary for this step. Model files can be trained with the NameFinderTrainer and WordTokenizerTrainer steps.",
    "Step Properties": [
      "Id - Step identification string.",
      "Names Model File - OpenNLP TokenNameFinderModel file.",
      "Words Model File - OpenNLP TokenizerModel file.",
      "Name Tag Column - Column containing proper name category (e.g. person, company).",
      "Input Column - Column containing input sentences.",
      "Output Column - Column for output names.",
      "Record Descriptor Column - Record descriptor column."
    ]
  },
  {
    "Step Name": "Name Finder Trainer",
    "Step Details": "Detailed Description Step creates the model for NameFinder . The sentence must be tokenized and contain spans (the START:entityType END:entityType tag pairs) which mark the entities. Training input can contain several entity types. The trained model will be able to detect these types. Although it is recommended to use only single entity type models, since the multi-entity-type support is still experimental. The following sample shows the correct input format: <START:person> Pierre Vinken <END> , 61 years old , will join the board as a nonexecutive\ndirector Nov. 29 . Mr . <START:person> Vinken <END> is chairman of Elsevier N.V. , the Dutch\npublishing group . More information about training can be found at OpenNLP website ( http://opennlp.apache.org ).",
    "Step Properties": [
      "Cutoff - The minimal number of times a feature must be seen, otherwise it is ignored.",
      "Id - Step identification string.",
      "Input Values - Training input column.",
      "Iterations - Number of training iterations.",
      "Model File - Output model file."
    ]
  },
  {
    "Step Name": "Normalizer",
    "Step Details": "Detailed Description Uses previously created model to normalize input data.",
    "Step Properties": [
      "Feature Configs - Configuration of features used to preform normalization of the data based on the model file loaded.",
      "Model File - Name of file with trained model that will be used for normalization.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Normalizer Trainer",
    "Step Details": "Detailed Description Trains normalization model based on selected normalization type and input data.\n                If result column is filled, use trained model to normalize training data and output the normalization to output column.",
    "Step Properties": [
      "Feature Configs - Configuration of features used to train the model and perform normalization on training input data.",
      "Output Model File - Name of the model output file.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Online services",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Online Services Component",
    "Step Details": "Detailed Description Initializes and deploys all services which should be available for online requests. The Online Services Component requires the Http Dispatcher component to be started. To change the configuration files without the need to stop or restart the online server, add paths to the folders containing the files to the Versioned Folders section of the Versioned File System Component . Note: it is not recommended for the service to produce any outputs or to modify any files inside the versioned folders (including log files). The changes to the files in versioned folders will be lost if the server stops or the configuration changes and the service restarts. Versioned folders are useful for changes that are done outside the running server.",
    "Step Properties": [
      "Disabled - Specifies whether component should be disabled.",
      "Listeners - Comma-separated list of names of HTTP listeners to which all service handlers should be registered. If the attribute is missing, all services will be deployed on all listeners.",
      "Service Lookup Folders - Relative (to the server configuration file) or absolute path to the file system folder(s) which contain all necessary configuration files, i.e. the*.onlinefiles that contain definition of online services. All*.onlinefiles from the configuration folder are processed and the defined services started."
    ]
  },
  {
    "Step Name": "Integration Output",
    "Step Details": "Detailed Description This step can be used instead of the other, more specialized, output steps, such as Text File Writer ,\n\t\t\tto create a Plan file that is usable as either a component or an online service . An output step in a Plan file in a component step\n\t\t\tcreates an endpoint named by the output step. Then data sent into this step through the 'out' endpoint are sent into the component step to the created endpoint. The\n\t\t\tformat of the records is determined by the record format of the 'out' endpoint.\n\t\t\tA similar situation is when a Plan file with an output step is used as an online service . In this case the output steps can be used as \n\t\t\ttransmitters of data to the online service and the data sent to the output step are transmitted to the online service. The \"opposite\" step is Integration Input .",
    "Step Properties": [
      "Required Columns - Columns required to be defined in the input connection.",
      "Enforce Format - When theEnforce Formatis checked only columns listed inRequired Columnssection will be provided to the output. If  theEnforce Formatis not checked all columns are provided to the output. By default this option is not checked.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "OutputStepWrap",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Parquet File Reader",
    "Step Details": "Detailed Description The step can create several output streams from one Parquet file where rows of one stream can be\n\t\t\tlogically children to rows in another stream. E. g. if a file contains client records\n\t\t\twith each client having several addresses, it can be read as a stream of clients\n\t\t\tand a separate stream of addresses of all clients.\n\t\t\tThe streams can also be independent.",
    "Step Properties": [
      "File Name - The input file.",
      "Data Streams - Output end points defined on the root level.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Parquet File Writer",
    "Step Details": "Detailed Description The data output for this step is a file in Parquet format . Along with a new parquet file, file for verifying the data integrity is generated (with suffix .crc ).",
    "Step Properties": [
      "File Name - The output file.",
      "Compression - Defines compression type of the output file.Default value: NONE.",
      "Block Size - The block size (in kilobytes) is the size of a row group being buffered in memory.Larger values will improve the IO when reading but consume more memory when writing.",
      "Page Size - The page size (in kilobytes) is for compression. When reading, each page can be decompressed independently.A block is composed of pages. The page is the smallest unit that must be read fully to access a single record.If this value is too small, the compression will deteriorate",
      "Columns - Definition of columns (corresponding by order, names and types).",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Pattern Parser",
    "Step Details": "Detailed Description This step performs parsing of the input string data based on defined parsing rules.\n\t\t\tIt tries to find a pattern that matches the input text. If such pattern is found, its \n\t\t\tdefinition is used to recognize individual parts (components) and store their values\n\t\t\tin the output. The input text is first split into tokens using the defined tokenizer. Tokens are then matched \n\t\t\tagainst defined patterns and their components. For parsing purposes, pattern\n\t\t\tdefinitions and the input string are both pre-compiled using an identical tokenizer and \n\t\t\ttherefore correct (identical) construction of all components is guaranteed. Patterns consists of components. Generally there are two types of components: basic components - these are components whose purpose is to recognize input chunks\n\t\t\t\tonly syntactically (for example: a word must contain only letters, a number must contain \n\t\t\t\tonly digits, etc.). They do not care about the meaning of the matched text - they\n\t\t\t\talso do not provide access to the matched value. user specified components - these are components that consists of some other \n\t\t\t\tcomponents (typically basic ones) and the addition of other features: naming - the component can be named and therefore used in \n\t\t\t\t\tthe patterns or other custom components storing of matched text - it is possible to define a column \n\t\t\t\t\tthat the matched value should be stored in validation - it is possible to define a lookup file where \n\t\t\t\t\tmatched text should be validated against scoring - it is possible to score if the given component\n\t\t\t\t\tdoes not match Basic components The predefined basic components are: Component name Description LETTER A letter (no parameters available) WORD A (single) word. Available parameters: int minLength - minimum number of required characters (default value: 2) int maxLength - maximum number of required characters (default value: integer max) string chars - string that defines which characters the words can be composed of;\n\t\t\t\t\t\t\t\tdefault value: [:letter:] (see ConfigurableTokenizerConfig ) MULTIWORD A multiple word string - a string consisting of more than one word. Available parameters: int minLength - minimum number of required characters (default value: 2) int maxLength - maximum number of required characters (default value: integer max) string chars - see WORD string wordSeparators A string that defines characters that are considered to be acceptable word delimiters (separators).\n\t\t\t\t\t\t\t\t\tIt is not necessary to define a space as a separator; it is automatically considered as a delimiter.\n\t\t\t\t\t\t\t\t\tThe default value is an empty string. MULTIWORD accepts various numbers of input tokens, depending \n\t\t\t\t\t\t\ton the verifier and the pattern where it is used: If a verifier is specified, then MULTIWORD accepts as many words \n\t\t\t\t\t\t\t\t\tas possible until: the whole read string is present in multi-word lookup, or each read chunk is present in single-word lookup If a verifier is not specified: then the number of words read from the input depends on the pattern where MULTIWORD is used. MULTIWORD will then contain \n\t\t\t\t\t\t\t\t\tas many words as possible according to the rest of the pattern. See the detailed multiword description here to find more information about this topic. The main purpose and advantage of the MULTIWORD component over a \n\t\t\t\t\t\t\tset of WORD components is its flexibility. Contrary to a set of WORD components that matches only exactly the same number of \n\t\t\t\t\t\t\tinput tokens, the MULTIWORD component allows you to read \n\t\t\t\t\t\t\tvarious numbers of input words depending on the given verifier or \n\t\t\t\t\t\t\tpattern. Known limitations: there must not be more separators in a sequence, otherwise MULTIWORD component will not accept the string (even it \n\t\t\t\t\t\t\t\t\tis specified in lookup file) only single-character separators are supported INTERLACED_WORD Word where each character is separated by a space character. string chars - see WORD ANWORD An alphanumeric word (no parameters available). DIGIT A single digit (no parameters available). NUMBER A natural number. Available parameters: boolean acceptSeparators - specifies whether the delimiter specified in separatorChar is acceptable between digits within a number.\n\t\t\t\t\t\t\t\t\tDefault value: false char separatorChar - value which is acceptable as a delimiter\n\t\t\t\t\t\t\t\t\tof single digits within a number. Default value: ' ' int minLength - minimum number of required characters (default value: 2) int maxLength - maximum number of required characters (default value: integer max) INTEGER Whole number. Available parameters: The same as in the case of NUMBER , with the following exceptions: acceptSeparators - default value: true operators are also acceptable ( '+' and '-') REAL_NUMBER Real number. Available parameters: char floatingPoint - a character representing a decimal separator.\n\t\t\t\t\t\t\t\t\tDefault value: '.' ROMAN_NUMBER Roman number (no parameters available). REGEXP Regular expression. Available parameters: string pattern - a string with a Java regex.Pattern (see class Pattern ). A component defined using a string. No parameters available. * Component accepting any text. NOTE: The components NUMBER and WORD are \n\t\t\tdefined by default using at least 2 digits (for NUMBER) or 2 characters (for WORD) in order\n\t\t\tto be distinguished from DIGIT or LETTER (which are defined using one \n\t\t\tdigit or character, respectively). MULTIWORD component - detailed information This component reads individual tokens from the input (tokenizer's output; the tokenizer \n\t\t\tpreprocesses data for the parser). When reading, single character tokens (and those which \n\t\t\tare equal to the one defined in the wordSeparators parameter) are considered \n\t\t\tto be a part of a word. The result of this reading process (acquired words) are merged using the ' ' character. \n\t\t\tThis way the component reads a maximum possible number of words and then processes the \n\t\t\trest of the string against the rest of the pattern (i.e. consecutive parsing when the \n\t\t\trest of the pattern is applied). If the consecutive parsing is not\tsuccessful, the \n\t\t\tcomponent detaches the last read word and performs consecutive parsing again.\n\t\t\tFinally the input string is either not parsed or when the process is successful, the \n\t\t\tcomponent stores the result in the specified output column. For accepting processes, the MULTIWORD component can be used either with the \n\t\t\tstandard single word verifier or with the MultiwordVerifier verifier, which \n\t\t\ttries to lookup the actual input in the multi-word dictionary (if defined - it is optional). \n\t\t\tIf the lookup is not successful, the step attempts to lookup individual words (i.e. \n\t\t\tsequences of characters delimited by spaces) in the single word dictionary. If any\n\t\t\tof this process succeeds, the token sequence is accepted. NOTE: the MULTIWORD component is the only component that uses \n\t\t\t\tthe multiword-lookup identifier\t'Multi file name' in the related Verifier If a dictionary based verification is not required for the MULTIWORD component, the first component defined this way accepts a maximum number of tokens. At \n\t\t\tthe same time, the rest of the components must be considered to meet pattern's \n\t\t\trequirements. For example, two consecutive MULTIWORD components which have \n\t\t\tno verification will share an input with N tokens in the following way: the first\n\t\t\tcomponent reads N-1 tokens and the second reads one token only. Example: Input string: Anna-Maria o'Donald - the string is split by the tokenizer of the parser in the\n\t\t\tfollowing way: Anna , - , Maria , , o , ' , Donald Using wordSeparators=\"-'\" definition, the MULTIWORD component merges this\n\t\t\t(the tokenizer's) output into this result: Anna-Maria , o'Donald .\n\t\t\tIf using a multi-word dictionary, the following combination is looked up: Anna-Maria o'Donald , in case of failure, the single-word dictionary is used and this\n\t\t\tcombination is looked up: Anna-Maria , o'Donald If case of no match, the MULTIWORD component detaches the last word ( o'Donald ) and\n\t\t\tif a consecutive parsing succeeds, the test is performed again, this time with the same value Anna-Maria in the multi-word dictionary and subsequently in the single-word dictionary. Pattern and component definitions There are two ways to define a component. a named component whose definition is specified in a component element. redefine a component directly within a pattern Using a component in a pattern: The component name is enclosed by brackets: '{' and '}' within the parsing pattern \n\t\t\tdefinition. Any other string defined outside brackets is considered literally; in the \n\t\t\tparsing process, such text must be part of the parsed input string to match the parsing \n\t\t\tpattern. The component usage specification is as follows: {component_name[:name_par=value_par,name_par=value_par] [| list_of_components_defining_the_structure]} Component customization In order to locally modify behavior of the component, you can: change values of component parameters redefine the component definition Change component parameters Setting of component behavior and properties can be performed using the component's parameters (see the table).\n\t\t\tParameters are specified in the following way: parameter_name=parameter_value. The specification requires\n\t\t\tthat the component name and parameter definition be separated by a  colon (':'). The component parameter's value can be quoted using both single and double quotes (' and \"). When a\n\t\t\tquote is intended to be part of the value, it must be escaped using the same quote (i.e. producing \"\" or '' ). It is also possible to use no quotation (the parameter value is not escaped) up to\n\t\t\ta first \",\" (comma), \"|\" (pipe) or \"}\" (right parenthesis). In this case, escape sequences for definition\n\t\t\tof the special characters (such as \\t, \\n, \\r or \\f) cannot be defined (the only possible definition is when the value is quoted).\n\t\t\tIt is also possible to use a special definition using the @ character. When escaped by @, every character\n\t\t\twithin escaping quotes has the explicit value (a value that has no special meaning) - e.g. definition\n\t\t\tof @\"\\t\" means that \\t is interpreted as the appropriate string (\\t) and not as a tab. Pay special attention to coding patterns of the regular expression component ( REGEXP ), where special\n\t\t\tcharacters such as '{' or '\\' are often used in the pattern, e.g. when backslash should be used in the quoted parameter,\n\t\t\tyou have to duplicate it. Examples: {MULTIWORD:wordSeparators=\"-'&quot;&quot;\\t\"} = delimiters: dash, single quote and (one) double quote, tab {MULTIWORD:wordSeparators=@\"-'.\\t\"} = delimiters: dash, single quote, dot, backslash, character 't' {WORD:minLength=2} {REGEXP:pattern=\"[a-zA-Z\\\\\\\\]\\\\d*\\\\.\\\\d{1,2}\"} = letter or backslash followed by digits, dot and 1-2 digits {REGEXP:pattern=@\"[a-zA-Z\\\\]\\d*\\.\\d{1,2}\"} = the same pattern as above without necessary double escaping For some parameters a special acronym can be used. An acronym definition must follow the\n\t\t\tparameter's name (before the separating colon). Currently only one acronym is available : \"!\"\n\t\t\t(the exclamation mark). When used, the presence of the specified component in the dictionary is required,\n\t\t\totherwise the component is not considered to be valid (it does not match the pattern). Redefinition of existing components For a named component definition, an attribute definition is used. The attribute contains\n\t\t\ta string of components representing actual components' structure definition. Redefining a component\n\t\t\tmeans changing the parsing pattern structure while other functional capabilities of the component\n\t\t\tremain unchanged (verifying against dictionaries, output column, etc.). For redefining components,\n\t\t\ta '|' character\tis used as a delimiter between the component name and its new definition. For example we\n\t\t\thave a named component COMP defined by {WORD} {WORD} with output column output then we can create the following definition {COMP|{WORD} a} . Then for parsing the {WORD} a will be used\n\t\t\tinstead of {WORD} {WORD} and if trashing is switched on the output will be stored in the output column. Optional components If you put after the component name the question mark then the parser will try first parse the input text\n\t\t\twith component and if it fails it tries to parse the input text without the component. Therefore we call\n\t\t\tit the optional component. An example is: {REGEXP?:pattern=\".\"} . If there are more optional \n\t\t\tcomponents in the pattern then the parser tries to omit the components from the right side. Pattern processing To allow for extended prioritization of parsing rules, they are divided\n\t\t\tinto parsing groups and each of them is parsed in isolation from\n\t\t\tothers. For a complete description see the Pattern Group configuration element. Scoring Scoring functionality of the Pattern Parser step is\n\t\t\tperformed via two types of scorers . The first is a main scorer applied to\n\t\t\tthe overall step. The second is applied separately to each component participating in the parsing process.\n\t\t\tEvery component scores its activity and thus it is possible to obtain detailed information about the process.\n\t\t\tComponent scorers are not required. <step id='alg' className='com.ataccama.dqc.tasks.parse.PatternParserAlgorithm'>\n\t<properties>\n\t\t<in>input</in>\n\t\t<parserConfig>\n\t\t\t<patternGroups>\n\t\t\t\t<patternGroup>\n\t\t\t\t\t<patterns>\n\t\t\t\t\t\t<!-- use of custom component -->\n\t\t\t\t\t\t<pattern name=\"rule#1\" definition=\"ulice {CP}\" priority=\"0\" />\n\n\t\t\t\t\t\t<!-- use of other named component -->\n\t\t\t\t\t\t<pattern name=\"rule#2\" definition=\"Match {MATCHED_WORD}\" priority=\"0\" />\n\n\t\t\t\t\t\t<!-- component override -->\n\t\t\t\t\t\t<pattern name=\"rule#3\" definition=\"{WORD | {REGEXP:pattern='[A-Z][a-z]+'}}\" priority=\"0\" />\n\n\t\t\t\t\t\t<!-- component override - the regular expression is not effective here, only the verifier if specified -->\n\t\t\t\t\t\t<!-- input text '123 456' is accepted by the parsing sequence: {NUMBER} {NUMBER} -->\n\t\t\t\t\t\t<pattern name=\"rule#4\" definition=\"{MATCHED_WORD | {NUMBER} {NUMBER}}\" priority=\"0\" />\n\n\t\t\t\t\t\t<!-- input text 'abc + def' is accepted (again by the parsing sequence) -->\n\t\t\t\t\t\t<!-- whatever verifier is still effective -->\n\t\t\t\t\t\t<pattern name=\"rule#5\" definition=\"{MATCHED_WORD | {WORD} + {WORD}}\" priority=\"0\" />\n\t\t\t\t\t</patterns>\n\t\t\t\t</patternGroup>\n\t\t\t</patternGroups>\n\n\t\t\t<components>\n\t\t\t\t<component name=\"CP\" definition=\"{WORD}\" storeParsedInto=\"parsed\">\n\t\t\t\t\t<verifier>\n\t\t\t\t\t\t<fileName>data/ext/street.sl.cif</fileName>\n\t\t\t\t\t\t<type>stringLookup</type>\n\t\t\t\t\t</verifier>\n\t\t\t\t</component>\n\n\t\t\t\t<component name=\"MATCHED_WORD\" definition=\"{REGEXP:pattern='[a-zA-Z0-9]+'}\" storeParsedInto=\"eWord\">\n\t\t\t\t</component>\n\t\t\t</components>\n\t\t</parserConfig>\n\n\t\t<scorer explanationColumn='expl'>\n\t\t\t<scoringEntries>\n\t\t\t\t<scoringEntry key='GP_NULL' score='300' explain='true' />\n\t\t\t\t<scoringEntry key='GP_NO_PATTERN' score='300' explain='true' />\n\t\t\t\t<scoringEntry key='GP_MORE_PATTERNS' score='300' explain='true' />\n\t\t\t</scoringEntries>\n\t\t</scorer>\n\t</properties>\n</step>",
    "Step Properties": [
      "When Condition - Expression that must be satisfied for this step to be executed.",
      "Id - Step identification string.",
      "In - Column that contains the text be processed.",
      "Pattern Name - Column that contains the name of the pattern which was used to parse the input string.",
      "Trash - Column that stores the trash information. Trash information is the part\n\t\t\t\tof the input text which was not recognized\n\t\t\t\tas a known component or, when theFull Trash Scopeflag is true,\n\t\t\t\ttext parsed by a component not having its own output columnStore Parsed Into.\n\t\t\t\tThe binding is not mandatory, thus the propertyFull Trash Scopeis effective only when this binding is set.",
      "Full Trash Scope - Specifies whether text parsed by components which don't use theStore Parsed Intoparameter is\n\t\t\t\tstored into the column defined by the bindingTrash. Text not parsed by any component is stored in any case.\n\t\t\t\tThis parameter has effect only if the bindingTrashis defined.Default value:false.",
      "Parser Config - Configuration of the parser.",
      "Tokenizer Config - Configuration of the tokenizer.",
      "Scorer - Element which stores basic scoring settings."
    ]
  },
  {
    "Step Name": "Performance runtime options",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Profiling",
    "Step Details": "Detailed Description This profiling step is used in statistical analysis of data.\n\t\t\t\t\tFor each data column this step will compute\n\t\t\t\t\tstatistics (values) such as minimum, maximum, standard and error values. The profiling step is capable of multiple analytical operations\n\t\t\t\t\tin a single pass over multiple columns of input data. All date types supported by [branding:product.name.abbreviation] can be specified as long as they correspond\n\t\t\t\t\tto the applied date operations. INTEGER, LONG DAY, DATETIME BOOLEAN STRING Data Count yes yes yes yes Null Count yes yes yes yes Not Null Count yes yes yes yes Different Value Count yes yes yes yes Unique Value Count yes yes yes yes Sum yes - yes - Variance Definition .\n\t\t\t\t\t\t\t\tResult for DAY/DATETIME values are in squared days. yes yes - - Standard Deviation Definition .\n\t\t\t\t\t\t\t\tResult for DAY/DATETIME values are in days. yes yes - - Average yes yes yes - Median yes yes yes yes Quantile yes yes yes yes Maximum yes yes yes yes Minimum yes yes yes yes First X Values yes yes yes yes Last X Values yes yes yes yes",
    "Step Properties": [
      "Id - Step identification string.",
      "Inputs - List of input sources on which the profiling is performed. Each element needs an \n\t\t\tappropriate input endpoint.",
      "Fk Analysis - List of definitions of foreign key analyses.",
      "Default Locale - Locale represents a specific geographical, political, or cultural region, with respect\n\t\t\tto data parsing and comparison as performed by the step.Default value:en_US",
      "Output File - Filename of the result file where the profile results will be stored.",
      "Export File - XML or JSON file where the profile results will be exported.",
      "Output Limit - Specifies the maximum number of frequency records being stored into the output.\n\t\t\tThe parameter is applied on both most frequent values\n\t\t\tand least frequent values.\n\t\t\tThe amount of data is thus at most twice as this value.\n\t\t\tThe value is applicable only to results of frequency and group size analysis\n\t\t\tand is applied to all profiled data separately.\n\t\t\tIf the value is set to zero, number of stored records is unlimited.Default value:1000",
      "Data Source - Data source name of database for storing drill-through data.\n\t\t\tRequired when at least oneinputhas specifiedDrill-through.",
      "Table Name Prefix - Prefix for names of database tables to which drill-through data will be stored.\n\t\t\tRequired when at least oneinputhas specifiedDrill-through.",
      "Masks - List of mask definitions.",
      "Domains - Specifies which and how domain analysis will be performed.",
      "User Metadata - Additional metadata appended to result file.",
      "Threaded - Specifies whether the step will perform calculations in single thread (one after one)\n\t\t\tor in multiple-threads (parallel processing)."
    ]
  },
  {
    "Step Name": "Quantiles",
    "Step Details": "Detailed Description Step outputs quantiles in the following format: quantile_name [STRING] - user defined name for the group of quantiles quantile_value [STRING] - value of the quantiles quantile_distinction [STRING] - rank of the quantiles",
    "Step Properties": [
      "Id - Step identification string.",
      "Quantiles - Root quantiles node."
    ]
  },
  {
    "Step Name": "Random Record Generator",
    "Step Details": "Detailed Description This step generates records with defined columns filled with random values according to definition of column generators.",
    "Step Properties": [
      "Record Count - Number of records to generate.",
      "Columns - List of columns in generated records with definition how to generate random value.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Random Record Multiplicator",
    "Step Details": "Detailed Description This step reads parent records from in endpoint and for every record it generates N random child records.\n\t\t\tN is provided by strategy configured in recordCountGenerator .\n\t\t\tChild records contain all columns and values of their parent record with additional columns defined in Columns .\n\t\t\tParent records are optionally output to out_parent endpoint.",
    "Step Properties": [
      "Distribution strategy - Strategy to get number of child records to generate per one parent record.",
      "Columns - List of added columns in generated child records with definition how to generate random value.",
      "Record Descriptor Column - Name of the column of type string to store the identification of child record in group. The identifier will be computed and\n\t\t\t\thas the form<group_id>:<record_count>:<record_number>.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "RC Validator CZ",
    "Step Details": "Detailed Description This step verifies input Czech birth numbers and recognizes and completes missing information in them. RC format The format of RC is the following: yymmddttt[t] The meaning of the fields and their restrictions follow: yy = last two digits of the year of the person's birth year. The allowed range is\n\t\t\t\t\t\t\t00-99 mm = month in year of the person's birth day, starting at 1. In this value is \n\t\t\t\t\talso encoded the gender of the person. So allowed ranges are: 01-12 : for male 51-62 : for female 21-32\t: for male - may appear only for dates after 1.4.2004 (including) 71-82\t: for female - may appear only for dates after 1.4.2004 (including) Values 00,20,50,70 are considered as artificial dates, other values are considered as\n\t\t\t\t\tinvalid date definition. dd - day in month of the person's birth day, starting at 1. The allowed\n\t\t\t\t\trange, depending on the month, is 1-31. The value 00 is considered an artificial date, \n\t\t\t\t\tother values are considered invalid date definitions. RCs issued before 1.1.1954 must consist of 9 digits, RCs issued this date or later must\n\t\t\t\t\t  consist of 10 digits all 10 digits numbers must fulfill the mod11 check. Assuming that: A = a numerical value of the first 9 digits n = the numerical value of the 10th digit the following equation must be true: A mod 11 = n If the result of the modulo operation is 10, then 0 is considered. Examples: Input birth number: 8151010043 , A = 815101004, n = 3, A modulo 11 = 3 ,\n\t\t\t\t       \t\twhich implies that the sample birth number meets the condition. Input birth number: 8102060110 , A = 810206011, n = 0, A modulo 11 = 10 , \n\t\t\t\t       \t\ti.e. result = 0 , and the input birth number also meets the condition. RCValidator automatically removes non-digit characters, checks all of above, so in order to be considered valid the RC must match all of these rules. Algorithm usage and modes (scenarios) The implemented step depends on the provided input data and therefore its processing continues according to \n\t\tone of the following three scenarios: Scenario A: The input birth number is provided in the input, in which case \n\t\t\t\t\tthe step verifies the birth number. If supplementary information about birth date and\t\n\t\t\t\t\tgender is filled in, an\tadditional comparison with data computed from the input birth \n\t\t\t\t\tnumber is made. Identified data\tare written to output. Scenario B: The input birth number is not provided but additional information about gender \n\t\t\t\t\tand birth date can be read from the input. A birth number is generated and written to the \n\t\t\t\t\toutput RC Generated together with gender and birth date input values. This scenario represents \"generation-mode\" when a missing RC is generated from provided gender\n\t\t\t\t\tand date information. Scenario C: The input birth number is not provided and both birth date and gender are missing. \n\t\t\t\t\tIn this\tcase, the birth number is not generated and output bindings are not set (contains\n\t\t\t\t\tnull values). The best case scenario is that all the output columns are filled in (input data came through the validation process\n    \t  successfully) and no scoring flag was set. Scenario B and C: If the birth number is not present in the input, the implemented step attempts to generate a fake birth number. \n\t\t\tThis operation requires valid input birth date and gender values to be provided.\n\t      \tIf any of these two pieces of information is missing, the scoring flag RC_NOT_GENERATED is set and \n\t      \tall algorithm outputs ( RC Out , RC Generated , Gender Out , Birth Date Out ) \n\t      \twill contain empty values. If the fake birth number has been successfully generated, the RC_GENERATED scoring flag is set. Note that contrary to the previous versions that were generating fake RC numbers \n\t      \tdirectly to the RC Out output, now the generated RC numbers are written to the optional RC Generated output. See the outputs description . Scenario A: RC verification process Generally the algorithm first tries to determine the date from the provided RC value and then verifies the rest of the \n\t\t\tinformation read from the RC against that date and provided hints. The algorithm first reads the RC value from the\n\t\t\tinput RC binding and cleans it up. Cleaning means that all non-digit characters are removed. Then the\n\t\t\tcleaned value is sent to validation. Generally, birth number validation includes: general RC length validation RC date guess and the following validations: artificial date validation validation of the date format of the date (valid month-day combinations, etc.) initial date guess comparison with the allowed date range which is defined by the interval \n\t\t\t\t\t\t\t  <BirthNumberSince, today's date> dummy date validation comparison of the determined date with the date hint RC gender recognition and the following validations: gender encoding format: checking gender-encoding-format vs. issue-era match. (Gender may be encoded in various forms depending on the RC issue era) comparison of the determined gender with the gender hint trailer checking & fixing verification of trailer length (RCs may have different lengths depending on on the issue era) mod11 check trailer fixing The following provides ad detailed description of each validation part: General length validation The algorithm verifies RC values that (after cleansing) consist of: 6 digits: in such case the flag RC_TRLR_MISSING is set, and the number is considered \n\t\t\t\t\t  invalid, but date & gender validations are still performed and scored 9 or 10 digits: RC is passed on to further validations other lengths: RC is considered invalid, the flag RC_INVALID is set and no further\n\t\t\t\t\t  validation is performed Artificial date validation Once the date string is read from the RC (the first 6 characters of the RC), it is checked for an artificial date. \n\t\t\tThe date is considered artificial if: its month part equals 0 its date part equals 0 this value is considered after gender decoding, so the RC date\n\t\t\t\t\t  is considered artificial if it contains these values for month in the input RC:\n\t\t\t\t\t  0 (male), 20 (extended male format), 50 (female), 70 (extended female format) When a date is considered artificial the flag RC_DATE_ARTIF is set and the whole RC is considered\n\t\t\tinvalid - no further validations are performed. Validation of date format Values read from the first 6 positions (after gender decoding) are tested if the date they represent is valid.\n\t\t\tIf some invalid date combination is detected (for example 30.2.1901 or 29.2.1901 - which are both invalid\n\t\t\tbecause date 30.2. has never existed, 29.2.1901 cannot exist since 1901 is not a leap year) then the FLAG_DATE_INVALID flag is set, the whole RC is considered invalid and no further checks are\n\t\t\tperformed. Initial date guess After checking date fields separately the initial date guess is performed. \n\t\t\tThis value depends on the length of the RC: birth number length is 6: The birth number is assumed to be formed by the first six digits without trailer characters. \n\t\t\t   \t\tIf the year value is greater than 53 then 1900 is added; if the year \n\t\t\t\t    value is less than or equal to 53, the value 2000 is added. If the resulting year value denotes a year \n\t\t\t\t    in the future, 100 is subtracted from the value. birth number length is 9: The birth number is assumed to be in the full format except for the last digit of the trailer(i.e. yyyyyyxxx). \n\t\t\t\t \tIf the year value is greater than 53 then 1800 is added, if the year value is less than or equal to 53 \n\t\t\t\t \tthen 1900 is added. birth number length is 10: The birth number is assumed to be in the full format including the trailer. The year value is computed in the same \n\t\t\t\t \tway as for the 6-digit birth numbers. Date correction - birthNumberSince, date hint and extended-gender role: After this initial guess is made, the date value is corrected by adding 100 years if one of following \n\t\t\tsituations occur: the guessed date is before the date defined by the Birth Number Since attribute the guessed date is older than 1.4.2004 but gender encoding in the RC uses the extended form (see RC format ) This correction is not performed if it would break these rules: the resulting date must not be in the future there's a date hint defined that supports (matches) the originally guessed date If the correction cannot be performed because of the above rules then: for a date before Birth Number Since the RC_DATE_BEFORE_BN_SINCE flag is set, \n\t\t\t\t\t  but further validations are still processed for a mismatch of the date and gender-encoding the RC_DATE_INVALID flag is set and no \n\t\t\t\t\t  further validation is performed Dummy date validation After the date is determined, it is compared to the Dummy Date value. If they match\n\t\t\tthe RC_DUMMY_DATE flag is set and the RC is considered invalid. If date is flagged as\n\t\t\tdummy, no further checks are performed. Date hint compare The final date check compares the date guessed from the RC with the given date hint (if present). If they do not match,\n\t\t\tthe RC_DATE_MISMATCH is set and the RC is considered invalid, but further checks are still\n\t\t\tperformed. Gender recognition and validation The gender is recognized from the RC (see RC format ) and the  \n\t\t\tdetermined value is compared to the given gender hint (if defined). In case they are not equal the RC_GNDR_MISMATCH flag is set and the RC is considered invalid. Further validations are still\n\t\t\tprocessed. Trailer validation and fixing Trailer length validation The trailer's length must match the era the RC was issued in (see RC format ) The algorithm compares the length of the trailer against the date guessed from the RC. If the trailer does not\n\t\t\t\t\tmatch the date, then the following attempts to fix the trailer are performed: extension of 9-digit RC to 10-digits This correction is invoked automatically (and cannot be influenced by the configuration). \n\t\t\t\t\t\t\tThe correction tries to extend the RC by appending a '0' digit to the beginning or to the end of the \n\t\t\t\t\t\t\ttrailer. The correction is performed only if: there's only one option how to fix the number (so numbers that can be fixed by adding '0' either to the\n\t\t\t\t\t\t\tbeginning or the end of the trailer are considered unrepairable - it is not possible to decide which \n\t\t\t\t\t\t\tcorrection is the right one) resulted number passes the mod11 trailer check (see RC format ) If the number was repaired the RC_TRLR_FIXED flag is set. Otherwise the RC_TRLR_INVALID is set and the RC is considered invalid. shortening 10-digit RC to 9-digits The correction tries to shorten a 10-digit RC to the 9-digit RC by removing a '0' from the beginning\n\t\t\t\t\t\t\tor the end of the trailer. mod11 trailer check RC must pass the mod11 check as described in the RC format description above. Setting up outputs The algorithm outputs the following values: RC (binding RC Out ) - contains verified/fixed RC from the input. Users may suppress writing invalid RCs\n\t  \t\t\t\tto this output by setting omitInvalidRc=true . It is also possible to choose whether to output the \n\t  \t\t\t\toriginal RC number or the cleansed one using the attribute Preserve Input RC Value . See their descriptions\n\t  \t\t\t\tfor more details. generated RC (binding rcGenerated) - see the RC Generated property description for more details. gender and date read from RC - these values are outputted always regardless on the overall validity of\n\t  \t\t\t\t  the RC. The only case when these values are not written out is the case when it was impossible to read\n\t  \t\t\t\t  or successfully parse the input RC value. This state is indicated by some of following flags: FLAG_RC_INVALID FLAG_DATE_INVALID FLAG_DATE_ARTIF FLAG_DATE_DUMMY",
    "Step Properties": [
      "RC - A string expression describing where to read the birth number from. Non-digit characters are automatically removed from the input.",
      "RC Out - A string column indicating where the output birth number should be stored.",
      "RC Generated - A string column indicating where the generated birth number should be stored. If this property is not defined then birth numbers are not generated.",
      "Birth Date - A day expression describing where to read the birth date hint from.",
      "Gender - A string expression describing where to read the gender hint from.",
      "Gender Out - A string column indicating where the output gender should be stored.",
      "Birth Date Out - A day column indicating where the output birth date should be stored.",
      "Allow Artificial Trailers - A flag determining if birth numbers containing artificial trailers should be considered\n\t\t\tvalid (true) or invalid (false). If thefalseoption is chosen (i.e. invalid), \n\t\t\tthe numbers are treated as any other invalid birth number - theOmit Invalid RCparameter \n\t\t\tvalue indicates whether the values should be written to the output or not.Default value:False.",
      "Birth Number Since - This attribute defines the oldest date that is acceptable as a date guess for the given RC.\n\t\t\t\tBecause there's no century part specified in RCs, the algorithm tries to guess the correct century value. \n\t\t\t\tSee thebirthNumberSince rolefor a detailed description.Usually it makes sense to set this value somewhere after the year 1900.Default value:1.1.1901.",
      "Fix 10 Digit RC - A flag indicating whether an incorrect trailer of a 10-digit birth number should be repaired or not.Default value:False.",
      "Dummy Date - A special value of the birth date in a birth number which is further considerednull. \n\t\t\t\tIf such a value is extracted from a birth number, the date is treated as invalid and the scoring flagRC_DUMMY_DATEis set. This flag does not prevent sending values read from RC to theBirth Date OutandGender Outoutputs. See theoutputs\n\t\t\t\tdescription.Note:date is considered a dummy date even if supported by the date-hint (because\n\t\t\t\tit is usually not possible to assure that hint was created independently of the RC in the primary system).\n\t\t\t\tThis is a change from the previous version where hint-supported dummy dates were accepted as valid.Default value:Null.",
      "Female Definition - A string value defining identification of the female gender. This value is compared to the inputGendervalue.Default value:F.",
      "Ignore Default Trailers - A flag indicating whether default artificial trailers (000,0000,111,1111,999,9999) should be ignored or not.Default value:False.",
      "Male Definition - A string value defining identification of the male gender. This value is compared to the inputGendervalue.Default value:M.",
      "Omit Invalid RC - A flag indicating whether invalid birth numbers should be written to the output or not. If set totrueand either an error occurs during the birth number validation or an error occurs \n\t\t\t\tduring comparison of the input date and date obtained from birth number, the birth number is not \n\t\t\t\twritten to the output. The birth number is written to the output in all other cases. The birth \n\t\t\t\tnumber is omitted when at least one of the following scoring flags is set:RC_MISSINGRC_INVALIDRC_DATE_INVALIDRC_DATE_ARTIFRC_DATE_MISMATCHRC_TRLR_MISSINGRC_GNDR_MISMATCHRC_TRLR_INVALIDRC_TRLR_ARTIF- only ifallowArtificalTrailersis set tofalseThe output ofBirth Date OutandGender Outis not influenced by theOmit Invalid RCflag.If both propertiesPreserve Input RC ValueandOmit Invalid RCare set, omitInvalidBn is used and therefore the output\n\t\t\t\tvalue is empty if one of the listed scoring flags is set.",
      "Preserve Input RC Value - A flag indicating whether the original input birth number (preserveInputValue = true) \n\t\t\t\tor the modified birth number(preserveInputValue = false)should be written to the output.If a scoring flagRC_TRLR_FIXEDis set, the property is overridden and there is a changed\n\t\t\t\tbirth number written to the output.If both propertiesPreserve Input RC ValueandOmit Invalid RCare set, omitInvalidBn is used and therefore the output\n\t\t\t\tvalue is empty if one of the listed scoring flags is set.See theoutputs descriptionfor details.Default value:False.",
      "Suffix - A string value specifying a suffix appended to the generatedfakebirth numbers.Default value is an empty string.",
      "Trailers - A list of trailers defined as artificial. Default items in the list are000,0000,111,1111,999and9999.",
      "When Condition - Expression that must be satisfied for this step to be executed.",
      "Id - Step identification string.",
      "Scorer - Element which stores basic scoring settings."
    ]
  },
  {
    "Step Name": "Record Descriptor Join",
    "Step Details": "",
    "Step Properties": [
      "Child Record Descriptor - Input column for child Record Descriptor.",
      "Column Definitions - ",
      "Id - Step identification string.",
      "Join Mode - -PARENT- Custom aggregations of children data are done for each parent record.-CHILD- Each child record is enriched with data from parent record with matching key.",
      "Outer - Step puts all records to the output (similarly to SQL outer join).",
      "Parent Key - Parent key for join"
    ]
  },
  {
    "Step Name": "Rdm Integration Output",
    "Step Details": "Detailed Description RDM Integration Output is used in conjunction with other RDM steps for the synchronization of RDM data with external databases in a process spanning several plans. The step reports back to RDM about the outcome of the process started\n                by RDM Integration Input. The step has one input endpoint, which should be connected to a data source with one row that contains parameters for the step. Usually, this a text file generated in the first plan that starts the DB synchronization. In order to\n                report errors during the sychronization process, the parameters source should also contain a column containing an error message.",
    "Step Properties": [
      "Credentials - Credentials to use when connecting to the web application. If not defined, the credentials defined in the server connection\n                definition are used.",
      "Errors - Mapping to the column where errors will be written.",
      "Id - Unique step identifier.",
      "Process Id - Mapping to the column contain the ID of the process started by RDM Integration Input. Usually, it isid.",
      "Application Url - Name of the server connection where the RDM app is accessible."
    ]
  },
  {
    "Step Name": "Rdm Reader",
    "Step Details": "Detailed Description RDM Reader is used in conjunction with other RDM steps for the synchronization of RDM data with external databases in a process spanning several plans. The step reads data from the specified RDM tables based on the ID of the process\n                started by RDM Integration Input.",
    "Step Properties": [
      "Credentials - Credentials to use when connecting to the web application. If not defined, the credentials defined in the server connection\n                definition are used.",
      "Tables - Tables.",
      "Id - Unique step identifier.",
      "Process Id - Mapping to the column containing the ID of the process started by RDM Integration Input; usually,id.",
      "Application Url - Name of the server connection where the RDM app is accessible."
    ]
  },
  {
    "Step Name": "Rdm Tables Importer",
    "Step Details": "Detailed Description RDM Tables Importer lets you import records to several RDM tables at the same time. Compared to the regular Reader step, Tables Importer locks all related tables of the table in which the data is imported: parents, children, and\n            any other related entities. This is a highly recommended way of importing data.",
    "Step Properties": [
      "Allowed Edit States - Determines the allowed changes that the imported records can cause.",
      "Credentials - Credentials to use when connecting to the web application. If not defined, the credentials defined in the server connection\n                definition are used.",
      "Tables - Tables.",
      "Id - Unique step identifier.",
      "Incremental - Turns on the incremental mode: new records are imported, existing records not present in the input file are untouched, matched records are updated.",
      "Move To Edit Action - Determines the edit state of the imported records.",
      "Application Url - Name of the server connection where the RDM app is accessible."
    ]
  },
  {
    "Step Name": "Rdm History Importer",
    "Step Details": "Detailed Description RDM History importer lets you import records to an RDM table along with their historical values. Compared to the Importer step, your source data should contain two additional columns: d_from and d_to column\n                that together form the validity period of a given edit. Therefore, the source data can contain multiple values for the same row of data with different historical validity periods. The step has an optional output endpoint for reporting errors. After you run a plan that contains this step, make sure to restart the RDM web application configuration to ensure the changes are correctly synchronized. For details, see the article How to Deploy an RDM Web App Configuration in Ataccama documentation.",
    "Step Properties": [
      "Column Mappings - This is where you map columns in the source data to the columns in the RDM table.",
      "Credentials - Credentials to use when connecting to the web application. If not defined, the credentials defined in the server connection\n                definition are used.",
      "Id - Unique step identifier.",
      "Table Name - Data is imported to this table.",
      "Application Url - Name of the server connection where the RDM app is accessible."
    ]
  },
  {
    "Step Name": "Rdm Importer",
    "Step Details": "Detailed Description RDM Importer is used for importing data to the specified RDM table. The step has an optional output endpoint for reporting errors.",
    "Step Properties": [
      "Allowed Edit States - Determines the allowed changes that the imported records can cause.",
      "Column Mappings - This is where you map columns in the source data to the columns in the RDM table.",
      "Credentials - Credentials to use when connecting to the web application. If not defined, the credentials defined in the server connection\n                definition are used.",
      "Id - Unique step identifier.",
      "Import Table - If checked, the data are imported to the corresponding table in the Import mode. This lets you check the imported data before moving it to the Edit mode and publishing.",
      "Incremental - Turns on the incremental mode: new records are imported, existing records not present in the input file are untouched, matched records are updated.",
      "Move To Edit Action - Determines the edit state of the imported records.",
      "Table Name - Data is imported to this table.",
      "Application Url - Name of the server connection where the RDM app is accessible."
    ]
  },
  {
    "Step Name": "Rdm Merge Importer",
    "Step Details": "Detailed Description RDM Merge Importer imports data from external synchronized system (database). The step sends the data to the INPUTS mode, where the incoming data can be reviewed and merged with the published.",
    "Step Properties": [
      "Column Mappings - This is where you map columns from the connected system to the columns in the RDM table.",
      "Credentials - Credentials to use when connecting to the web application. If not defined, the credentials defined in the server connection\n                definition are used.",
      "Id - Unique step identifier.",
      "Input Table Name - Name of the counterpart connected system table. The name consists of<system_name>_<table_name>as defined in the model project. Data are exported from this\n                table.",
      "Table Name - Data are imported to this RDM table.",
      "Application Url - Name of the server connection where the RDM app is accessible."
    ]
  },
  {
    "Step Name": "Rdm Integration Input",
    "Step Details": "Detailed Description RDM Integration Input is used in conjunction with other RDM steps for the synchronization of RDM data with external databases in a process spanning several plans. The step creates a shadow column id and populates it with a\n                unique process identifier which can be consumed by the RDM Reader step. Part of the configuration is specifying the Process Name , which is the name of a DB synchronization defined under Synchronization > Database in the RDM model project. The step has one input endpoint, which should be connected to a data source with one row that contains parameters for the step. The columns in the parameters input source can have arbitrary names; they will be mapped to the\n                configuration fields in the step: Username (Optional: if you are passing credentials manually) - a valid RDM user with read rights to the table. Password (Optional: if you are passing credentials manually) - user password. Type - type of synchronization to perform: FULL - exports all published data as seen on the date specified in the timestamp. INCREMENT - exports only the changes since the last synchronization until the date specified in the timestamp. Timestamp - data are downloaded in the published state as of this date. Below is a sample parameters source: username;password;type;timestamp\n                betty;5ecurePassword;INCREMENT;2016-01-01 00:00:00",
    "Step Properties": [
      "Timestamp - Mapping to thetimestampparameter.",
      "Credentials - Credentials to use when connecting to the web application. If not defined, the credentials defined in the server connection\n                definition are used.",
      "Id - Unique step identifier.",
      "Process Name - Process name. One of the synchronization tasks defined underSynchronization > Databasein the RDM model project.",
      "Process Mode - Mapping to thetypeparameter",
      "Application Url - Name of the server connection where the RDM app is accessible."
    ]
  },
  {
    "Step Name": "Rdm Extended Reader",
    "Step Details": "Detailed Description RDM Extended Reader lets you import data from a selected RDM table, including historical values. The step has one input endpoint, which should be connected to a data source with one row that contains parameters for the step. The\n                columns in the parameters input source can have arbitrary names; they will be mapped to the configuration fields in the step. Below is a sample parameters source: username;password;timestamp;timestamp_increment\n                betty;5ecurePassword;;2016-01-01 00:00:00 Leaving the timestamp column empty like in the example above is treated as \"now.\"",
    "Step Properties": [
      "Timestamp - The column in the parameters which contains (1) the date as of which to export data; (2) in case incremental timestamp is set, the end of the period for which to export data from the\n                ALL_HISTORY mode.",
      "Columns - This is where you define which columns to export.",
      "Credentials - Credentials to use when connecting to the web application. If not defined, the credentials defined in the server connection\n                definition are used.",
      "Deduplicate Increment - Controls the number of exported records in case one record has been changed and published multiple times in the selected period. The input data source attribute should\n                contain eithertrueorfalse. If true, only the latest publication will be exported. If false, all changes in the selected period will be exported.",
      "Id - Unique step identifier.",
      "Ignore Business Dates - Allows the regulation of the impact of business dates on the exported data (makes a difference in versioned tables only). The input data source attribute should contain\n                either true or false. If true, Extended Reader will ignore records' business dates when selecting records to export. If false, Extended Reader will place an additional constraint on the period, from which it will export records (per\n                record).",
      "Incremental Timestamp - The beginning of the period for which to export data from the ALL_HISTORY mode.",
      "Input Table Name - Name of the counterpart connected system table.",
      "Row filter - The column that contains a condition limiting data selection. Condition example: \"id = '19'\"",
      "Table Name - Data is exported from this RDM table.",
      "Application Url - Name of the server connection where the RDM app is accessible."
    ]
  },
  {
    "Step Name": "Rdm Write Errors",
    "Step Details": "Detailed Description RDM Write Errors is used in conjunction with other RDM steps for the synchronization of RDM data with external databases in a process spanning several plans. The step reports errors that occur when trying to synchronize RDM with the\n                external system. The step expects two inputs: A list of errors associated with any particular record, which should contain the following columns (names are arbitrary; these columns are then mapped to the relevant step properties): RDM generated id of the record - this id is usually captured by the Reader step together with other data as a column with the PRIMARY_KEY column type. Mapped to the Primary Key property. History Change Number - the column captured by the Reader step with the HCN column type. Mapped to the Publish Operation Number property. Error string - user populated column that contains the error message associated with the record. Mapped to the Errors property. Parameters: Username (Optional: if you are passing credentials manually) - a valid RDM user with read rights to the table. Password (Optional: if you are passing credentials manually) - user password. The id of the process started by RDM Integration Input. Mapped to the Process Id property. Example of the list of errors input: primary_key hcn error 123 5 Cannot write to DB: size limit exceeded 679 24 Cannot write to DB: wrong data type Example of the parameters input (a text file): username;password;id\n                betty;5ecurePassword;12",
    "Step Properties": [
      "Credentials - Credentials to use when connecting to the web application. If not defined, the credentials defined in the server connection\n                definition are used.",
      "Errors - Mapping to the input column that contains the error strings.",
      "Publish Operation Number - Mapping to the input column that contains the History Change Number of the record.",
      "Id - Unique step identifier.",
      "Primary Key - Mapping to the input column that contains the RDM generated primary key of the record.",
      "Process Id - Mapping to the parameters column that contains the ID of the process started by RDM Integration Input.",
      "System Name - Name of the external synchronized system.",
      "System Table Name - Name of the table in the synchronized system.",
      "Application Url - Name of the server connection where the RDM app is accessible."
    ]
  },
  {
    "Step Name": "Reconcile",
    "Step Details": "Detailed Description The algorithms calculates accounting records based on input records and reconcile rules. \n    \t\tEach rule defines a condition when it is to be applied, expression for getting\n    \t\taccount number and amount value, string for account name (informative to make\n    \t\trules better readable) and a flag whether the rule is to sum into a debit or credit\n    \t\tcolumn. The algorithm sends all records to its out endpoint and\n    \t\tproduces a new record for each distinct account number with aggregated debit\n    \t\tand a new record for each distinct account number with aggregated credit  \n    \t\tto outAccountingRecords endpoint. The records at this endpoint have the following format: account : String (account number) debit : Float credit : Float See also steps Compute General Ledger and Mapped Reconcile .",
    "Step Properties": [
      "Rules - Rules defining creating accounting records.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Record Counter",
    "Step Details": "Detailed Description This step serves for counting (tracking) records in a data flow. The tracking is done by virtually splitting the\n\t\trecords into batches of a specified size and tracing the exact event when the\n\t\tlast record of each batch passed. This information is then sent to the specified output. This\n\t\tcan be either a file, the standard output, or the standard error output. This step tracks processing time for each batch, and based on the\n\t\tsettings (the properties Report Perf Per Second and Report Perf Per Batch )\n\t\toutputs this information in a given format. The step also outputs information about the\n\t\tstart (event when the first record came) and the end of the processing (event when the\n\t\tactivity of the step finished). The last record event is sent to the output\n\t\tat the moment the last batch is finished. Note The Record Counter step is disabled by default. To enable it, use -DenableRecordCounters=true .",
    "Step Properties": [
      "Append - Indicates whether the output should be appended at the end\n\t\t\tof the report file or to rewrite the report file.\n\t\t\tDefault value is set.",
      "Batch Size - Size of the virtual batch (in number of records)Default value: 100 000.",
      "Report File Name - Filename for sending the output. It can be either a filename or \\\\stdout for standard\n\t\t\toutput or \\\\stderr for standard error output or \\\\logger for logger output.",
      "Report Perf Per Batch - Indicates whether to output the time needed to process each batch.Default value is set.",
      "Report Perf Per Second - Indicates whether to output the speed of processing in records per second.Default value is set.",
      "Time Stamp Format - Formating string for the timestamp. It is a string supported by the date formatter\n\t\t\t(SimpleDateFormat) from Java SE.Default value:dd.MM.yyyy HH:mm:ss",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Record Descriptor Aggregate Filter",
    "Step Details": "Detailed Description",
    "Step Properties": [
      "Condition - Logical expression that controls record filtering. The expression allows usage of Aggregating functions.",
      "Id - Step identification string.",
      "Record Descriptor Column - Input and output column for Record Descriptor."
    ]
  },
  {
    "Step Name": "Record Descriptor Builder",
    "Step Details": "Detailed Description While other steps that generate record descriptors have other primary functions, Record Descriptor Builder's sole purpose is generating record descriptors by specified expressions. A record descriptor builder step is used to assign a record descriptor to specified data. Example Data # Color Brakes Windows 1 red smart electric 2 blue analog manual 3 red smart manual 4 blue analog electric 5 red analog manual 6 blue smart electric 7 yellow smart manual 8 yellow analog manual 9 yellow analog electric We will demonstrate the following cases 1. Group by color 2. Group by color='blue' 3. Group by a. color b. brakes Group by Color Grouping by the color column creates n groups, where n equals the number of unique values in the color column. Values in other columns are irrelevant. When using color as the Record Descriptor Builder expression, source data is sorted by color, and three Group ID's are created. A Group ID of 1 corresponds to records of one sort (in this case, color Yellow), Group ID 2 corresponds to records of another (Red), and Group ID 3 corresponds to Blue. Had there been more colors, in the column, they would be assigned unique Group ID's as well. The input data with generated record descriptors will look like follows (ordered by record_descriptor_color ): # color brakes windows record_descriptor_color 7 yellow smart manual 1:3:1 8 yellow analog manual 1:3:2 9 yellow analog electric 1:3:3 1 red smart electric 2:3:1 3 red smart manual 2:3:2 5 red analog manual 2:3:3 2 blue analog manual 3:3:1 4 blue analog electric 3:3:2 6 blue smart electric 3:3:3 Group by Color Blue Grouping by color='blue' creates two groups: a group for records where the values in the color column are \"blue\" and all other records. Again, values in other columns are irrelevant. The input data with generated record descriptors will look like follows (ordered by record_descriptor_color_blue ): # color brakes windows record_descriptor_color_blue 2 blue analog manual 1:3:1 4 blue analog electric 1:3:2 6 blue smart electric 1:3:3 1 red smart electric 2:6:1 3 red smart manual 2:6:2 5 red analog manual 2:6:3 7 yellow smart manual 2:6:4 8 yellow analog manual 2:6:5 9 yellow analog electric 2:6:6 Group by Color and Brakes Grouping by color and brakes (two grouping rules) creates a separate group for each unique color/brakes combination. The values in the windows column are irrelevant. The input data with generated record descriptors will look like follows (ordered by record_descriptor_color_brakes ): # color brakes windows record_descriptor_color_brakes 5 red analog manual 1:1:1 6 blue smart electric 2:1:1 8 yellow analog manual 3:2:1 9 yellow analog electric 3:2:2 7 yellow smart manual 4:1:1 1 red smart electric 5:2:1 3 red smart manual 5:2:2 2 blue analog manual 6:2:1 4 blue analog electric 6:2:2",
    "Step Properties": [
      "Already Sorted - Check indicating that input records are already sorted by columns defined inPartition Byand don't need to be sorted. Use with caution, step logs the \t\t\t\t\t\t\tinconsistencies, however the output might contain invalid Record Descriptor values when the input is in wrong order.",
      "Id - Step identification string.",
      "Output Record Descriptor Column - Output column for Record Descriptor.",
      "Partition By - Sort column/key definition; column list inorderBy."
    ]
  },
  {
    "Step Name": "Record Descriptor Filter",
    "Step Details": "Detailed Description An example use case would be to filter a record containing columns with invalid or repeating data. Example With the following input data # Color Shape Size Record Descriptor 1 Red Round Big 2:3:1 2 Red Round Small 2:3:2 3 Red Square Small 2:3:3 4 Blue Square Small 4:3:1 5 Blue Square Big 4:3:2 6 Blue Round Big 4:3:3 Filtering Size = 'Small' would result in this output: # Color Shape Size Record Descriptor 2 Red Round Small 2:3:2 3 Red Square Small 2:3:3 4 Blue Square Small 4:3:1",
    "Step Properties": [
      "Condition - Logical expression that controls record filtering.",
      "Id - Step identification string.",
      "Record Descriptor Column - Input and output column forRecord Descriptor."
    ]
  },
  {
    "Step Name": "Record Descriptor Merger",
    "Step Details": "Detailed Description Usage of this step is after the input records were multiplied by a Multiplicative step and then multiplied again by another one or more steps and they have multiple different Record Descriptors .\n\t\t\tThe output of this step is a single Record Descriptor value based on the input Record Descriptors , which reflects\tgroups based on the input Record Descriptors . Example Outer Record Descriptor Inner Record Descriptor Output Record Descriptor 1:1:1 1:2:1 1:2:1 1:1:1 1:2:2 1:2:2 2:2:1 2:3:1 2:5:1 2:2:1 2:3:2 2:5:2 2:2:1 2:3:3 2:5:3 2:2:2 3:2:2 2:5:4 2:2:2 3:2:1 2:5:5",
    "Step Properties": [
      "Id - Step identification string.",
      "Inner Record Descriptors - Columns containing inner Record Descriptors to be merged.",
      "Outer Record Descriptor Column - Outer record descriptor describing the group as whole.",
      "Output Record Descriptor Column - Output column for Record Descriptor."
    ]
  },
  {
    "Step Name": "Record Descriptor Partitioner",
    "Step Details": "Detailed Description Example With the following input data # Color Shape Size Record Descriptor 1 Red Square Small 1:3:3 2 Red Round big 1:3:1 3 Red Round Small 1:3:2 4 Blue Square Small 2:2:1 5 Blue Square Big 2:2:2 6 Blue Round Big 2:3:3 Partitioning by shape would remove all small sizes from the current record descriptor and output the following: # Color Shape Size Record Descriptor Partitioned Record Descriptor 1 Red Square Small 1:3:3 1:1:1 2 Red Round big 1:3:1 2:2:1 3 Red Round Small 1:3:2 2:2:2 4 Blue Square Small 2:2:1 3:2:1 5 Blue Square Big 2:2:2 3:2:2 6 Blue Round Big 2:3:3 4:1:1",
    "Step Properties": [
      "Id - Step identification string.",
      "Input Record Descriptor Column - Input column for Record Descriptor.",
      "Output Record Descriptor Column - Output column for Record Descriptor.",
      "Partition By - List of columns containing key for partitioning input Record Descriptor."
    ]
  },
  {
    "Step Name": "RecordFormatInjector",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "RecordSize",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Redshift Reader",
    "Step Details": "Detailed Description This step reads data records from the results of a specified query executed on a specified Redshift database table. If the parameter Distributed is checked, this step loads data to Amazon S3 when running on cluster\n\t\t\t\tby calling LOAD command and then reads the data from there in distributed fashion.\n\t\t\t\tData from Amazon S3 is NOT automatically deleted.",
    "Step Properties": [
      "Id - Step identification string.",
      "After Script - SQL instructions representing the script to be executed in the database after reading is done.",
      "Before Script - SQL instructions representing the script to be executed in the database before reading.",
      "Columns - Contains definitions of columns which will be constructed from the JDBC query result set.",
      "Data Source Name - Name of the DataSource. DataSource groups together information regarding\n\t\t\t\taccess to the database, such as: URL, driver name, username and password.\n\t\t\t\tSee theData Source descriptionfor more details.",
      "Query File Encoding - Encoding of the query file if such a file is used.",
      "Query File Name - When defined, it represents the name of the file that contains the query to execute.\n\t\t\t\tWhen this attribute is filled in, the attributeQuery Stringmust not contain\n\t\t\t\tany value, otherwise a query conflict error is reported.",
      "Query String - SQL query to execute on the data source to obtain input data records. Only selection operations are supported.",
      "Amazon S3 Resource - Reference to Amazon S3 Resource inruntime configuration.\n\t\t\t\tIf this property is set, the step will useAmazon S3to UNLOAD the data when running on cluster.",
      "Temporary Folder - The temporary folder - folder on S3 - relatively to the resource address.\n\t\t\t\tIt is the folder where the UNLOADed data should be stored.",
      "Shadow Columns - Contains a set of columns that are not present in the input data, but should be\n\t\t\t\tcreated in the output (so that they are then available for further use as any\n\t\t\t\tother \"real\" column read from a file)."
    ]
  },
  {
    "Step Name": "Redshift Writer",
    "Step Details": "Detailed Description The standard output for this step are data records written to a database table.\n\t\t\tSaving to the database is done utilizing the SQL INSERT command. All data is written in\n\t\t\tbatches to speed up the process. For better error management, the parameter Error Handler handles\n\t\t\t\terror situations when writing to the DB. If this property is defined then the\n\t\t\t\tstep has one mandatory output, err_out . Otherwise it has no\n\t\t\t\toutput. If the parameter Distributed is checked, this step saves data to Amazon S3 when running on cluster\n\t\t\t\tand then calls COPY command to copy the saved data to Redshift database. Important notes: When the Error Handler property is defined, the step tries to handle\n\t\t\t\t\t  writing problems with the database. When it is not possible to handle them (for\n\t\t\t\t\t  example, when the driver used does not support all required functionality) it falls\n\t\t\t\t\t  back to processing as without Error Handler defined. This usually\n\t\t\t\t\t  ends up with the exception in case of writing problems, while data written to\n\t\t\t\t\t  the database depends on the error strategy used. Consult the Error Handler property description for more detail on this. The ROLLBACK and ROLLBACK_AND_STOP strategies should be used only with Commit Size set\tto 0 to ensure data integrity. In that case are all\n\t\t\t\t\tdata are rolled back (since all data comes in a single commit) when an error occurs or\n\t\t\t\t\tall data are written to the database when no error has occurred. If there are many invalid entries in the input and Error Handler is defined, writing may take significantly more time (although it is heavily dependent on the database used).\n\t\t\t\t\tWhen writing data without errors to the DB, there should not be a significant time difference between errorHandler-defined and\n       \t\t\t\terrorHandler-disabled configurations.",
    "Step Properties": [
      "After Script - SQL instructions representing the script to be executed in the database after writing is done.",
      "Batch Size - Number of elements to be used in the batch. The minimum size for a batch is 0 (no batching).\n\t\t\t\tThe maximum batch size is currently limited to 3000 rows. When the commit size is smaller\n\t\t\t\tthanBatch SizethenBatch Sizeis set to the value ofCommit Size.",
      "Before Script - SQL instructions representing the script to be executed in the database before writing.",
      "Columns - A list of columns to be written to the database. Note that column names should be defined\n\t\t\t\tin an \"unquoted\" form, even if they contain special characters or represents SQL reserved word.\n\t\t\t\tContrary to theTable Nameproperty, the column names are quoted automatically\n\t\t\t\tas needed.",
      "Commit Size - Gives the number of elements after which the commit should be performed. The currently opened\n\t\t\t\tbatch is executed and the whole current transaction is committed. The commit size is\n\t\t\t\tnot limited. When set to 0, the whole writing process will be sent as single-commit\n\t\t\t\ttransaction.",
      "Data Source Name - Name of the DataSource. The DataSource groups together information regarding\n\t\t\t\taccess to the database, such as: URL, driver name, username and password.\n\t\t\t\tSee theData Source descriptionfor more details.",
      "Error Handler - Error handler which defines the behavior of the step in case of writing problems.\n\t\t\t   When this property is set the step has one mandatory output,out_err,\n\t\t\t   where the invalid rows are sent to. The format of this output is the same as that of the input with one\n\t\t\t   additional textual column named as specified by the propertyerrorFieldName, where\n\t\t\t   the cause of problem as reported by the database are written.",
      "Table Name - Name of the table to write data to. This table must exist in the given database and must\n\t\t\t\tbe writable for the given user.Notethat it is necessary to enclose individual table name parts with quotes when:- given part contains special character(s) (according to the SQL specification)- given part matches SQL reserved word (according to the SQL specification)It's because table name can represent structured value following the catalog.schema.table\n\t\t\t\tpattern and therefore it's on the user to specify and quote individual parts when needed.Example:having catalog 'test:catalog' with schema 'User' and table 'table.1', the correct\n\t\t\t\tvalue is\"test:catalog\".\"User\".\"table.1\"(because both 'table.1' and 'test:catalog'\n\t\t\t\tcontains special characters and 'User' is an SQL reserved word).Warning:quoted parts should exactly represent DB object name since many DB's are case sensitive\n\t\t\t\twhen dealing with quoted names.",
      "Write All Columns - Specifies whether to write out all columns as defined in the input format. This\n\t\t\t\tattribute is exclusive to column definitions. If this value is set to on, then there\n\t\t\t\tmust be no columns defined in the columns element, otherwise an error is reported.",
      "Amazon S3 Resource - Reference to Amazon S3 Resource inruntime configuration.\n\t\t\t\tIf this property is set, the step will useAmazon S3to COPY the data from when running on cluster.",
      "Temporary Folder - The temporary folder - folder on S3 - relatively to the resource address.\n\t\t\t\tIt is the folder where the data from COPY command should be stored.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Referential Join",
    "Step Details": "Detailed Description The reference source contains records indexed by several keys. The definition of keys (name, components, condition)\n\t\t\tis stored in source metadata.\n\t\t\tThis step uses key definition for generating values searched in index. Each key value is calculated\n\t\t\texactly as specified in metadata using mapping from input columns to columns used in key components. Each input records and its search keys can fetch several records from reference source. All records having at least one\n\t\t\tkey from set are fetched. These records are accessed through dot-source named ref in configured expressions.\n\t\t\tFetching reference records can be limited by specifying maximal number of appropriate records found by each key. For example\n\t\t\tfor searching based on personal name it would be accepted max. 50 records to eliminate huge (and actually unusable)\n\t\t\tload caused by too common names as \"John\". Each pair of the input record and fetched reference record is then tested using specified matching rules. Records satisfying\n\t\t\tat least one rule are accepted for output. Again, maximal number of output records for one input can be specified and only\n\t\t\tthe best records (based on selection criterion) are accepted. Fetched, matched and selected reference records form output group of records calculated by output expressions.\n\t\t\tThe expressions can use input record, reference record or some metadata values accessed by result dot-source\n\t\t\tand can aggregate values in group. There are two special dot-sources used in expressions: query Applicable in matching and selection rules, i.e. during selection from all fetched records. \n\t\t\t\t\t\t\tContains columns: pk - reference record's primary key ruleName - name of satisfied matching rule count - number of all fetched reference records result Applicable in final generating of output records, in outputColumn expression.\n\t\t\t\t\t\t\tContains columns: pk - reference record's primary key ruleName - name of satisfied matching rule count - number of matched reference records order - record's order in matched group recordDescriptor - record descriptor",
    "Step Properties": [
      "Ref Source - Specifies where the reference data is stored.",
      "Match Keys - List of keys used for fetching of records from reference data.",
      "Key Column Mappings - List of expressions assigned to reference data columns.",
      "Matching Rules - List of matching rules used by selecting records fetched from reference data.",
      "Maximum Matched - Maximal number of records selected for one input record.\n\t\t\t\tWhen used,Selection Rulesmust be configured to determine how select best records.When not specified or <= 0 (default), all matched records are valid.",
      "Selection Rules - Defines order and comparator of selected records. The expressions in item uses dot-sourcesref,measuresandquery",
      "Matching Measures - List of Matching Measures.",
      "Output Columns - List of output columns.",
      "Output Unmatched - Specifies whether input records for which no reference record is matched have to be send to output.\n\t\t\t\tThen output columns are calculated usingNo Match Expression(if specified)\n\t\t\t\tand \"fake\" reference record with all columns null.Default = true.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Referential Join Builder",
    "Step Details": "Detailed Description",
    "Step Properties": [
      "Keys - Definitions of reference keys.",
      "Ref Source - Specifies where the reference data will be stored.",
      "Incremental - Incremental build. Input records will be merged with existing reference records.Default: false (whole reference data will be rebuilt)",
      "Primary Key Column - String expression for record's primary key. Mandatory when incremental build.",
      "Delete Flag Column - Boolean expression specifies whether the record have to be deleted. Meaningful when incremental build.",
      "Map All Columns - All input columns will be used as reference record.",
      "Columns - List of columns of reference record.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Regex Matching",
    "Step Details": "Detailed Description This step extracts values from the input expression and tries to apply them to regular\n\t\t\texpressions in order to extract values to output columns.\n\t\t\tIt works according to the following steps: The step evaluates the input expression and tries to \"match\" the\n\t\t\t\tresult value with one of the defined regular expressions. Matching against regular expressions is done one by one in the same order as it\n\t\t\t\tis defined in the configuration. Matching stops when the first match is found. If a match is found, then the first possible match of the regular\n\t\t\t\texpression with the input value is evaluated and the divided data is sent to the\n\t\t\t\tpredefined output columns. Format of the output data is given by the\n\t\t\t\tproperties of the step. If no match is found, i.e., the value from the input column doesn't match the\n\t\t\t\tregular expression (the value can't correctly be divided into output columns),\n\t\t\t\ta scoring flag is set and processing of the current\trecord is stopped For information about regular expressions see the Regular expressions section on Expressions page.",
    "Step Properties": [
      "When Condition - Expression that must be satisfied for this step to be executed.",
      "Id - Step identification string.",
      "Input - Expression applied to the input to get the input string, which will be matched with\n\t\t\t\tregular expressions.",
      "Regex Name Column - Name of the column where the name of the regular expression which has\n\t\t\t\tbeen matched to the input string will be stored. Column type must be string.",
      "Append Regex Name - If set to true then the name of the regular expression is appended to the\n\t\t\tvalue in theRegex Name Columnotherwise the value in theRegex Name Columnis rewritten.",
      "Regex Name Separator - If theAppend Regex Nameis set to true and then this property\n\t\t\tis used to separate the old value in theRegex Name Columnand the\n\t\t\tname of the regular expression.",
      "Regular Expressions - Contains definitions of regular expressions.",
      "No Match Columns - Contains a list of expressions and target columns, which should be assigned when no match is found.\n\t\t\t\tThere are two cases when this element is used.If the input is NULL, then the column specified in these elements will be\n\t\t\t\t\tfilled with the NULL value (regardless of the expression).The input is not NULL, but it did not match to any of the regular expressions.\n\t\t\t\t\tIn this case values of expressions defined in this element will be sent to the output.",
      "Scorer - Element which stores basic scoring settings."
    ]
  },
  {
    "Step Name": "Regex Splitter",
    "Step Details": "Detailed Description This step splits the input into separate words using one or more regular expressions. New record is created for each part of the original input.\n      \t\tIf the regular expression pattern doesn't match, original input is returned.\n      \t\tThe All Sentence Column property is an input expression for the splitting. The One Word Column property\n      \t\tis an output column for the split words. Regular Expressions is the array of regular expressions for splitting the input. \n      \t\tRegular expressions are applied in the same order as they are inserted.",
    "Step Properties": [
      "Id - Step identification string.",
      "All Sentence Column - Column containing the input string to be split.",
      "One Word Column - Column containing split output words.",
      "Record Descriptor Column - Name of the column of type string to store the identification of record in group. The identifier\n\t\t\thas the form<group_id>:<record_count>:<record_number>.",
      "Regular Expressions - List of regular expression patterns for splitting the input."
    ]
  },
  {
    "Step Name": "Relation Analysis",
    "Step Details": "Detailed Description Analyzes two sets of keys coming from two input data flows and\n\t\t\tcalculates occurences of several relation types. The types of relations\n\t\t\tare: 1:1 - keys unique in both inputs 1:M - keys unique in left input N:1 - keys unique in right input N:M - keys not unique 1:0 - keys unique in left but not present in right N:0 - keys not present in right 0:1 - keys unique in right but not present in left 0:M - keys not present in left null - null keys in left or right One record for each of these relations is send to the out endpoint. The record contains the number of corresponding distinct keys\n\t\t\tand number of corresponding records in the left and right. Optionally, the output records sent to out_left/out_right can\n\t\t\tstore appropriate counts of records with the same key in the left and/or right\n\t\t\tdata flow.",
    "Step Properties": [
      "Key - Components of key on which the entities are joined.",
      "Left - Specifications of left entity data flow.",
      "Right - Specifications of right entity data flow.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Remap Tool",
    "Step Details": "Detailed Description This step sorts data by primary keys and inserts group numbers (IDs) to specified fields, which preserves\n\t\t\tthe assignment of groups and their IDs.\n\n\t\t\tThis step is suitable for functional tests where it is necessary to \"normalize\"\n\t\t\tcandidate/client groups independent of IDs assigned by the unification step.",
    "Step Properties": [
      "Primary Key Column - Definition of primary key by column.",
      "Id Columns - List of columns (client ID column, candidate ID column) to be re-mapped.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Remote Access Component",
    "Step Details": "Detailed Description Remote Access Component exposes WebHCat-like REST API interface for browsing Hive Metastore. It enables browsing Hadoop and Databricks Hive Metastore from IDE.",
    "Step Properties": [
      "Cluster - ",
      "Disabled - Specify whether component should be disabled.",
      "Impersonate - ",
      "Prefix - ",
      "Services - "
    ]
  },
  {
    "Step Name": "Remote Executor Component",
    "Step Details": "Detailed Description REST API service for Ataccama jobs. Used for starting remote jobs from any Ataccama Server, Ataccama IDE and ONE web application.",
    "Step Properties": [
      "Disabled - Specify whether component should be disabled.",
      "Local Root Folder - Folder to store temporary files required for the processing.",
      "Prefix - The prefix under which ONE Server will listen to REST API requests and accept jobs.",
      "Max Running Jobs - Maximum number of simultaneously running jobs. Any additional jobs will be queued.0means there is no limit.",
      "Enable Edit Properties - Enables editingexecutor.properties,hadoop.propertiesanddbricks.propertiesvia Admin Center.",
      "Properties File - Specify path toexecutor.propertiesthe main configuration file for executor processes."
    ]
  },
  {
    "Step Name": "Repository Key Converter",
    "Step Details": "Detailed Description This step generates keys based on unification constraints so that\n\t\t\tthe output can be used by the Repository Writer to build the new repository applicable to incremental Unification . Also, this step can generate the \"merge survivor\" data record property based on the input\n\t\t\tunification role. Lastly, it is mostly used in converting repository keys from one version of a repository (older) to\n\t\t\tanother (newer).",
    "Step Properties": [
      "Groups - List of unification methods. Must be the same as the Unification step configuration\n\t\t\t\tfor which the repository is generated.NOTE: For conversion purposes, it is not necessary to specify matching and/or pivot selection rules\n\t\t\t\tfor definition of matching groups,",
      "Key Column - Column used to store generated unification keys.",
      "Merge Survivor Role Column - Column used to store the \"merge survivor role\" specification for candidate and/or matching groups.\n\t\t\t\tRelevant only when the parameterUse Pivot As Survivoris set to \"false\".",
      "Unification Role Column - Input column containing unification role codes/values.\n\t\t\t\tRelevant only when the parameterUse Pivot As Survivoris set to \"false\" and\n\t\t\t\t\"merge survivor role\" is generated.",
      "Use Pivot As Survivor - This property is counterpart of the same property in the target unification process.When true, unification role itself will determine \"Merge survivor\" records.\n\t\t\t\tWhen false, merge survivor role will be generated intoMerge Survivor Role Columnbased on input unification role.\n\t\t\t\tSeeID StabilityDefault value: false.",
      "When Condition - Expression that must be satisfied for this step to be executed.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Repository Reader",
    "Step Details": "Detailed Description This step reads all valid items from a specified file representing a local repository storing\n\t\t\tincremental changes for database identification keys. This file is located in the repository's directory and is named\n\t\t\t\"repos_data.cif\" (or \"data.cif\" for [branding:product.name.abbreviation] version 2 (then known as Purity)). The output column Key Column contains the pairing-key values that were assigned when the repository was created. By using classes com.ataccama.dqc.tasks.identify.[ver].repository.read.RepositoryReader it is possible to read the repository format of previous [branding:product.name.abbreviation] versions {v2, v3, v35, etc.}.",
    "Step Properties": [
      "Id - Step identification string.",
      "Pk Column - Column name that stores the primary key value.",
      "Key Column - Column name that stores the pairing-key value.",
      "Columns - Contains a set of column definitions that are read from the repository. These must match the \n\t\t\t\trepository column definitions. In the directory where the repository is located, the \"repository.xml\"\n\t\t\t\tfile defines the repository's internal structure. It is recommended to\n\t\t\t\tinspect the elements in the repository.xml file to ensure format consistency with the repository reader.",
      "Repository - Repository specification.",
      "Shadow Columns - Contains a set of columns that are not present in the input data, but should be\n\t\t\t\tcreated in the output (so that they are then available for further use as any\n\t\t\t\tother \"real\" column read from a file)."
    ]
  },
  {
    "Step Name": "Repository Writer",
    "Step Details": "Detailed Description This step write all valid items from a specified source to a local repository storing incremental changes for\n\t\t\tdatabase identification keys.\n\t\t\tFor each repository file, the write step creates a data file and an index file.",
    "Step Properties": [
      "Key Column - Column or expression containing the pairing key.",
      "Pk Column - Name of the column that contains the primary key of entries.",
      "Candidate Id Column - Name of the column that contains the candidate group.",
      "Matching Id Column - Name of the column that contains the ID of the matching group.",
      "Unification Role Column - Name of the column that contains the instance unification role.",
      "Merge Survivor Role Column - Name of the column that contains the encoded MSR roles.",
      "Columns - Contains a list of columns to be written to the repository file. This element is not\n\t\t\t\trequired and, if not specified, all columns will be written out.",
      "Repository - Repository specification.",
      "Write All Columns - Specifies whether to write out all columns as defined in the input format. This attribute is exclusive\n\t\t\t\tto column definitions. If this value is set to on, then there must be no columns defined\n\t\t\t\tin the columns element, otherwise an error is reported.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Representative Creator",
    "Step Details": "Detailed Description This step computes a set of representative records from source data utilizing a process where\n\t\t\trecords are classified by a specified rule and grouped by an appropriate key. The records are sorted for each group, and the best records are selected for each group.\n\t\t\tThe new representative record is collected from a specific data record or from aggregated\n\t\t\tvalues of a group of records. New values can be also stored in the original records.",
    "Step Properties": [
      "Default Locale - Defines the locale for parsing non-numerical data (for example, abbreviated months in dates -\n\t\t\t\te.g., Sep 18, 1999). This value is the same as the value of the corresponding locale in Java. \n\t\t\t\tFor more detail seeJava locales.Default value: System locale.",
      "Grouping Strategy - Specifies how groups are defined. There are two strategies.1. groups are defined by one or more keys as in GROUP BY.2. groups are defined byrecord descriptors.",
      "Rules - Set of rules for record selection. The rule defines record grouping and the process of selecting record attributes (column values).\n\t\t\t\tEach record is processed by the first rule satisfying theWhencondition.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "RUIAN",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Task Run MDM Export",
    "Step Details": "Detailed Description Starts an MDC export operation.",
    "Step Properties": [
      "Operation Name - Comma-separated list of operation IDs to be invoked. Workflow fails at runtime if any of the operations is unknown. Trailing space is allowed (operation names are trimmed),\n                empty operation names are discarded. If the expression evaluates to an empty list, the task succeeds as well.",
      "Parameters - Set of parameters to pass to the batch operations. These can be used as component parameters, path variable definitions, or elements in more\n                complex parameter definitions."
    ]
  },
  {
    "Step Name": "Task Run MDM Load",
    "Step Details": "Detailed Description Starts a batch load for a connected system.",
    "Step Properties": [
      "Operation Name - ID of the operation to be invoked. Workflow fails to load if the operation is not known.",
      "Parameters - Set of parameters to pass to the batch operations. These can be used as component parameters, path variable definitions, or elements in more\n                complex parameter definitions."
    ]
  },
  {
    "Step Name": "Task Run MDM Multi Load",
    "Step Details": "Detailed Description Starts a batch load for one or more connected systems. Grouping of multiple batch operations can increase processing throughput.",
    "Step Properties": [
      "Operation Names - ID of the operation to be invoked. Workflow fails to load if the operation is not known.",
      "Parameters - Set of parameters to pass to the batch operations. These can be used as component parameters, path variable definitions, or elements in more\n                complex parameter definitions."
    ]
  },
  {
    "Step Name": "Runtime configuration",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "RVN Validator",
    "Step Details": "Detailed Description Tests the input string as a RVN (Rentenversicherungsnummer - German personal ID) and verifies its validity.\n\t\t\tThis step verifies RVN length, value of the check digit and whether a character corresponding to the first character\n\t\t\tof the maiden name is in the correct position.\n\t\t\tIf the binding Gender Hint or the binding Date Hint is filled in, then the RVN is tested for these values as well. The method of the resultant gender computation is as follows: Input gender (binding Gender Hint ) is written to the output (binding Gender Out ). If the input gender (binding Gender Hint ) is not filled in and the RVN is valid, then the gender computed from the RVN is written to the output and the scoring flag RV_GNDR_HINT_MISSING is set. If the input gender (binding Gender Hint ) and computed gender differ, then the input gender is written to the output and the scoring flag RV_GNDR_MISMATCH is set. If the input gender is invalid, then the input gender is written to the output and the scoring flag RV_GNDR_HINT_INVALID is set. If the input gender is missing, then the scoring flag RV_GNDR_HINT_MISSING is set. The method of the resultant birth date computation is as follows: Input birth date (binding Date Hint ) is written to the output (binding Date Out ). If the input birth date (binding Date Hint ) and computed birth date differ, then the input birth date is written to the output and the scoring flag RV_DATE_MISMATCH is set. If the input birth date is missing, then the scoring flag RV_DATE_HINT_MISSING is set. In addition to this standard behavior, the step takes care of the special RVN forms which can occur when the \n\t\t\t\tday or month of birth (or both) is not specified. Such RVN are defined as valid. Since the date part is not\n\t\t\t\tcomplete, the birth date cannot be computed, and if the input birth date is also missing (binding Date Hint ), then the output birth date (binding Date Out )\n\t\t\t\tremains empty. RVN date information is verified by separate parts - days, months and years are validated independently.\n\t\t\t\tIf a part is missing then an appropriate scoring flag is set - RV_RVN_DAY_UNKNOWN or RV_RVN_MONTH_UNKNOWN . These flags are set independently of the comparison of the computed and input date and\n\t\t\t\tare set only if the RVN is valid (passes length and sum check tests). If the input RVN is not specified, but input birth date and gender are both specified, then the step attempts to generate a fake RVN.\n\t\t\t\tThe generation requires also information about location of the RVN release and first letter of the person's maiden name.\n\t\t\t\tThese additional values are defined in the Gen Location and Gen Surname Letter properties.\n\t\t\t\tThe value specified for Gen Location must contain only two digit characters and the \n\t\t\t\tvalue for the Gen Surname Letter only one letter character. An RVN is generated only if the input RVN is empty, e.g., for the input RVN value 'abcd' which is eliminated during the cleansing\n\t\t\t\tto an empty string, the fake RVN is not generated and only the scoring flag RVN_MISSING (which indicates an empty RVN value after cleansing) is set.",
    "Step Properties": [
      "Rvn - Column that contains the input RVN.",
      "Rvn Out - Column that stores the verified RVN.",
      "Date Hint - Column that contains the birth date used to verify the RVN.",
      "Date Out - Column that stores the resulting birth date (either input birth date or birth date computed from the RVN, depending on property settings).",
      "Gender Hint - Column that contains the gender used to verify the RVN.",
      "Gender Out - Column that stores the resulting gender (either input gender or gender computed from the RVN, depending on property settings).",
      "Female Definition - String defining identification of the female gender.Default value:F.",
      "Gen Location - The stringlocationis used in generation of the fake RVN. The length of the\n\t\t\tstring must be only 2 characters (representing digits).Default value:00",
      "Gen Surname Letter - The stringGen Surname Letteris used in the generation of the fake RVN as a substitute\n\t\t\tfor the first character of the maiden name.\n\t\t\tThe length of the string must be 1 and the given character must represent a simple letter\n\t\t\t(letter from the range A-Z).Default value:A",
      "Male Definition - String value defining identification of the male gender.Default value:M.",
      "Omit Invalid RVN - Flag that determines whether invalid RVNs should be written to the output (flag set tofalse)\n\t\t\tor not(flag set totrue).Default value:False",
      "Preserve Input Value - Flag that determines whether the input RVN (flag set totrue)\n\t\t\tor the corrected RVN (flag set tofalse) is written to the output.\n\t\t\tThe value has meaning only for valid input data - for invalid input the cleaned input value is set to the output.Default value:False",
      "When Condition - Expression that must be satisfied for this step to be executed.",
      "Id - Step identification string.",
      "Scorer - Element which stores basic scoring settings."
    ]
  },
  {
    "Step Name": "S 3  Health State Provider",
    "Step Details": "Detailed Description Adds sensor to the Server Health Status section of the Admin Center. Sensor reports health status of the Amazon S3 server connections.",
    "Step Properties": [
      "Disabled - Specifies whether component should be disabled.",
      "Data Source Refresh Rate - Frequency of the database connections health state refresh."
    ]
  },
  {
    "Step Name": "Salesforce Reader",
    "Step Details": "Detailed Description Salesforce Reader allows users to connect to the backend of all Salesforce systems. The data retrieved can then be profiled and documented. The step can only be used if ONE Desktop is connected to a Salesforce server. It is possible to select which SObject is loaded from Salesforce, as well restrict the maximum number of records read.",
    "Step Properties": [
      "SObject - You will be provided with aChoose Itemdialog where you select an SObject (available SObjects are listed here). Once you make your choice, the identification of that SObject will be stored in this property.",
      "Columns - Definition of columns which will be read from the SObject.",
      "Id - Step identification string.",
      "Max entries - The maximum number of rows that will be read from the selected SObject.",
      "Salesforce server - Connection details for connecting to the Salesforce server (File Explorer > Servers).",
      "Shadow Columns - TheShadow Columnsare used by steps to define new columns of a specific type in the output format. The created columns can contain initial data as a result ofDefault Expression."
    ]
  },
  {
    "Step Name": "Salesforce Writer",
    "Step Details": "Detailed Description This step is used for modifying data in SObjects using one of the following operations: INSERT : Creates a new column. UPDATE : Updates a column. In this case, the SObject ID field  or external ID column must be specified in the Columns element. UPSERT : Creates a new column or updates an existing one, depending on whether a column with the same name is already present in the SObject or not. DELETE : Moves a column to the Salesforce Recycle Bin. In this case, the SObject ID field must be specified in the Columns element. HARD_DELETE : Deletes a column permanently. In this case, the SObject ID field or external ID column must be specified in the Columns element.",
    "Step Properties": [
      "SObject - Name of the target SObject.",
      "Columns - Definition of SObject columns.",
      "External ID field name - Name of the column to be modified. Must match an existing column in the SObject. If the selected operation type isINSERT, a column with this name will be created.",
      "Id - Step identification string.",
      "Operation - Type of operation that will be performed on the selected SObject.",
      "Salesforce server - Name of the Salesforce server to be used.",
      "Write All Columns - Specifies whether to write out all columns as defined in the input format. This attribute is exclusive to column definitions. If this option is enabled, then there must be no columns defined in the Columns element, otherwise an error is reported."
    ]
  },
  {
    "Step Name": "SAP RFC Execute",
    "Step Details": "Detailed Description Modifies SAP RFC data using a preconfigured SAP function and writes its output results. The availability of function modules depends on the connected SAP system. The results are outputted to a suitable format, for example, a database or an XML file.",
    "Step Properties": [
      "Columns - Definition of SAP RFC columns. These correspond to the SAP function parameters.",
      "SAP RFC Function - Name of the SAP function that will be executed on the data.",
      "Function Parameters - Definition of the SAP function parameters. The necessary parameters are automatically loaded after the function is selected.",
      "Id - Step identification string.",
      "SAP server - Name of the SAP RFC server to be used.",
      "Write All Columns - Specifies whether to write out all columns as defined in the input format. This attribute is exclusive to column definitions. If this option is enabled, then there must be no columns defined in the Columns element, otherwise an error is reported."
    ]
  },
  {
    "Step Name": "SAP RFC Reader",
    "Step Details": "Detailed Description SAP RFC Reader allows users to connect to an SAP RFC server. The Remote Function Call (RFC) interface enables communication between remote SAP systems. The data retrieved can then be profiled and documented. The step can only be used if ONE Desktop is connected to an SAP RFC server. It is possible to select which table is loaded from SAP RFC, as well restrict the maximum number of records read.",
    "Step Properties": [
      "Columns - Definition of columns which will be read from the SAP RFC table.",
      "Id - Step identification string.",
      "Number Of Rows - The maximum number of rows that will be read from the selected table.",
      "SAP server - Connection details for connecting to the SAP RFC server (File Explorer > Servers).",
      "Shadow Columns - TheShadow Columnsare used by steps to define new columns of a specific type in the output format. The created columns can contain initial data as a result ofDefault Expression.",
      "Table name - You will be provided with aChoose Itemdialog where you choose an SAP RFC table (available tables are listed here). Once you make your selection, the identification of that table will be stored in this property."
    ]
  },
  {
    "Step Name": "Scheduler Server Component",
    "Step Details": "Detailed Description Adds Scheduler page to Admin Center and enables scheduling and running generic jobs. Scheduler component can be used only together with a Workflow Server Component .",
    "Step Properties": [
      "Disabled - Specifies whether component should be disabled.",
      "Resources Folder - Relative (to the server configuration file) or absolute location where scheduler stores its supporting files such as generator state-file, id mapper etc. It is also the location where job execution state files are stored (if the filesystem persister is used).",
      "Result Persister - Defines which backend should be used for storing the job execution states. Currently, there are two storage provider implementations: DB persister and filesystem persister.",
      "Sources - Defines a set of sources which will be used for reading the schedules."
    ]
  },
  {
    "Step Name": "Schema Discovery",
    "Step Details": "Detailed Description Relationships between two tables and their columns are represented by primary and foreign key pairs.\n\t\t\tThe discovery of these pairs is based on a simple fact that a set of distinct foreign key values is\talways\n\t\t\ta subset of a set of primary key values. While this condition is essential for the pair of a primary\n\t\t\tand a foreign key, it is not determinative. Many of the inclusion pairs may be false positives.\n\t\t\t\n\t\t\tThe step computes so called inclusion dependencies. An inclusion dependency is a relation between a referenced\n\t\t\tcolumn (a potential primary key) and a dependent column (a potential foreign key) that holds if distinct values\n\t\t\tof dependent column are all included in values of referenced column.\n\t\t\t\n\t\t\tThe step returns all inclusion dependencies where the referenced columns are unique.\n\t\t\t\n\t\t\tA leeway can be defined so that not all of the distinct values of a dependent column must be present in the referenced\n\t\t\tcolumn. This aims e.g. for cases with failed or missing cascading operations on tables. Note that specifying this value\n\t\t\tmay lead to an increase of the computation time.",
    "Step Properties": [
      "Id - Step identification string.",
      "Inputs - List of input sources on which the discovery is performed. Each element needs an \n\t\t\tappropriate input endpoint.",
      "Inclusion Leeway - Maximum percent of values of a dependent column that may not be included\n\t\t\t\tin the values of a referenced columns. It's computed against the number of\n\t\t\t\tdistinct values of the dependent column.",
      "Maximal Key Length - Maximum length of a column value for the column to be considered as a key. If a value of a column exceeds this bound,\n\t\t\t\tthe column will be eliminated from the computation."
    ]
  },
  {
    "Step Name": "Scoring",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Scoring",
    "Step Details": "Detailed Description This step analyzes input data according to given rules. The rule is\n\t\tactually an expression and some intervals against the expression value are checked.\n\t\tThe rules can be nested. This means that if some expression value falls into an interval\n\t\tthen it can be checked against a nested rule if there is one defined. Each interval can increment the score.\n\t\tThe scoring and explanation output columns can be defined at the step level.\n\t\tIf a finer scoring output is required, scoring columns can be defined for each interval.",
    "Step Properties": [
      "Id - Step identification string.",
      "When Condition - Expression that must be satisfied for this step to be executed.",
      "Rules - A set of rules. There can be multiple rules.",
      "Default Explain Column - The default explanation column. The explanation values are appended and are separated\n\t\t  \t\tby a space. This value will be overridden if explanationColumn is filled in.",
      "Default Score Column - The default scoring column. The scoring values are incremented.\n\t\t  \t\tThis value will be overridden if scoreColumn is filled in."
    ]
  },
  {
    "Step Name": "Task SCP Download File",
    "Step Details": "Detailed Description Downloads a remote file from a specified host via the SCP protocol.",
    "Step Properties": [
      "File or Directory - Remote path to the file/directory for download. A directory is downloaded into the Target Directory including its files (without recursion).",
      "Known Hosts Path - Local path to the known hosts file.",
      "Private Key Path - Local path to the private key for SSH authentication.",
      "Session Parameters - Set of parameters to customize a JSch session.Note:When connecting to server that still uses deprecatedssh-rsaalgorithm, session parameterserver_host_keymust be set tossh-rsa.",
      "Target Directory - Path to the destination directory. The directory can be local, on Amazon S3 server and on HDFS (if your product package contains Big Data Engine).",
      "Url Resource - Name of the URL resource to be used."
    ]
  },
  {
    "Step Name": "Task SCP Upload File",
    "Step Details": "Detailed Description Uploads a local file to a specified host via the SCP protocol.",
    "Step Properties": [
      "File or Directory - Path to the file or directory to upload. The file/directory can be local, on Amazon S3 server and on HDFS (if your product package contains Big Data Engine).A directory is uploaded into the Target Directory including its files (without recursion).",
      "Known Hosts Path - Local path to the known hosts file.",
      "Private Key Path - Local path to the private key for SSH authentication.",
      "Session Parameters - Set of parameters to customize a JSch session.Note:When connecting to server that still uses deprecatedssh-rsaalgorithm, session parameterserver_host_keymust be set tossh-rsa.",
      "Target Directory - Remote path to the destination directory.",
      "Url Resource - Name of the URL resource to be used."
    ]
  },
  {
    "Step Name": "Seed Table",
    "Step Details": "Detailed Description This step transforms the values in the replacement column. When a first record with a certain value comes, this step stores this value - replacement pair and\n\t\t\tfor all following records with the same value , the replacement is overwritten with the stored one. The storing\n\t\t\tonly occurs if both the value and replacement are non-empty. The pairs are stored in the database and therefore\n\t\t\tpersisted. The scorer values are as follows: ST_NULL - the value was empty ST_FOUND - the previously stored pair was found and therefore the replacement occurred ST_INSERTED - the pair was not found and therefore the current one was stored for further replacements ST_NOT_FOUND - the pair was not found, but couldn't be stored because the replacement was empty Only a single Seed Table step should be running over a single table at any given moment. For two or more concurrent steps over a single\n\t\t\ttable it is absolutely necessary to set Batch Size and Commit Size to 1. Note that this will cause significant performance drop.\n\t\t\tThe alternative of this is to ensure that the value sets for each of the concurrent steps are disjoint, i.e. there is no value which occurs in the data sets for more than one of these concurrent steps. The name of the table that will be used for persisting the value-replacement tuples is derived from a global prefix \"ST\" and Domain Prefix\n\t\t\tand Table Name properties. The table will be created if it doesn't exist. If it does exist, it has to have the proper format -\n\t\t\tit has to contain a column named value and a column named replacement , both of a correct type to store strings \n\t\t\t(char/varchar/varchar2) with the maximum length equal to the step property Maximum String Length value. Index on the column value is not needed, but highly recommended.",
    "Step Properties": [
      "Value - Thevaluecolumn containing the keys for the transformation.",
      "Replacement - Thereplacementcolumn containing the values that will be replaced or used as a replacement value for next records.",
      "Data Source Name - Name of the data source.",
      "Domain Prefix - The name of the table that will be used for persisting the value-replacement tuples is derived from a global prefix \"ST\" and Domain Prefix\n\t\t\t\tand Table Name properties. The table will be created if it doesn't exist. If it does exist, it has to have the proper format (see step description).",
      "Table Name - The name of the table that will be used for persisting the value-replacement tuples is derived from a global prefix \"ST\" and Domain Prefix\n\t\t\t\tand Table Name properties. The table will be created if it doesn't exist. If it does exist, it has to have the proper format (see step description).",
      "Batch Size - The size of the batches in which the transformation is done. For transformation of a single record at a time, this has to be\n\t\t\t\tset to 1 to avoid unnecessary waiting for following records. For transformation of a large number of records at once, setting this\n\t\t\t\tto a larger number will improve performance.",
      "Commit Size - The number of inserts to database after which a commit is done.",
      "Maximum String Length - The maximum length of both thevalueandreplacementstring (each separately, not together). May depend on the\n\t\t\t\ttarget database charset.",
      "Scorer - Element which stores basic scoring settings.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Selective Transliterate",
    "Step Details": "Detailed Description Transforms a specified set of characters from the input column \"in\" to another character set,\n                and sends the result text to the output column \"out\".\n\n                The transformation is applied to words. A new word starts either at the beginning of the whole input\n                text or after a switch between a sequence of digits+letters and a sequence of special symbols (or vice versa) was detected.\n                A word is defined as a successive sequence of digits+letters or as a successive sequence of special symbols.\n\n                Transformation replaces characters occurring in the \"from\" string by characters at the corresponding positions\n                in the \"to\" string.\n\n                The words which are either shorter than the \"minWordLength\" parameter value or where transformation would break conditions\n                defined by \"maxChangeRatio\" and \"maxConsecutiveChanges\", stay unchanged.",
    "Step Properties": [
      "From - A string defining input characters of the transformation.",
      "In - Column providing input data.",
      "Max Change Ratio - Maximum ratio of the replacement count to the input word length. Default value is 0.5.",
      "Max Consecutive Changes - Maximum count of successive replacements in one word. Default value is 2.",
      "Min Word Length - Minimum input word length. Default value is 3.",
      "Out - Column consuming output data.",
      "To - A string defining output characters of the transformation. The length must be the same as it is in the \"from\" parameter value.",
      "When Condition - Expression that must be satisfied for this step to be executed.",
      "Id - Step identification string.",
      "Scorer - Element which stores basic scoring settings."
    ]
  },
  {
    "Step Name": "Selector",
    "Step Details": "Detailed Description",
    "Step Properties": [
      "Record Descriptor Column - Name of string input column in which special recordDescriptor is stored.\n\t\t\t\tThe descriptor contains group id, group size and record number in group as\n\t\t\t\tthree numbers separated by colon. For example4152:3:2.",
      "Selection Rules - List of comparisons for select the best (first) record.",
      "Assignments - List of assignments that will be performed on selected record.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Sendmail",
    "Step Details": "Detailed Description Many of string parameters in this element are templates . Templates may\n\t\t\tcontain placeholders for inserting column value obtained from input record. The\n\t\t\tplaceholder is coded as {name} , where name is column\n\t\t\tname. Curly brace itself must be escaped by backslash.\n\t\t\tFor example: template {first_name} {last_name}, \\{{age}\\} can result\n\t\t\tin text John Smith, {34} .",
    "Step Properties": [
      "Templates - List of templates. Each template can have condition when will be sent.",
      "Max Sent - Maximal number of sent mails.Default = 100.",
      "Smtp Server - Name of the SMTP server (defined under Servers in File Explorer). For server deployments and command line usage, the server connection name should be present in Runtime Configuration.Note: To enable sending emails from a server secured with TLS/SSL, run the plan with the-Dmail.smtp.auth=trueJVM parameter.",
      "From - Default source e-mail address.",
      "Charset - Specifies encoding of national characters and will be used\n\t\t\t\t(if necessary) in coding to, from and subject headers and body of mail.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Sentence Tokenizer",
    "Step Details": "Detailed Description Step creates a new record for each sentence in the input string. OpenNLP model files are necessary for this step. Model files can be trained with the SentenceTokenizerTrainer step.",
    "Step Properties": [
      "Id - Step identification string.",
      "Input Column - Column containing input sentences.",
      "Model File - OpenNLP SentenceDetectorModel file.",
      "Output Column - Column for output tokens.",
      "Record Descriptor Column - Record descriptor column."
    ]
  },
  {
    "Step Name": "Sentence Tokenizer Trainer",
    "Step Details": "Detailed Description Step creates the model for SentenceTokenizer . Proper training input format is one sentence per record. End of document is signalized by an empty record. If document boundary is unknown, it is recommended to use an empty record every 10 sentences.",
    "Step Properties": [
      "Abbreviations - Known abbreviations column.",
      "Cutoff - The minimal number of times a feature must be seen, otherwise it is ignored.",
      "Eos Characters - End of sentence characters.",
      "Id - Step identification string.",
      "Input Values - Training input column.",
      "Iterations - Number of training iterations.",
      "Model File - Output model file.",
      "Use Token End - If checked, sentences are not split in the middle of tokens. E.g. \"correct answers here,but there\" would be split as \"correct answers here,but<SPLIT>there\", not \"correct answers here,<SPLIT>but there\"."
    ]
  },
  {
    "Step Name": "Sequence Generator",
    "Step Details": "Detailed Description Step generates sequence of numbers, using several strategies for restarting\n\t\t\tand sharing.",
    "Step Properties": [
      "Column - Column ofINTEGERorLONGtype that stores the sequence number.",
      "Start - First assigned number.Default = 0.",
      "Step - Difference between numbers.Default = 1.",
      "Strategy - Strategy for sharing/restaring of sequence.",
      "Name - Sequence name. Used inGLOBALorREQUESTstrategy only\n\t\t\t\tto distinguish or share sequences on several places of plan.\n\t\t\t\tEach sequence use is identified by unique start/step/name.",
      "When Condition - ",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Server Components",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Server Config Bean",
    "Step Details": "Detailed Description Server configuration parameters.",
    "Step Properties": [
      "Port - Port to run.",
      "Runtime Configuration - Path to a runtime configuration file.",
      "Server Components - List of server components.",
      "Temp Folders - List of temporary folders.",
      "Disabled - Specifies whether component should be disabled."
    ]
  },
  {
    "Step Name": "Server Filesystem Service Component",
    "Step Details": "Detailed Description Enables access to a remote (server) filesystem from the GUI and defines root folders for the remote filesystem (at least one root must be defined).",
    "Step Properties": [
      "Disabled - Specifies whether component should be disabled.",
      "Roots - Defines root folders for the remote filesystem.Defined paths:are not restricted any way (e.g. you can refer toc:/)are either absolute or relative (to the server configuration file) pathssupport path variables from the runtime configuration."
    ]
  },
  {
    "Step Name": "Server Metrics Web Console",
    "Step Details": "Detailed Description Adds Server Metrics section to the Admin Center.",
    "Step Properties": [
      "Config File - Relative (to the server configuration file) or absolute path to the configuration file.",
      "Disabled - Specifies whether component should be disabled."
    ]
  },
  {
    "Step Name": "Installing server as service",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Server Configuration",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Task SFTP Download File",
    "Step Details": "Detailed Description Downloads a remote file from a specified host via the SFTP protocol.",
    "Step Properties": [
      "File or Directory - Remote path to the file/directory for download. A directory is downloaded into the Target Directory including its files (without recursion).",
      "Known Hosts Path - Local path to the known hosts file.",
      "Private Key Path - Local path to the private key for SSH authentication.",
      "Session Parameters - Set of parameters to customize a JSch session.Note:When connecting to server that still uses deprecatedssh-rsaalgorithm, session parameterserver_host_keymust be set tossh-rsa.",
      "Target Directory - Path to the destination directory. The directory can be local, on Amazon S3 server and on HDFS (if your product package contains Big Data Engine).",
      "Url Resource - Name of the URL resource to be used."
    ]
  },
  {
    "Step Name": "Task SFTP Upload File",
    "Step Details": "Detailed Description Uploads a local file to a specified host via the SFTP protocol.",
    "Step Properties": [
      "File or Directory - Path to the file or directory to upload. The file/directory can be local, on Amazon S3 server and on HDFS (if your product package contains Big Data Engine).A directory is uploaded into the Target Directory including its files (without recursion).",
      "Known Hosts Path - Local path to the known hosts file.",
      "Private Key Path - Local path to the private key for SSH authentication.",
      "Session Parameters - Set of parameters to customize a JSch session.Note:When connecting to server that still uses deprecatedssh-rsaalgorithm, session parameterserver_host_keymust be set tossh-rsa.",
      "Target Directory - Remote path to the destination directory.",
      "Url Resource - Name of the URL resource to be used."
    ]
  },
  {
    "Step Name": "Cluster step",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Cluster",
    "Step Details": "Detailed Description See detailed description",
    "Step Properties": [
      "Config File Name - File path to the configuration plan file which will be run on the\n\t\t\texecution nodes.",
      "Chunk Size - The input will be split by chunk size counted in records. The number\n\t\t\tof records can be slightly bigger if the chunk size is chosen very small (<100).",
      "Group - Execution nodes can be configured to allow run only listed groups. Only\n\t\t\tnodes with this group can be used to run the plan.",
      "Registry - Name of the url resource pointing to the registry node.",
      "Compressed Chunks - The chunks of records sent to the execution and received back can be \n\t\t\tcompressed.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Simple Data Sampler",
    "Step Details": "Detailed Description Simple version of Data Sampler step. Using this step it is possible to define a size of the final data sample. Data sampling is made based on selected mode: FIRST - selection of first Count records from input RANDOM - random selection of Count records from input",
    "Step Properties": [
      "Mode - Mode of data sampling. Random or sequential sampling.",
      "Count - Number of records in the data sample.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Simple Group Classifier",
    "Step Details": "Detailed Description This step works with groups of records usually acquired from the Unification process. Utilizing predefined rules, which compare members of the same group,\n\t\t\tthe overall group quality can be evaluated.\n\t\t\tGroups are defined by the value in Group Id Column . If this value is empty ( null ), the record in question defines its own single member group\n\t\t\twith a group role of 'C'. The following group qualities are defined: U - single member group of A quality. A - best quality group (records can be automatically assigned to single entity). M - minor differences in group (recommended for manual control). C - major differences in group or empty group id (recommended for manual control and correction).",
    "Step Properties": [
      "Default Locale - Default locale to be used.",
      "Id - Step identification string.",
      "Group Id Column - Column or expression that contains group identifiers (groupId) for each record.",
      "Role Column - Column that stores group quality.",
      "Column Sets - Rule list for group evaluation. Group quality is evaluated by the first columnSet member with a satisfying condition (\"when\" property).",
      "Already Grouped - Informs the process that the input record flow is already grouped by a used groupId and so it's unnecessary to group it again.\n\t\t\t\tThis switch is usable in the common case that the group classifier follows a unification step and its groupId corresponds to \n\t\t\t\tthis unification's candidate or matching group ids.Default = false."
    ]
  },
  {
    "Step Name": "Simple Profiling",
    "Step Details": "",
    "Step Properties": [
      "Columns - Contains definitions of columns which will be read from the input file.",
      "Default Locale - Locale represents a specific geographical, political, or cultural region, with respect to data parsing and comparison as performed by the step.Default value:en_US",
      "Domain Params - Specifies which and how domain analysis will be performed.",
      "Id - Step identification string.",
      "Masks - List of mask definitions.",
      "Nested Flow Params - Parameters that specifies implementation used for expression evaluation to be used (Roll Up, Condition, None).",
      "Output Sources With Result - Filename of the result file where the profile results will be stored.",
      "Params Graph Output File - Output file contains results, transformations and graphs.",
      "Params Output File - Output file contains results and profiling parameters.",
      "Result Output File - File with resulting profiled data.",
      "Sampling Params - Sampling Parameters."
    ]
  },
  {
    "Step Name": "Simple Scoring",
    "Step Details": "Detailed Description Passes through all defined scoring cases and evaluates their conditions.\n\t\t\tIf the condition is true it appends the explanation to the\n\t\t\texplanationColumn and increments the value in scoreColumn.",
    "Step Properties": [
      "Id - Step identification string.",
      "Scoring Cases - Cases that are evaluated.",
      "Default Explain Column - The default explanation column. The explanation values are appended and are separated\n\t\t  \t\tby a space. This value will be overridden if explanationColumn is filled in.",
      "Default Score Column - The default scoring column. The scoring values are incremented.\n\t\t  \t\tThis value will be overridden if scoreColumn is filled in.",
      "When Condition - Expression that must be satisfied for this step to be executed."
    ]
  },
  {
    "Step Name": "SingleFlowWrap",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "SingleInputWrap",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Single Online Services Component",
    "Step Details": "Detailed Description Initializes and deploys a service which should be available for online requests. The component is a simplified version of an Online Services Component and is used to start online services from GUI during the plan testing and debugging. Difference from Online Services Component : the component does not search a directory, instead it expects a single *.online file.",
    "Step Properties": [
      "Disabled - Specifies whether component should be disabled.",
      "Listeners - Comma-separated list of names of HTTP listeners to which all service handlers should be registered. If the attribute is missing, all services will be deployed on all listeners.",
      "Service Config File Name - Relative (to the server configuration file) or absolute path to the*.onlineconfiguration file that contains definition of an online service.",
      "Service Lookup Folders - Relative (to the server configuration file) or absolute path to the file system folder(s) which contain all necessary configuration files, i.e. the*.onlinefiles that contain definition of online services. All*.onlinefiles from the configuration folder are processed and the defined services started."
    ]
  },
  {
    "Step Name": "SIN Validator",
    "Step Details": "Detailed Description Tests the input string as a SIN (Social Insurance Number - Canadian personal ID) and verifies its validity.\n\t\t\tA valid SIN is a 9-digit number which fulfills a specific check sum. All non-digit characters that appear in the input string\n\t\t\tare removed before the actual validation - only digits are processed.",
    "Step Properties": [
      "When Condition - Expression that must be satisfied for this step to be executed.",
      "Id - Step identification string.",
      "Omit Invalid SIN - Flag that determines whether invalid SIN numbers should be replaced in the output bindingSin Outby an empty string (flag set totrue) or written to the output (flag set tofalse).Default value:False",
      "Preserve Input Value - Flag that determines whether the original SIN value is written to the the output bindingSin Out(flag set totrue) or the corrected SIN value is written to the output (flag set tofalse).Default value:False",
      "Sin - Column that contains SIN numbers to be verified.",
      "Sin Out - Column that stores processed SIN numbers.",
      "Scorer - Element which stores basic scoring settings."
    ]
  },
  {
    "Step Name": "SleepStep",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Smart Swap Name Surname",
    "Step Details": "Detailed Description Swaps the first name and the last name in the input ( First Name and Last Name ) string. The swapping\n\t\t\tonly occurs if it is applicable according to the contents of defined dictionaries (i.e., first name string\n\t\t\tis found only in the last names dictionary and vice versa - for detailed settings see the example).\n\t\t\t\n\t\t\tIf it is not possible to determine which is the first name and which is the last name,\n\t\t\tthe original order is preserved. Behavior of the step can be changed using the table of actions, which is\n\t\t\tdefined in the element \"actions\" (see properties of the step).",
    "Step Properties": [
      "When Condition - Expression that must be satisfied for this step to be executed.",
      "Id - Step identification string.",
      "Actions - List of output flags and actions that should be applied to the input data in defined situations.",
      "First Name - Column that contains the input first name.",
      "First Name Lookup File Name - Dictionary file that contains known first names.\n\t\t\t\tFor more information about the dictionary seethe detailed description here.",
      "First Name Out - Column that stores the final determined first name.",
      "Last Name Lookup File Name - Dictionary file that contains known last names.\n\t\t\t\tFor more information about the dictionary seethe detailed description here.",
      "Last Name - Column that contains the input last name.",
      "Last Name Out - Column that stores the final determined last name.",
      "Scorer - Element which stores basic scoring settings."
    ]
  },
  {
    "Step Name": "Soap Call",
    "Step Details": "Detailed Description The step calls soap service installed on the URL. The request is built using Input Template where column names in curly brackets are replaced by values in the\n\t\t\tcolumns. The response is expected to be in XML form and therefore the\n\t\t\tvalues can be extracted\n\t\t\tusing XPATH expressions. First root XML\n\t\t\telement for further processing is\n\t\t\textracted using Root Xpath . Then for each attribute from Attributes a value is extracted\n\t\t\tusing Xpath and the extracted value is written into the\n\t\t\tcorresponding column.",
    "Step Properties": [
      "Attributes - Columns filled from the response.",
      "Namespaces - It defines prefixes and corresponding namespaces which can be\n\t\t\t\tused in\n\t\t\t\tthe XPATH\n\t\t\t\texpressions. Implicitly soap prefix is added.",
      "Data Format Parameters - General parameters for data formatting. This configuration is\n\t\t\t\tapplied to\n\t\t\t\tall specified columns unless the column defines its ownData Format Parameterssection.\n\t\t\t\tFor more information, seeDataFormatParameters",
      "Http Request Headers - Additional HTTP header fields attached to the request. All inserted\n\t\t\t\tHTTP header fields overwrite existing HTTP headers (including\n\t\t\t\t\"soapAction\"...). If the header field doesn't exist yet, it is\n\t\t\t\tappended.\n\t\t\t\tFor description seeHttpRequestHeaderDefinition",
      "Encoding - The request data encoding. You can use all encoding sets that supported by the target Java platform. Most commonly used encoding sets are: ISO-8859-1, ISO-8859-2, and UTF-8.",
      "Url - Name of the URL\n\t\t\t\tpointing the soap service.",
      "Wsdl Url - URL where WSDL\n\t\t\t\tschema can be obtained for the called soap service. It\n\t\t\t\tis used\n\t\t\t\tin the\n\t\t\t\tGUI to generate request template. The step does not evaluate\n\t\t\t\tthis URL\n\t\t\t\tin any way (except validation).",
      "Soap Action - The soap\n\t\t\t\taction name.",
      "Input Template - The template contains full soap request. The request can contain\n\t\t\t\tcolumns wrapped by curly brackets. The columns are replaced by\n\t\t\t\tcolumn values from current input record. Example:<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<soap:Envelope\n        xmlns:soap=\"http://schemas.xmlsoap.org/soap/envelope/\"\n        xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n        xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\"\n        xmlns:ns1=\"http://www.example.com/ws\">\n  <soap:Body>\n    <ns1:component>\n      <ns1:SRC_ACCOUNT_ID>{input_column_name}</ns1:SRC_ACCOUNT_ID>\n    </ns1:component>\n  </soap:Body>\n</soap:Envelope>If the column used in the template contains XML, the output can be escaped as follows:<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<soap:Envelope\n        xmlns:soap=\"http://schemas.xmlsoap.org/soap/envelope/\"\n        xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n        xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\"\n        xmlns:ns1=\"http://www.example.com/ws\">\n  <soap:Body>\n    <ns1:component>\n      <ns1:SRC_ACCOUNT_ID>\n      <?javax.xml.transform.disable-output-escaping ?>\n      {input_column_name}\n      <?javax.xml.transform.enable-output-escaping ?>\n      </ns1:SRC_ACCOUNT_ID>\n    </ns1:component>\n  </soap:Body>\n</soap:Envelope>",
      "Root Xpath - XPATH\n\t\t\t\texpression which extracts the starting element in which values\n\t\t\t\tfor\n\t\t\t\tcolumns are searched.",
      "Error Column - If some errors\n\t\t\t\toccurred then they can be stored into this column.",
      "Error Length Limit - Maximum length of value stored to error column. Pass negative number to always store complete error.",
      "Timeout (ms) - If the call of the service takes longer time than theTimeout (ms)then\n\t\t\t\tthe call is aborted.",
      "Delay Between Requests (ms) - Sometimes when calling an outside service it is needed to make some pauses\n\t\t\t\tbetween requests in order not to get banned for DOS attack. The value is in milliseconds.",
      "Soap Version - Version of the soap service.",
      "Debug Response File Name - The responses\n\t\t\t\tcan be written into the file. If the file name contains\n\t\t\t\tpercent\n\t\t\t\tcharacter\n\t\t\t\tthen it is replaced by the number of the record for which\n\t\t\t\tthe response\n\t\t\t\twas generated. If\n\t\t\t\ta file with the same name exists then\n\t\t\t\tit is rewritten.",
      "Debug Request File Name - The same likeDebug Response File Namebut for the request.",
      "Id - Step identification string.",
      "Scorer - Element which stores basic scoring settings.",
      "When Condition - Expression that must be satisfied for this step to be executed."
    ]
  },
  {
    "Step Name": "Soap Multi Call",
    "Step Details": "Detailed Description The step calls soap service installed on the url. The request is built using Input Template where column names in curly brackets are replaced by values in the\n\t\t\tcolumns. The response is expected to be in XML form and therefore the\n\t\t\tvalues can be extracted\n\t\t\tusing XPATH expressions. First root XML\n\t\t\telement for further processing is\n\t\t\textracted using Root Xpath .\n\n\t\t\tIt can create several output streams from each soap request where\n\t\t\trows\n\t\t\tof one stream can be\n\t\t\tlogically children to rows from another\n\t\t\tstream of rows. E. g. soap request can\n\t\t\tcontain records\n\t\t\tof clients and\n\t\t\teach client have several addresses and then the XML file\n\t\t\tcan be read\n\t\t\tas a stream of clients and for each client there can be read\n\t\t\taddresses\n\t\t\twhere addresses\n\t\t\tare put to another stream. The streams can\n\t\t\talso be independent. The\n\t\t\tnamespaces official\n\t\t\tspecification can be a\n\t\t\tlittle tricky and therefore if they are used it is good to\n\t\t\tread namespace specification .\n\t\t\tEach output stream can have unique record descriptor assigned in\n\t\t\tformat\n\t\t\t\"unique_number_of_group:count_of_records_in_group:order_in_group\" and\n\t\t\tthese record descriptors\n\t\t\tcan be accessed by child output streams in\n\t\t\tshadows columns and therefore\n\t\t\tthe parent and\n\t\t\trelated children streams\n\t\t\tcan be joined using the record descriptor columns.\n\t\t\tThe input columns\n\t\t\tare also accessible in the shadow columns. If some\n\t\t\tsoap request fails\n\t\t\tthen\n\t\t\ta record describing error with added input columns is sent to\n\t\t\t\"error\"\n\t\t\tend point.",
    "Step Properties": [
      "Namespaces - It defines prefixes and corresponding namespaces which can be\n\t\t\t\tused in\n\t\t\t\tthe XPATH\n\t\t\t\texpressions. Implicitly soap prefix is added.",
      "Data Streams - Output end points defined on the root level.",
      "Data Format Parameters - General parameters for data formatting. This configuration is\n\t\t\t\tapplied to\n\t\t\t\tall specified columns unless the column defines its ownData Format Parameterssection.\n\t\t\t\tFor more information, seeDataFormatParameters",
      "Http Request Headers - Additional HTTP header fields attached to the request. All inserted\n\t\t\t\tHTTP header fields overwrite existing HTTP headers (including\n\t\t\t\t\"soapAction\"...). If the header field doesn't exist yet, it is\n\t\t\t\tappended.\n\t\t\t\tFor description seeHttpRequestHeaderDefinition",
      "Encoding - The request data encoding. You can use all encoding sets that are supported by the\n\t\t\t\ttarget Java platform. Most commonly used encoding sets are: ISO-8859-1, ISO-8859-2, and UTF-8.",
      "Url - Name of the URL\n\t\t\t\tresource pointing to the soap service.",
      "Wsdl Url - URL where WSDL\n\t\t\t\tschema can be obtained for the called soap service. It\n\t\t\t\tis used\n\t\t\t\tin the\n\t\t\t\tGUI to generate request template. The step does not evaluate\n\t\t\t\tthisURL\n\t\t\t\tin any way (except validation).",
      "Soap Action - The soap\n\t\t\t\taction name.",
      "Input Template - The template contains full soap request. The request can contain\n\t\t\t\tcolumns wrapped by curly brackets. The columns are replaced by\n\t\t\t\tcolumn values from current input record. Example:<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<soap:Envelope\n        xmlns:soap=\"http://schemas.xmlsoap.org/soap/envelope/\"\n        xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n        xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\"\n        xmlns:ns1=\"http://www.example.com/ws\">\n  <soap:Body>\n    <ns1:component>\n      <ns1:SRC_ACCOUNT_ID>{input_column_name}</ns1:SRC_ACCOUNT_ID>\n    </ns1:component>\n  </soap:Body>\n</soap:Envelope>If the column used in the template contains XML, the output can be escaped as follows:<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<soap:Envelope\n        xmlns:soap=\"http://schemas.xmlsoap.org/soap/envelope/\"\n        xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n        xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\"\n        xmlns:ns1=\"http://www.example.com/ws\">\n  <soap:Body>\n    <ns1:component>\n      <ns1:SRC_ACCOUNT_ID>\n      <?javax.xml.transform.disable-output-escaping ?>\n      {input_column_name}\n      <?javax.xml.transform.enable-output-escaping ?>\n      </ns1:SRC_ACCOUNT_ID>\n    </ns1:component>\n  </soap:Body>\n</soap:Envelope>",
      "Root Xpath - XPATH\n\t\t\t\texpression which extracts the starting element in which values\n\t\t\t\tfor\n\t\t\t\tcolumns are searched.",
      "Timeout (ms) - If the call of the service takes longer time than theTimeout (ms)then\n\t\t\t\tthe call is aborted.",
      "Delay Between Requests (ms) - Sometimes when calling an outside service it is needed to make some pauses\n\t\t\t\tbetween requests in order not to get banned for DOS attack. The value is in milliseconds.",
      "Soap Version - Version of the soap service.",
      "Debug Response File Name - The responses\n\t\t\t\tcan be written into the file. If the file name contains\n\t\t\t\tpercent\n\t\t\t\tcharacter\n\t\t\t\tthen it is replaced by the number of the record for which\n\t\t\t\tthe response\n\t\t\t\twas generated. If\n\t\t\t\ta file with the same name exists then\n\t\t\t\tit is rewritten.",
      "Debug Request File Name - The same likeDebug Response File Namebut for the request.",
      "Error Column - Name of column\n\t\t\t\tin the \"error\" end point where soap fault messages are\n\t\t\t\twritten.",
      "Error Length Limit - Maximum length of value stored to error column. Pass negative number to always store complete error.",
      "Id - Step identification string.",
      "Scorer - Element which stores basic scoring settings."
    ]
  },
  {
    "Step Name": "SOAP and XML services",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Sort",
    "Step Details": "Detailed Description Sorting step that will sort according to a specified key.",
    "Step Properties": [
      "Id - Step identification string.",
      "Default Locale - Locale definition represents a specific geographical, political, or cultural region, with respect to data parsing and comparison as performed by the step.",
      "Sorting Key - Sort column/key definition; column list inorderBy."
    ]
  },
  {
    "Step Name": "Spark RDD Repartition",
    "Step Details": "Detailed Description Reshuffles the data in the RDD randomly to create either more or fewer partitions and balance it across them. This always shuffles all data over the network.",
    "Step Properties": [
      "Partitions - Target number of RDD partitions or value interpreted regarding topropertyMeaning.",
      "Property Meaning - The value ofpartitionsproperty is interpreted by this option.EXACT - Value is target number of partitions.\n\t\t\t\t\tMULTIPLY - Number of partitions is multiplied by the value (may be < 1).\n\t\t\t\t\tADD - Number of partitions is increased by the value (may be negative).\n\t\t\t\t\tSIZE - The value is estimated size of target partitions (in MB). If not specified or zero, global executor's propertyspark.ata.partSizewill be used",
      "Use Coalesce - Use Coalesce instead of repartitioning. Applicable only when number of partitions is decreased.",
      "Id - Step identification string.",
      "When Condition - Expression that must be satisfied for this step to be executed."
    ]
  },
  {
    "Step Name": "Spark Reader",
    "Step Details": "Detailed Description Spark Reader works only in Spark mode.\n        \tThis step represents a 1 to 1 mapping of Spark DataFrame API in order to bypass limitations of built-in Ataccama Reader steps, like JDBC Reader, Parquet Reader, etc.\n        \tVia this step you can bypass additional options to the Spark DataFrame Reader or utilize external Spark connectors for data sources that are not built-in in Ataccama tools. For example, you can utilize the following external 3rd-party connectors: Spark BigQuery Connector . Spark Snowflake Connector . Spark Cassandra Connector . Parallel JDBC Reader for Spark engine . Please note, since these connectors are open-sourced and originating from 3rd-party providers, Ataccama Software is not responsible for their functionality and stability in production environments.",
    "Step Properties": [
      "Path - Path to the file/folder to be read by Spark Reader step. By default, it resolves the relative path to user HOME folder and reads the data from cluster file system, e.g. HDFS or DBFS.\n\t\t\t\tYou can also use the following path prefixes:Use path with prefix '/' to resolve the path from root, e.g. /data/ínput/table/Use path with prefix 'file://' to read a file on your local machine, e.g. file://data/ínput.csvUse path with prefix 'resource://' to read the data from Google Cloud Storage, Azure Data Lake Storage or S3, e.g. resource://adls/data/table",
      "Format - Name of the data source format. e.g. \"parquet\", \"orc\", \"csv\". It is also possible to use custom Spark connectors in order to read data from other data sources, e.g. \"BigQuery\". Make sure that you have added proper spark connector for your datasource in classpath.",
      "Options - List of Spark data source options that can be used with specified \"format\". Full list of supported options for specific can be found on Spark website for built-in connectors or on the web page of custom connectors.",
      "Columns - Definition of table columns (corresponding by order, names and types).",
      "Shadow Columns - Definition of shadow columns.",
      "Options Encrypted - List of Spark data source encrypted options that can be used with specified \"format\". Full list of supported options for specific can be found on Spark website for built-in connectors or on the web page of custom connectors.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Spark SQL Reader",
    "Step Details": "Detailed Description This step reads data records from the results of a specified query executed on a specified Spark SQL data source.\n\t\t\t\tAs a data source, it is using the environment which is used for running the plan. SQL Function getLatestPartition() This SQL function gets a value of the last partition in a Hive Databricks table. getLatestPartition([database,] table [, level]) database : name of the database, optional, default value is default table : name of the table for which it should return the partition level : level of table partition, default is \"1\", i.e. the top level partition How to Use When the partition column is a type of bigint . CREATE TABLE customers (customer_id int,customer_name string) PARTITIONED BY (load_dt bigint);\nselect * from customers where load_dt = ##getLatestPartition(default, customers)## \nwill be replaced by \nselect * from customers where load_dt = 20190101 When the partition column is a type of string . CREATE TABLE customers (customer_id int, customer_name string) PARTITIONED BY (load_dt string);\nselect * from customers where load_dt > \"##getLatestPartition(default, prices_2019)##\"\nwill be replaced by \nselect * from customers where load_dt = \"20190101\" Two partitions CREATE TABLE customers (customer_id int, customer_name string) PARTITIONED BY (load_dt string, load_hours string);\nselect * from customers where load_dt = \"##getLatestPartition(default, customers)##\" and load_hours = \"##getLatestPartition(default, customers, 2)##\"\nwill be replaced by \nselect * from customers where load_dt = \"20190101\" and load_hours = \"0100\" How to Enable Custom SQL query is currently considered as an experimental feature and is disabled by default. To turn on the functionality, the following property has to added to spark configuration (e.g. spark.properties file): spark.ata.enableCustomSqlTemplates=true",
    "Step Properties": [
      "Id - Step identification string.",
      "Columns - Contains definitions of columns which will be constructed from the SQL query result set.",
      "Query File Encoding - Encoding of the query file if such a file is used.",
      "Query File Name - When defined, it represents the name of the file that contains the query to execute.\n\t\t\t\tWhen this attribute is filled in, the attributeQuery Stringmust not contain\n\t\t\t\tany value, otherwise a query conflict error is reported.",
      "Query String - SQL query to execute on the data source to obtain input data records. Only selection operations are supported.",
      "Shadow Columns - Contains a set of columns that are not present in the input data, but should be\n\t\t\t\tcreated in the output (so that they are then available for further use as any\n\t\t\t\tother \"real\" column read from a file)."
    ]
  },
  {
    "Step Name": "Spark Writer",
    "Step Details": "Detailed Description Spark Writer works only in Spark mode.\n        \tThis step represents a 1 to 1 mapping of Spark DataFrame API in order to bypass limitations of built-in Ataccama Writer steps, like JDBC Writer, Parquet Writer, etc.\n        \tVia this step you can bypass additional options to the Spark DataFrame Writer or utilize external Spark connectors for data sources that are not built-in in Ataccama tools. For example, you can utilize the following external 3rd-party connectors: Spark BigQuery Connector . Spark Snowflake Connector . Spark Cassandra Connector . Parallel JDBC Reader for Spark engine . Please note, since these connectors are open-sourced and originating from 3rd-party providers, Ataccama Software is not responsible for their functionality and stability in production environments.",
    "Step Properties": [
      "Data Type Parameters - Data Type Parameters. A Decimal that must have fixed precision (the maximum number of digits) and scale (the number of digits on right side of dot).",
      "Bucket Options - Buckets the output by the given columns. If specified, the output is laid out on the file system similar to Hive's bucketing scheme. This is applicable for all file-based data sources (e.g. Parquet, JSON) starting with Spark 2.1.0.",
      "Partition Options - Partitions the output by the given columns on the file system. If specified, the output is laid out on the file system similar to Hive's partitioning scheme.",
      "Save Options - Save Options is used to specify the expected behavior of saving a DataFrame to a data source.",
      "Write All Columns - Specifies whether to write out all columns as defined in the input format. This attribute is exclusive to column definitions. If this value is set to true there must be no columns defined in the columns element, otherwise an error is reported.",
      "Id - Step identification string.",
      "Save Path - Path to the file/folder which should be read by Spark Reader step. By default, it is reading the data from cluster file system, e.g. HDFS. If you need to read the data from Google Cloud Storage, Azure Data Lake Storage or S3, you can use resources.",
      "Save Mode - Spark DataFrameSave Modethat will be used for writing the data set.ERRORIFEXISTS: ErrorIfExists mode means that when saving a DataFrame to a data source, if data already exists, an exception is expected to be thrown.APPEND: Append mode means that when saving a DataFrame to a data source, if data/table already exists, contents of the DataFrame are expected to be appended to existing data.OVERWRITE: Overwrite mode means that when saving a DataFrame to a data source, if data/table already exists, existing data is expected to be overwritten by the contents of the DataFrame.IGNORE: Ignore mode means that when saving a DataFrame to a data source, if data already exists, the save operation is expected to not save the contents of the DataFrame and to not change the existing data.",
      "Format - Name of the data source format. e.g. \"parquet\", \"orc\", \"csv\". It is also possible to use custom Spark connectors in order to read data from other data sources, e.g. \"BigQuery\". Make sure that you have added proper spark connector for your data source in classpath.",
      "Options - List of Spark data source options that can be used with specified \"format\". Full list of supported options for specific can be found on Spark website for built-in connectors or on the web page of custom connectors.",
      "Options Encrypted - List of Spark data source encrypted options that can be used with specified \"format\". Full list of supported options for specific can be found on Spark website for built-in connectors or on the web page of custom connectors.",
      "Columns - Definition of table columns (corresponding by order, names and types).",
      "Shadow Columns - Definition of shadow columns."
    ]
  },
  {
    "Step Name": "SplitStep",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Splitter",
    "Step Details": "Detailed Description This step splits the input into separate words. For each of these words (created by splitting the input) it\n\t\t\tcreates a new record. Each of these new records is an exact copy of the input record with the only difference\n\t\t\tbeing the column containing the words created by splitting the input.\n\t\t\tThe All Sentence Column property is an input expression for the splitting and\n\t\t\tthe One Word Column property is an output column for the split words. All other columns from the input are automatically copied to the output\n\t\t\tunchanged. The Separator Config property defines which characters should be considered delimiters.",
    "Step Properties": [
      "Id - Step identification string.",
      "All Sentence Column - Column containing the input string to be split.",
      "One Word Column - Column containing split output words.",
      "Separator Config - Defines the set of characters to serve as delimiters. By default all characters except letters and digits serve as a separator.",
      "Record Descriptor Column - Name of the column of type string to store the identification of record in group. The identifier\n\t\t\t\thas the form<group_id>:<record_count>:<record_number>."
    ]
  },
  {
    "Step Name": "SQL Execute",
    "Step Details": "Detailed Description Executes a defined SQL command/procedure call over the input data and writes its output values.\n\t\t\tThis step performs some write-like SQL commands for each input row. It provides attributes\n\t\t\tto affect batch size and commit size. Usage notes This step is suitable for write-like operations only (must NOT return a result set). \n\t\t\t\t\t  When there is a result set as the result of query execution, an error is thrown. \n\t\t\t\t\t  This is considered a configuration problem that should be be fixed. For read-like \n\t\t\t\t\t  operations use the SQL Select step. Because it is not possible now to analyze a SQL command without being run, some \n\t\t\t\t\tproblems are reported after the first result set is obtained (validation of the \n\t\t\t\t\tresult of the SQL query). Note that when an error is thrown at this moment, then \n\t\t\t\t\tthe state is as follows: 1) possible beforeScript is executed and \n\t\t\t\t\tcommited 2) query is commited only if connection enforces autocommit mode \n\t\t\t\t\t3) afterScript is not executed. The step requires one input and has an optional output. If no output is specified the SQL \n\t\t\t\t\tcommand\tis performed without writing data to the output. Placeholders can be inserted in any part of the SQL query and are replaced before the query executed. This allows you to define custom schema or table names, etc. A warning is shown if a placeholder is provided in the query but not defined in the table. Specifying two placeholders with the same name results in an error. Placeholders can only be used in non-batching strategies. Therefore, when placeholders are specified, the batch size is automatically set to 0. Parameter processing The SQL command uses a special form for parameter definition. The general syntax is: $[direction]{columnName} where direction is one of following: empty string - defines the INPUT parameter '+' - defines the OUTPUT parameter '*' - defines the INOUT parameter (of the procedure) The name of an input column must be given as columnName , otherwise the \n\t\t\tstep will report an error. SQL query notes: Parameter cannot stand for the column or variable names. Also constructs such as create function must not be used as parameters since all of those constructs modify the structure of the query and therefore\n\t\t\t\tthe query execution itself (in DB engine). When there's need to use parameters in these places - for example\n\t\t\t\tyou need to use something like this: select ${column_name} as my_out from some_table then you must use Dynamic SQL . Dynamic SQL is the way how to execute dynamically created queries \n\t\t\t\tin the runtime. Note that dynamic SQL is DB dependent and some DB engines support it (Oracle, MS SQL, PostgreSQL...) \n\t\t\t\twhile some others don't (mySQL for example). To find out how to write Dynamic SQL query, please consult your DB engine\n\t\t\t\tdocumentation. it is not recommended to put comments and other unnecessary parts in the SQL command since it may become unparsable \n\t\t\t\tfor JDBC drivers (they may not support these items). Parameter types When a parameter is defined as an input, then the value from the associated column is read and put to the command as it's \n\t\t\tparameter before command execution. Similarly the output parameter's assigned column is filled with values taken \n\t\t\tfrom the SQL command after the command's execution. INOUT parameters do both of \n\t\t\tread and write operations. Execution mode This step may perform SQL execution in two modes: traditional SQL query mode and procedure \n\t\t\tcall query mode. The decision what mode to use is done automatically and is based on the \n\t\t\tparameters defined in the query. If there's some OUTPUT or INOUT parameter defined, then the procedure call mode is used\t(because it is the only mode that \n\t\t\tsupports OUT parameters). When there are no parameters or all of parameters are INPUT ones then the traditional \n\t\t\tquerying mode is used. Nevertheless, it is possible to \"force\" the step to use procedure \n\t\t\tcall mode even for input-only parameter queries by using a \"procedure-call \n\t\t\tscheme\". It has the following format for procedure call: { call procedureName(attributes,...) } and similarly the following format for the function call: { result = call functionName(attributes,...) } To identify the query as the procedure call query, the query's first non-blank character must \n\t\t\tbe '{' and the last non-blank character must be '}' . Batching and commiting This step supports various batchSize and commitSize settings \n\t\t\t(see the parameters' descriptions) with the following effects: commitSize=1 emulates \"autocommit\" mode batchSize=0 enforces \"no batching\" mode - this prevents algorithm to use \n\t\t\t   \t      JDBC batching mechanism (addBatch) - classical statement.execute is used instead batchSize is ignored when the SQL command is a procedure call or \n\t\t\t    contains OUT or INOUT parameters. In that case the command is executed as \n\t\t\t    non-batched to allow reading of possible output parameters of each statement. Note: the possibility of using this step effectively depends heavily on the\n\t\t\tJDBC driver's abilities. As of now, the jTDS, MS SQL, DB2, Oracle and mySQL \n\t\t\tdrivers were tested and only the DB2 and Oracle drivers seem to have support for complex \n\t\t\tusage (usage of INPUT/OUTPUT parameters together in a traditional (nonprocedural) \n\t\t\tSQL query). The other ones seems to support OUTPUT parameters only if the \n\t\t\t\"procedure-call scheme\" query is used. Values read/written to the database depends on the JDBC \n\t\t\ttypes conversion .",
    "Step Properties": [
      "Id - Step identification string.",
      "After Script - SQL script to execute after step processing is done.",
      "Batch Size - Determines the size of the batch to use when updating the database. Note that batch size is used only if the query is not a procedure call. Default value: 0 (non batching mode). Reasonable values start at about 1000.",
      "Before Script - SQL script to execute before the step is run.",
      "Data Source Name - Data source definition to use.",
      "Query - SQL query to execute. See the detailed step description for information how to define query parameters.",
      "Commit Size - Defines after how many statements commit should be invoked. A value of 1 means autocommit.",
      "Error Handler - Defines behavior when some exceptions happen during processing the query.",
      "Placeholders - Placeholders",
      "Placeholder Begin Mark - The string marking the beginning of column placeholder.",
      "Placeholder End Mark - The string marking the end of column placeholder."
    ]
  },
  {
    "Step Name": "SQL Select",
    "Step Details": "Detailed Description This step reads some data from a database using a SQL command that may be parameterized with\n\t\t\tsome data from the input row. For every entry of the SQL command's result set it creates a copy of the\n\t\t\tprocessed input row with data from the result set row added according to the given\n\t\t\tmappings. In another words, if the SQL command returns n rows in the result set, then \n\t\t\tthere are n copies of the input row in the output, with data from the \n\t\t\trespective result set row added. If there is an input row for which the SQL command does not return any entry in the result set, \n\t\t\tthere is no entry for such row in the output data flow as well. This means that such \n\t\t\tinput row is \"dropped\" from the data flow. This behavior may be overridden by setting Include Empty to true (default value). This will cause \"empty\" input \n\t\t\trows to be written to the output as well (result set mapped columns will be empty then). Usage notes This step is suitable for read-like operations only (must return a result set). \n\t\t\t\tFor write-like operations use the SQL Execute step. When there is no result set as the result of the query execution, an error is thrown. This\n\t\t\t\tis considered a configuration problem that needs to be fixed. Because it is not possible now to analyze a SQL command without being run, some\n\t\t\t\tproblems are reported after the first result set is obtained. Those are validations of \n\t\t\t\tmapped result set columns, whether the query returns a result set, etc. Note\n\t\t\t\tthat when error is thrown at this moment, the state is as follows: \n\t\t\t\t1) possible Before Script is executed and committed 2) statement for the first row is not committed \n\t\t\t\t3) After Script is not executed Placeholders can be inserted in any part of the SQL query and are replaced before the query executed. This allows you to define custom schema or table names, etc. A warning is shown if a placeholder is provided in the query but not defined in the table. Specifying two placeholders with the same name results in an error. Processing parameters For a detailed description of how to define input and output parameters of the SQL query see\n\t\t\tthe description of the SQL Execute step. This step uses parameters the same way. ORACLE cursors: Step supports reading data returned by Oracle's stored procedures using cursors. To read from \n\t\t\ta cursor use the following syntax: { call ${#cursor#} := sp_procedure_name(${parameter},...) } Cursor value is identified by the special variable-keyword #cursor# - this\n\t\t\twill cause reading result set data from the variable rather than from the statement's result\n\t\t\tset. Take care of proper spacing : same spacing as the one in the example\n\t\t\tshould be used, otherwise Oracle may reject the query. Mapping result set values To map values from the result set to the data rows, mappings must be provided.\n\t\t\tTo determine SQL output column names reliably, it is strongly recommended to name output \n\t\t\tcolumns in the query using SQL's AS operator. In some cases this is necessary, such as \n\t\t\twhen, for example, the SQL query returns an unnamed column, then it is not possible to \n\t\t\taccess it via mappings unless it is named with the AS operator to something nonempty (because\n\t\t\tthe mapping's sqlColumn value is required and it cannot be empty). NOTE: Some database drivers (for example Oracle, ApacheDerby and possibly \n\t\t\tsome others) return names of the result set columns in uppercase (even for column names \n\t\t\tdefined with the AS operator). If this is the case, then uppercase names\n\t\t\tmust also be used in result set column mappings, since name matching is case sensitive\n\t\t\tthere. The same problem may occur with diacritics. Again, the same form must be used either\n\t\t\tin the SQL query and result set mapping. To convert SQL values to data flow values the JDBC data \n\t\t\tconversion is used.",
    "Step Properties": [
      "Id - Step identification string.",
      "After Script - SQL script to execute after step processing is done.",
      "Before Script - SQL script to execute before the step is run.",
      "Data Source Name - Data source definition to use.",
      "Include Empty - If false, only input records that yield records in the SQL result set are sent to the output. If set to true, then the processed input row is sent to the output even if there are no entries in the result set. Default value: true.",
      "Query - Select-like SQL query to execute.",
      "Mappings - Mappings that define where to map values read from the result set.",
      "Error Handler - Defines behavior when some exceptions happen during processing the query.",
      "Autocommit - Defines whether the statement should be committed after each row.",
      "Placeholder Begin Mark - The string marking the beginning of column placeholder.",
      "Placeholder End Mark - The string marking the end of column placeholder."
    ]
  },
  {
    "Step Name": "Statistics",
    "Step Details": "Detailed Description This step is capable of multiple analytical operations\n\t\t\t\t\tin a single pass over multiple columns of input data.\n\t\t\t\t\tAll data types supported by [branding:product.name.abbreviation] can be used as long as they correspond\n\t\t\t\t\tto the applied data operations. Analytical Operation FLOAT, INTEGER, LONG DAY, DATETIME BOOLEAN STRING Record Count yes yes yes yes Null Count yes yes yes yes Not Null Count yes yes yes yes Distinct Value Count yes yes yes yes Unique Value Count yes yes yes yes Sum yes - yes - Average yes yes yes - Median yes yes yes yes Standard Deviation yes yes - - Variance yes yes - - Fist X values yes yes yes yes Last X values yes yes yes yes Minimum length of sequence - - - yes Minimum length of non-empty sequence - - - yes Average length of sequence - - - yes Median length of sequence - - - yes Maximum length of sequence - - - yes Quantile Value yes yes yes yes Approximate Quantile Value yes yes - - Step outputs: out_stat - statistical output : \n\t\t\t\t\t\t\tcontains results of statistical operations. Its format is as follows: stat_name [STRING] - user defined name for the statistic\n\t\t\t\t\t\t\t\t\t(see Column Statistics Name ) stat_result [STRING] - value of the statistic stat_distinction [STRING] - distinction number of the statistic's record \n\t\t\t\t\t\t\t\t\t(among all the records belonging to the same statistic). Forms a row 0,1,2... out_rec - data output. It has the same format as\n\t\t\t\t\t\t\tinput data flow \n\t\t\t\t\t\t\tData Output - has the same format as Data and for both\n\t\t\t\t\t\t\trequired fields in input and output sections they are\n\t\t\t\t\t\t\treferenced with the identifier out_rec . For each statistic computed by [branding:product.name.abbreviation] a single row is returned, as long as\n\t\t\t\t\tthe statistic does not have the parameter count , or an input supplied by the parameter count .",
    "Step Properties": [
      "Id - Step identification string.",
      "Stat Name - This column contains the name of the statistic which is added to the data output row. SeeColumn Statistics Name.",
      "Stat Distinction - This column of resulting statistical records contains the value of the corresponding record identifier.For statistics with single record output (e.g. median; average) this is always 0; for multi-record output, like quantiles, this is a sequence (\"0\", \"1\", \"2\", \"3\" and so on).",
      "Default Locale - Locale definition represents a specific geographical, political, or cultural region, with respect\n\t\t\tto data parsing and comparison as performed by the step.",
      "Statistics - Root statistics node."
    ]
  },
  {
    "Step Name": "Steps",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Stop Reading",
    "Step Details": "Detailed Description This step signals the readers that they should stop reading data. The signal is sent when when condition is true. Usage notes The readers will read more records than expected due to caching and batching between steps. Once stop is signalled, it cannot be cancelled, even when condition no longer holds true.",
    "Step Properties": [
      "Id - Step identification string.",
      "When Condition - Expression that must be satisfied for this step to be executed."
    ]
  },
  {
    "Step Name": "StreamReaderWrap",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "String Lookup",
    "Step Details": "Detailed Description Searches for the input value (from the property in ) in the lookup file defined by the property Lookup File Name . If the value\n            is found it is stored in the property Out . The search result is reported by the scoring entries MCL_FOUND/MCL_NOT_FOUND .",
    "Step Properties": [
      "Id - Step identification string.",
      "When Condition - Expression that must be satisfied for this step to be executed.",
      "In - Column that contains the input value.",
      "Lookup File Name - Lookup file name.",
      "Omit If Found - If the input value is found in the lookup file, the output value isnull.Default value:false",
      "Omit If Not Found - If the input value is NOT found in the lookup file, the output value isnull.Default value:true",
      "Out - Column that stores the output value (dependent onomitIf....properties).",
      "Scorer - Element which stores basic scoring settings."
    ]
  },
  {
    "Step Name": "Strip Titles",
    "Step Details": "Detailed Description This step removes titles (academic, social, etc.) from person names. If specified, removed titles are stored in the\n\t\t\toutput Titles Out and separated by a delimiter. Known titles (title dictionary) are stored in a dictionary file\n\t\t\twhich is defined by the Title Lookup File Name property. The algorithm is as follows: initialize the algorithm so that each translation from the dictionary is loaded into the memory as sequence of tokens (do once only). create matching value from the input text according to matching value generator configuration stored in the dictionary. tokenize the matching value (hence \"match-then-tokenize\" approach). iterate over all input tokens and find the best match to the token sequences from the dictionary. Use the following rules: accept matched tokens at input when a whitespace is detected at input, ignore it but remember it for cases when expected token might be skipped for unmatched token t u at input and ignorable expected token t e : skip the token t u if it is ignorable virtually insert expected token t e otherwise remove the matched sequences from the input and put translations into output column Using this mechanism, characters such as dot ('.'), slash ('/'), underscore ('_') but also other\n\t\t\tcharacters not covered by Tokenizer can be skipped/ignored when put into Ignored Separators Consider the following dictionary (the bullet characters are used to emphasize ignored characters): title (left side) translation (right side) wd• stripped nd stripped A•B stripped A A• stripped abc | def stripped The following table clarifies the meaning of accepting unexpected tokens as described above. It summarizes both states of Ignored Separators : \"I\" ... ignored, \"R\" ... required Characters '|' and '%' are never ignored. input output (I) translation (I) output (R) translation (R) comment wd• stripped stripped found exactly wd◦ stripped wd◦ (I) dot can be ignored wd% % stripped wd% (I) dot can be ignored, % not nd stripped stripped found exactly wd stripped wd (R) cannot add dot nd• stripped • stripped (R) dot not accepted A•B stripped stripped found exactly A B• • stripped A B• dot not expected A A• stripped stripped found exactly A•A• A•A• A•A• Cannot skip inner dot ◦A••B ◦A••B ◦A••B too many consecutive dots ◦A•B•◦ ◦•◦ stripped ◦•◦ dots at ends remain ◦A A•◦ ◦◦ stripped ◦◦ dots at ends remain abc | def stripped stripped found exactly abc % def abc % def abc % def not matched, '%' required always",
    "Step Properties": [
      "When Condition - Expression that must be satisfied for this step to be executed.",
      "Id - Step identification string.",
      "In - A string expression whose result is examined by the step.",
      "Out - A string column where the input with titles removed should be stored.",
      "Titles Out - A string column where the removed titles should be stored.",
      "Min Word Count - Minimum number of words which must remain in the output (after removing titles).Default value:0.",
      "Separator - A single character or sequence of characters used as string delimiters to break output data\n\t\t\t\t(titles) into words.",
      "Title Lookup File Name - Dictionary file that contains known titles.\n\t\t\t\tFor more information about the dictionary seethe detailed description here.",
      "Tokenizer - Configuration how to split input text into tokens.\n\t\t\t\tThe default tokenizer setting distinguishes words, numbers and other single characters, except\n\t\t\t\tfor whitespace characters (space, tab).",
      "Ignored Separators - Definition of characters that might be ignored when input token\n\t\t\t\tcannot be matched to expected token in translation rule. \n\t\t\t\tExpected token can be ignored only if it is defined as ignorable,\n\t\t\t\twith input token either being skipped when it is ignorable as well, or preserved.The set is defined by means ofcharacter set).",
      "Preserve Unsupported Chars - If true then unsupported characters immediately following replaced text will be copied to\n                output, otherwise these characters will be removed. For detailed description of these characters,\n                seeMatching Value Generator Config.Default value:true",
      "Scorer - Element which stores basic scoring settings."
    ]
  },
  {
    "Step Name": "Tasks",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Text File Reader",
    "Step Details": "Detailed Description The data source is a standard text file (or an archive containing such a file, where records are saved as rows and\n\t\t\tcolumns are separated by delimiter characters. The step is capable of processing compressed files using either ZIP or GZIP format.\n\t\t\t\tWhen processing ZIP file, it searches for a file named after the archive itself (without the extension)\n\t\t\t\tor uses the only file present in the archive (regardless of its name). Moreover, the step is capable of reading multiple files having the same format\n\t\t\t\twith names defined as template where both, ordinary text and column variables\n\t\t\t\tcan appear.\n\t\t\t\tColumn variable has the following format: {<column_name>}.\n\t\t\t\tColumn variables that reference columns that are unknown are\n\t\t\t\ttreated as ordinary text.\n\t\t\t\tWhen reading, the step scans file system for files matching the file name template.\n\t\t\t\tFor each matched file and all its records it assigns corresponding\n\t\t\t\tcolumns the values extracted from the file's name. This step supports encodings supported by Java, including Unicode formats (supported\n\t\t     Unicode formats are: UTF-8, UTF-16, UTF-16BE, UTF-16LE). The input is processed by UnicodeAwareReader , which allows correct processing of files with Byte Order Mark (BOM)\n\t\t     signatures used in some Unicode formats (for their identification) and\n\t\t     therefore allows correct processing of non-Unicode formats. The file encoding detection procedure is as follows: The UnicodeAwareReader tries to read the BOM from the file and compares the received\n      \t\tresults with the encoding entered within the step configuration (the Encoding parameter). If both types match and this file type is supported by the version of Java being used,\n      \t\tthe BOM is skipped and the file is read in the original format. If the file type in the\n      \t\tconfiguration is not the same as the type set by the BOM, a format discrepancy warning is\n      \t\tdisplayed and the file processing proceeds in the format set within the configuration (including\n      \t\tBOM bytes, which are considered to be common content). If the BOM cannot be detected from the file, it is assumed that the encoding used within\n      \t\tthe step configuration is valid (i.e., either Unicode format without BOM signature\n      \t\tor non-unicode format) and the file is read in the format set within the configuration. If the configuration has UTF-16 as the general encoding type and it is\n      \t\tpossible to get the exact sub-format from the BOM (i.e., UTF-16BE or UTF-16LE), the type from the\n      \t\tBOM is used. But if there is no BOM signature contained within the file, processing ends\n      \t\twith an error because it is not clear which format should be used. The following table shows possible file encodings and configuration values for Unicode files: Real File Format Configuration Value BOM Signature Result (Used) Encoding UTF-8 UTF-8 yes UTF-8 UTF-8 UTF-8 no UTF-8 UTF-8 non/bad-unicode yes non/bad-unicode + warning UTF-8 non/bad-unicode no non/bad-unicode UTF-16BE/LE UTF-16BE/LE yes UTF-16BE/LE UTF-16BE/LE UTF-16BE/LE no UTF-16BE/LE UTF-16BE/LE UTF-16 yes UTF-16BE/LE UTF-16BE/LE UTF-16 no error UTF-16BE/LE non/bad-unicode yes non/bad-unicode + warning UTF-16BE/LE non/bad-unicode no non/bad-unicode non-unicode UTF-8 - UTF-8 non-unicode UTF-16 - error non-unicode UTF-16BE/LE - UTF-16BE/LE non-unicode other-format - other-format UTF-32(LE/BE) formats are not supported. Non-Unicode formats are untouched by this\n     \t\t\treader and they are read using formats entered within the configuration. Input data records are processed using parameters specified in the element Data Format Parameters . Additional details are available in DataFormatParameters .\n\t\t\t\tIf a column does not define (override) its own format settings, a global formatting setting\n\t\t\t\tis used by default. This step may produce the following errors: SHORT_LINE , INVALID_DATE , UNPARSABLE_FIELD , LONG_LINE , EXTRA_DATA , PROCESSING_ERROR . When a SHORT_LINE error occurs the input value is considered to be null for further parsing. Error management is configured by the element Error Handling Strategy .\n\t\t\t\tError handling strategy allows disabling of processing of the incorrect entries,\n\t\t\t\twhich can be send to the \"rejected\" output endpoint. For a more detailed description of\n\t\t\t\terror handling see Error Handling Strategy . When creating a reject file, the following rules are observed: The initial name for the reject file is rejected.txt . The encoding defined for the input data file is used as the encoding of the rejected file. The line separator defined for the input data file is used as the line separator. Every input row is written to the reject file at most once. So,\n\t\t\t\t\teven if there are more error fields in the same row whose instructions\n\t\t\t\t\trequire writing to the reject file the row is written there only once. Empty reject files are not created. A reject file is created when\n\t\t\t\t\tan instruction requires writing to this file.",
    "Step Properties": [
      "Columns - Contains definitions of columns which will be read from the input file.",
      "Data Format Parameters - General parameters for data formatting. This configuration is applied to\n\t\t\t\tall specified columns unless the column defines its owndataFormaParameterssection.\n\t\t\t\tFor more information, seeDataFormatParameters",
      "Encoding - File data encoding. The possible encodings are all encodings supported by the\n\t\t\t\ttarget Java platform. Some commonly used encodings are: ISO-8859-1,\n\t\t\t\tISO-8859-2, and UTF-8.",
      "Error Handling Strategy - Definition of error handling strategies that define how to respond to every error\n\t\t\t\tstate the step can recognize. For more details, see the description ofError Handling Strategy.",
      "Field Separator - String to be recognized as a field delimiter.Escaped string property.",
      "File Name - Name of the file containing the input data.",
      "Compression - Defines compression type of the input file.Default value: NONE.",
      "Ignored Row Reg Ex - A regular expression that is compared to lines in the input file. If a line (read as a string) matches the\n\t\t\t\tregular expression, the line will be ignored.",
      "Line Max Read Length - Specified maximum number of characters per line to be read and processed.\n\t\t\t\tIf the length of the line exceeds this value, an error occurs.",
      "Line Separator - Specifies the string to be recognized as a line delimiter. The line delimiter\n\t\t\t\tcan be any sequence of letters, although a specific symbol is often used\n\t\t\t\twhich usually depends on the operating system where the file originated from.\n\t\t\t\tThe special symbols are:  \\r = CR, \\l = LF, \\n = LF.Escaped string property.",
      "Number Of Lines In Header - Specifies the number of lines from the beginning of the file that will be excluded from\n\t\t\t\tprocessing (header lines, comments, etc).",
      "Number Of Lines In Footer - Specifies the number of lines from the end of the file that will be excluded from\n\t\t\t\tprocessing (footer lines).",
      "Shadow Columns - Contains a set of columns that are not present in the input data, but should be\n\t\t\t\tcreated in the output (so that they are then available for further use as any\n\t\t\t\tother \"real\" column read from a file).",
      "String Qualifier - String enclosing a text string. Not defined by default.Escaped string property.",
      "String Qualifier Escape - Escape character escaping theString Qualifierin the original meaning of\n\t\t\t\tthis character. Not defined by default.Escaped string property.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Text File Writer",
    "Step Details": "Detailed Description The data output for this step is a standard text file (possibly compressed). Data records are stored as lines consisting of\n\t\t\tcolumns that are separated using defined delimiter characters. The step is capable of creating compressed files using either ZIP or GZIP format.\n\t\t\t\tWhen processing ZIP file, it searches for a file named after the archive itself (without the extension)\n\t\t\t\tor uses the only file present in the archive (regardless of its name). Moreover, the step is capable of creating multiple files having the same format\n\t\t\t\twith names defined as template where both, ordinary text and column variables\n\t\t\t\tcan appear.\n\t\t\t\tColumn variable has the following format: {<column_name>}.\n\t\t\t\tColumn variables that reference columns that are unknow are\n\t\t\t\ttreated as ordinary text. NULL value is transformed to __default__ ,\n\t\t\t\tsome special characters (/\\?%*:|\"<>) are converted to underscore (_).\n\t\t\t\tWhen writing each record, file name is determined using the file name template\n\t\t\t\tand this record's columns to produce resulting file name the record\n\t\t\t\tis to be written to.",
    "Step Properties": [
      "Generate Metadata - Specifies whether to generate metadata file.Set by default.",
      "Id - Step identification string.",
      "Columns - Aggregates elements with a description of columns to be written to a file.",
      "Data Format Parameters - Contains parameters needed for output data formatting.",
      "Encoding - Specifies what encoding to use when writing to a file.",
      "Field Separator - Specifies the sequence to write as a field separator.Escaped string property.",
      "File Name - Name of the file to be written to.",
      "Compression - Defines compression type of the output file.Default value: NONE.",
      "Line Separator - Specifies the string to be recognized as a line delimiter. The line delimiter\n\t\t\t\tcan be any sequence of letters, although a specific symbol is often used\n\t\t\t\twhich usually depends on the operating system where the file originated from.Escaped string property.",
      "String Qualifier - Character to be used as a boundary for text strings in the output data.Default value: double quote (\")Escaped string property.",
      "String Qualifier Escape - Escape character to be used for escaping theString Qualifiercharacter, used to preserve the original meaning of this\n\t\t\t\tcharacter.Default value: double quote (\")Escaped string property.",
      "Use String Qualifier On All Columns - Specifies whether to apply the string qualifier to all format columns.Not set by default.",
      "Write All Columns - Specifies whether to write out all columns as defined in the input format. This attribute is exclusive\n\t\t\t\tto column definitions. If this value is set to true there must be no columns defined\n\t\t\t\tin the columns element, otherwise an error is reported.",
      "Write Header - Specifies whether to write the column's header to the output file.A header line is written by default."
    ]
  },
  {
    "Step Name": "Time Series Predictor",
    "Step Details": "Detailed Description This steps tries several algorithms for time series prediction and uses the best to predict next values.\n                All different models and their variations created from corresponding model settings create state space that is searched by a grid search in order to find the best model for prediction.\n                Algorithms in a grid search are tried sequentially, or in parallel (if parallelism level is greater than 1). Evaluation of single model instance in a grid search: First, input data of size s is split by the Train Ratio to train data of size n and test data of size m . Second, model is fitted with the first n data and predicts the next value ( n+1 ). Next, model is fitted with the first n+1 data and predicts the next value ( n+2 ). ... Next, model is fitted with the first s-1 data and predicts the next value ( s ). All of these m predicted values are used to compute model error using selected metric. After all models were evaluated, model with the least error is used for final prediction.",
    "Step Properties": [
      "Input Column - Name of the column with time series values. The column must be of numeric type (integer, long, float).",
      "Group By - Set of definitions of grouping keys. This is used when there are multiple time series, where each time series has its own group key.",
      "Order By - Expression used for sorting time series values.",
      "Metric - Metric used when evaluating model performance in the grid search.",
      "Train Ratio - Lower bound of proportion of data that will be used for fitting and evaluating the model.Default value =0.75.",
      "Number Of Predictions - Number of future points to be predicted by the step.Default value =1.",
      "Model Settings - Different model definitions to be tried in the grid search.",
      "Record Number Binding - Column for outputting order of predicted values (sequence starting from 1).",
      "Model Binding - Column for outputting model used for prediction.",
      "Metric Bindings - Columns for outputting metrics computed while fitting the model that is used for prediction.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "toc",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Tokenizer",
    "Step Details": "Detailed Description This step splits content provided by the input columns into specified output columns\n\t\t\tusing the defined tokenizer. If the tokenizer is not defined, a default one is used.\n\t\t\tThe default tokenizer recognizes tokens comprised of numbers and text characters only;\n\t\t\tother characters are considered as delimiters. Default tokenizer settings are illustrated\n\t\t\tin the following example (i.e., the output is the same whether the tokenizer is configured\n\t\t\tor not): Tokenizer output columns are combined into a single string, and individual tokens are delimited\n\t\t\twith a character defined using the Separator property. This property can be defined\n\t\t\ton both global and local levels. If a separator is not defined on the global level, a default value of ' ' is used. On the local level, the Separator can be defined within individual Columns properties. Otherwise, the global value is used instead.",
    "Step Properties": [
      "When Condition - Expression that must be satisfied for this step to be executed.",
      "Id - Step identification string.",
      "Columns - This section associates individualcolumntags representing inputs/outputs.\n\t\t\t\tIndividualcolumndefinitions are processed in sequence as they are\n\t\t\t\tdefined within this configuration section.",
      "Separator - Global separator value. If not defined, a default value of' 'is used.",
      "Tokenizer Config - Configurable tokenizer Settings."
    ]
  },
  {
    "Step Name": "Text Vectorizer",
    "Step Details": "Detailed Description Uses previously created Vectorizer model to process input data.",
    "Step Properties": [
      "Feature Configs - Configuration of features used to preform vectorization of the data based on the model file loaded.",
      "Model File - Name of the file with trained model that will be used for vectorization.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Text Vectorizer Trainer",
    "Step Details": "Detailed Description Trains Text Vectoriez model based on selected parameters and input data.\n                If result column is filled, use trained model to vectorize input data and output the result to output column.",
    "Step Properties": [
      "Feature Configs - Configuration of features used to train the Text Vectorizer and vectorize the input data.",
      "Output Model File - Name of the model output file.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Transform Legal Forms",
    "Step Details": "Detailed Description This step transforms legal forms found in an input value (the property In ) and generates a final translated value stored in\n\t\t\tthe output (the property Out ) data record.\n\t\t\tThe transformation consists of finding all known forms (i.e., different ways how users type legal forms) within the input\n\t\t\tstring and transforming them into the correct and official forms. If the output for obtained official legal forms (the property In ) is defined, they are all stored in the specified column.\n\t\t\tIf more than one legal form is found, they are all separated by a delimiter specified in the Separator property.\n\t\t\tIf no input data is available, the scoring flag TLF_NULL is set. If any transformations have been performed,\n\t\t\tthe scoring flag TLF_CHANGED is set.",
    "Step Properties": [
      "Id - Step identification string.",
      "When Condition - Expression that must be satisfied for this step to be executed.",
      "Form Out - Column that stores transformed legal forms (if any).",
      "In - Column that contains the input string to be transformed.",
      "Legal Forms Lookup File Name - Dictionary file that contains known legal forms.\n\t\t\t\tFor more information about the dictionary seethe detailed description here.",
      "Out - Column that stores the final string with all legal forms transformed into their official versions.",
      "Separator - A single character or sequence of characters used as string delimiters to break output data\n\t\t\t\t(legal forms) into words.",
      "Tokenizer - Configuration how to split input text into tokens.\n\t\t\t\tThe default tokenizer setting distinguishes words, numbers and other single characters, except\n\t\t\t\tfor whitespace characters (space, tab).",
      "Ignored Separators - Definition of characters that might be ignored when input token\n\t\t\t\tcannot be matched to expected token in translation rule. \n\t\t\t\tExpected token can be ignored only if it is defined as ignorable,\n\t\t\t\twith input token either being skipped when it is ignorable as well, or preserved.The set is defined by means ofcharacter set).\n\t\t\t\tFor a detailed description seeStrip titlesand itsexample.",
      "Preserve Unsupported Chars - If true then unsupported characters immediately following replaced text will be copied to\n                output, otherwise these characters will be removed. For detailed description of these characters,\n                seeMatching Value Generator Config.Default value:true",
      "Scorer - Element which stores basic scoring settings."
    ]
  },
  {
    "Step Name": "Transliterate",
    "Step Details": "Detailed Description For each row it can transform value given by an expression and store it into an output\n         column. There can be more input expressions and output columns. The transformation\n         tries to find substrings (given by the property Rules ) in the input expressions and\n         substitutes them by string given in the matching rule. When there are two rules with the same prefix,\n         the longest possible rule is always used (i.e. considered as matched) .",
    "Step Properties": [
      "Columns - Each row defines an input expression whose value is transformed according to the rules and stored into an output column.",
      "Rules - Each row defines one rule which consists of a searched string and a replacing string.",
      "When Condition - Expression that must be satisfied for this step to be executed.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Trash",
    "Step Details": "Detailed Description Reads all data from the specified input and no operation is performed. Input data records are neither persisted to disk,\n\t\t\tnor passed along to other processing steps. Its behavior is similar to the /dev/null device in UNIX based operating systems.",
    "Step Properties": [
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Address Lookup Builder CZ",
    "Step Details": "Detailed Description This step compiles source data of the UIR-ADR into a format readable\n\t\t\tby the Address Identifier step. The address identification suite relies on synonyms defined for each\n\t\t\taddress component separately (e.g., streets named like 'Boženy Němcové',\n\t\t\t'Němcové', 'B. Němcové' etc. are unified under a common name 'Němcové').\n\t\t\tThis unification is a two step process consisting of partial\n\t\t\tand full replacement.",
    "Step Properties": [
      "Id - Step identification string.",
      "Dest Folder - Folder where compiled files are stored. If the folder doesn't exist, it is created. \n\t\t\t\tIf it does exist, it is cleared so that any .lkp, .cif or .cifIndex file is removed.",
      "Replacements - List of replacements for particular address components. The list doesn't necessarily\n\t\t\t\tdefine replacements for all components.Default value:empty list.",
      "Source Folder - Folder containing source files of the UIR-ADR",
      "Work Folder - The working folder.Default value: folder referenced by the java.io.tmpdir variable or\n\t\t\t\tsystem temporary folder if the variable is not set.",
      "Uir Adr Encoding - Specifies the encoding of input files.Default value: windows-1250 for UIR-ADR mode, UTF-8 for RUIAN mode.",
      "Active Addresses Only - Indicates whether accept only valid (i.e. current) addresses or removed addresses as well.Default value:false.",
      "Generate Exact Indices - Indicates to generate indices to be used byExactAddressIdentifieralgorithm.Default value: false.",
      "Mode - Defines type of source data.Possible values:UIR-ADRandRUIAN.\n\t\t\t\tDefault value: UIR-ADR."
    ]
  },
  {
    "Step Name": "Unification",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Unification",
    "Step Details": "Detailed Description",
    "Step Properties": [
      "Id - Step identification string.",
      "Attributes - Specification of step input and output parameters.",
      "Ignore Older - WhenTimestamp Columnis used and the incoming record update is older than the repository record,\n\t\t\t\tthis flag specifies how this status is managed.false: (newer) the repository record is processed instead of the incoming one.true: the incoming record is fully ignored as it hasn't been contained in the input.",
      "Ignore Manual Override - Do not process manual override rules even though they are contained in the repository.Default value: false.",
      "Export Changed Only - Send only changed records to the output. Changed records are all records\n\t\t\t\tfrom input, including deletions, and records from repository that have been changed during unification\n\t\t\t\t(some of id or role has changed).Default value: false.",
      "Default Max Iterations - Default value for theMax Iterationsparameter of grouping methods.Default value: 1.",
      "Use External Ids - Use candidate and matching ids and roles from the input rather than from the repository.\n\t\t\t\tUsed only in the special case of reunifying all records which already have some unification attributes.\n\t\t\t\tThe repository must be empty.Default value: false.",
      "Default Locale - Default locale definition for string values to compare.",
      "Default Merge Survivor Selection Rule - DefaultMerge Survivor Selection Rulecriterion for all grouping methods.",
      "Groups - List of grouping method definitions. Each unified record belongs to the first method\n\t\t\t\twhere theWhencondition evaluates to true.",
      "Matching Measures - List of Matching Measures.",
      "Default Matching Rules - DefaultMatching Rulescriterion for all grouping methods.Note: two records named \"pivot\" and \"candidate\" are used inExpressionproperty of each matching rule.",
      "Use Pivot As Survivor - Specifies if pivot records (with rolesMandI)\n\t\t\t\tare used as \"Merge survivors\" of candidate or matching groups, respectively.Default value: false. (Special MSR records and msr-role is used).\n\t\t\t\tSeeID Stability.",
      "Minimum Id To Assign - Minimal value for the newly assigned group id. Must be >= 1.Default value: 1.",
      "Default Pivot Selection Rule - DefaultPivot Selection Rulecriterion for all grouping methods.",
      "Repository - Repository specification.",
      "Repository Columns - List of columns stored in the repository. At least all columns needed for unification\n\t\t\t\tmust be present.",
      "Map All Columns - Specifies that all columns of the input record format will be stored in the repository.\n\t\t\t\tThe propertyRepository Columnsmust be empty when true.Default value: true.",
      "Delete Mark - Records with this value inChange Type Columnwill\n\t\t\t\tbe considered as deleting commands.Default value: \"D\".",
      "Identify - Specifies identification mode (not unification). If set, repository access is\n\t\t\t\tread-only and primary key column is not mandatory.Default = false.",
      "Group Id Strategy - Defines how merge survivors will be used in some special cases."
    ]
  },
  {
    "Step Name": "Union",
    "Step Details": "Detailed Description This step merges input formats of two defined inputs into a single output format. \n\t\t\tThis merge is based on defined Column Mappings . Data are read from their \n\t\t\tindividual inputs ( in_a or in_b ) and written to the output. Therefore, \n\t\t\tin the output there are merged-format data records, where each row contains data read from \n\t\t\teither in_a or in_b input.",
    "Step Properties": [
      "Id - Step identification string.",
      "Column Mappings - Contains \"columnMapping\" elements with column mapping definitions."
    ]
  },
  {
    "Step Name": "Union Same",
    "Step Details": "Detailed Description This step combines multiple separate data flows into single data flow. The input data flows must have the same column definitions (meaning the same names and types of columns).\n\t\tThe columns must also be in the same order. The column definition of the output data flow corresponds\n\t\tto the column definition of the input flows.",
    "Step Properties": [
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Update Gender",
    "Step Details": "Detailed Description Based on a specified first name and last name, this step determines gender value and verifies it against\n\t\t\tany provided input value. The final result can be one of the following: Gender value is confirmed - value determined by the step corresponds to the original (input) value\n\t\t\t\tand in the output the original value is stored (CONFIRMED). A problem - mismatch found during evaluation: the value determined from the first name doesn't match\n\t\t\t\tthe value determined from the last name. The original value is stored in the output (MISMATCH). A problem - mismatch between the determined and original value was found, but the determined\n\t\t\t\tgender was not verified definitely/sufficiently (with percentage above the required threshold - see below)\n\t\t\t\tusing both first and last name.\n\t\t\t\tThe original value is stored in the output and the value that is, according to the step, probable is stored in the property Estimated Gender (CHANGE_SUGGESTION). Gender value is changed - the step result unambiguously (using both first and last name) confirms\n\t\t\t\tone definite value. In this case the original gender value is changed (CHANGED). Value cannot be determined because the step is not capable of determining the gender value using the \n\t\t\t\tgiven first name and last name (UNDECIDABLE). Determination of the gender value is dictionary based. Dictionaries contain known first and last names\n\t\t\ttogether with information about a ratio in which the name is represented within men and women.\n\t\t\tThe final decision about the gender value is then based on the threshold, which is 51 percent by default (i.e.,\n\t\t\tonly first names and last names with a minimum of 51 percent ratio each are considered to confirm the gender value\n\t\t\tunambiguously). The default value can be changed using the properties Name Sureness Level and Surname Sureness Level (a percentage; the step accepts values between 51 and 100).\n\t\t\tNOTE: statistical data with first names and last names for\n\t\t\tCzech Republic were used as a resource for the dictionaries. The original gender value is replaced only if the derived gender value is determined identically\n\t\t\tby both\tfirst and last name (both satisfy the defined threshold).\n\t\t\tIn other cases the original (input) value is stored in the output . If the input value\n\t\t\tis incorrect (and/or not found in the dictionary), it is considered to be empty and the step sets\n\t\t\tthe scoring flag GNDR_GENDER_MISMATCH . This step supports also single-based verification - using the last name only, but in this case it only confirms the \n\t\t\tgender value according to the last name or it sets a scoring flag Estimated Gender - a string indicating\n\t\t\tthe estimated result. However, the original value is retained and stored in the output In Gender (but a replacement in this\tsituation can be forced using the the property Overwrite If Guess ). The same situation occurs (with the same result) when determination of the gender value is not definite, however\n\t\t\tone of the components suggests what the probable gender value is  (i.e., only one of the components satisfies\n\t\t\tthe threshold in the appropriate dictionary). Because the step uses dictionaries with exact forms of first names and last names, input first name and\n\t\t\tlast name need to be cleansed and identified beforehand (e.g. using the Guess Name Surname step). Comparision of the determined and input gender value is case-insensitive. The following table shows scoring flags set by the step for combinations of different input values. If no input gender value is specified, the step has no data to compare the result against, instead the values\n\t                   from \"F\" and \"M\" rows and columns (blue) are used. If an input gender value is provided, the columns input gender value differs and input gender value equals (green) are used. The rest of table values (black on gray) are not dependent on the input gender value, i.e. they are the same for both cases. Scoring flags conform to the overwriteIfGuess=false status. If this property is set to true , the scoring flag CHANGED applies (instead of CHANGE_SUGGESTION ). Last name First name Not provided Not found Not conclusive F (female) M (male) Input gender value differs Input gender value equals Not provided UNDECIDABLE UNDECIDABLE NAME_UNKNOWN UNDECIDABLE CHANGE_SUGGESTION CHANGE_SUGGESTION CHANGE_SUGGESTION CONFIRMED Not found UNDECIDABLE SURNAME_UNKNOWN UNDECIDABLE NAME_UNKNOWN SURNAME_UNKNOWN UNDECIDABLE SURNAME_UNKNOWN CHANGE_SUGGESTION SURNAME_UNKNOWN CHANGE_SUGGESTION SURNAME_UNKNOWN CHANGE_SUGGESTION SURNAME_UNKNOWN CONFIRMED SURNAME_UNKNOWN Not conclusive UNDECIDABLE UNDECIDABLE NAME_UNKNOWN UNDECIDABLE CHANGE_SUGGESTION CHANGE_SUGGESTION CHANGE_SUGGESTION CONFIRMED F(female) CHANGE_SUGGESTION CHANGE_SUGGESTION NAME_UNKNOWN CHANGE_SUGGESTION CHANGED MISMATCH M(male) CHANGE_SUGGESTION CHANGE_SUGGESTION NAME_UNKNOWN CHANGE_SUGGESTION MISMATCH CHANGED Input gender value opposite CHANGE_SUGGESTION CHANGE_SUGGESTION NAME_UNKNOWN CHANGE_SUGGESTION CHANGED MISMATCH Input gender value equals CONFIRMED CONFIRMED NAME_UNKNOWN CONFIRMED MISMATCH CONFIRMED",
    "Step Properties": [
      "When Condition - Expression that must be satisfied for this step to be executed.",
      "Id - Step identification string.",
      "Change Gender Case - If set totrueand if the scoring flag \"CONFIRMED\" occurs, the output gender value\n\t\t\t\tis transformed to upper case. In other cases it is stored in lower case form. If this property\n\t\t\t\tis set tofalse, the format is copied fromMale Definition,Female Definitionor input. The setting of this property has no influence on the\n\t\t\t\tcomparision process of the input gender value withMale DefinitionandFemale Definition- this comparision is always case insensitive.Default value:False.",
      "Estimated Gender - Column that stores assumed (estimated) gender value if applicable (e.g., in case of a single component\n\t\t\t\t(last name) based determination or in the case that determination of the gender value is not definite). At the\n\t\t\t\tsame time, the scoring flagCHANGE_SUGGESTIONis set. The value inIn Genderis retained. This functionality can be suppressed using theOverwrite If Guessproperty.",
      "Female Definition - A string value defining identification of the female gender. This value is expected in the input\n\t\t\t\tand also stored in the output.Default value:F.",
      "First Name Ratio Lookup File Name - Dictionary file that contains first names and their\n\t\t\t\trepresentative ratio within men and women. The definition of this dictionary is mandatory\n\t\t\t\twhen the propertyIn Nameis defined.\n\t\t\t\tFor more information about the dictionary seethe detailed description here.",
      "In Gender - Column that contains the input gender value and stores the final output gender value.",
      "In Name - Column that contains the input first name.",
      "In Surname - Column that contains the input last name.",
      "Male Definition - A string value defining identification of the male gender. This value is expected in the input\n\t\t\t\tand also stored in the output.Default value:M.",
      "Min Surname Length - Minimum allowed length of the last name. If the input last name does not meet this criteria, it is\n\t\t\t\tconsidered to be empty.Default value:3.",
      "Name Sureness Level - Integral value (threshold) in which the first name must be represented within men or women, respectively,\n\t\t\t\tto be considered as conclusive. The value range 51-100 is accepted.Default value:51.",
      "Overwrite If Guess - If set totruethis property, instead of setting scoring flag \"CHANGE_SUGGESTION\", stores\n\t\t\t\tthe value directly in the outputIn Genderand changes the scoring flag to \"CHANGED\".Default value:False.",
      "Surname Ratio Lookup File Name - Dictionary file that contains last names and their\n\t\t\t\trepresentative ratio within men and women.\n\t\t\t\tFor more information about the dictionary seethe detailed description here.",
      "Surname Sureness Level - Integral value (threshold) in which the last name must be represented within men or women, respectively,\n\t\t\t\tto be considered as conclusive. The value range 51-100 is accepted.Default value:51.",
      "Scorer - Element which stores basic scoring settings."
    ]
  },
  {
    "Step Name": "USAddressesBuildLookupData",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Address Identifier US",
    "Step Details": "",
    "Step Properties": [
      "Input - Input endpoint column mappings.",
      "Output - Output endpoint column mappings.",
      "Output - Cleansed output column mappings.",
      "Id - Step identification string.",
      "Data Dir - Directory with lookup files for US addresses. Must point to directory named 'data'\n                in the standard folder hierarchy (<project>/data/ext/lkp/us_address_identifier)."
    ]
  },
  {
    "Step Name": "Validate Bank Account Number CZ",
    "Step Details": "Detailed Description This step validates input column data as a bank account number used in the Czech republic (and Slovakia).\n\t          Numbers in this format consist of three parts: prefix, base and bank ID. \n\t          The step first trashes characters in the input that cannot occur in the bank number and saves them\n\t          to the trashed output. Then, it tries to determine the bankId , base and prefix parts of the account\n\t          number. According to the bank account format specification, the prefix and base must pass the mod11 test to be considered as valid. The step also \n\t          supports output data formatting as specified by the property Output Format and configurable invalid data handling as defined in the invalid data handling configuration \n\t          properties. See the description of each property for more details.\n\t          If the output is not processed by invalid data handling routines, the output format\n\t          determines what will be written out: If there is an Output Format defined, it is used to format the parsed data \n\t                    and the formatted data are written out. If the corresponding part of the Output Format could not be determined, the empty string value is used instead. If no Output Format is defined, the original value is written to the \n\t                    output. If the bankIdOut output that has no relevant input: the empty value is written out if bank id was not recognized the standardized bank id is written out if it was recognized",
    "Step Properties": [
      "Id - Step identification string.",
      "When Condition - Expression that must be satisfied for this step to be executed.",
      "In Bank Account - The input column with account number data.",
      "Out Bank Account - Where the result should be stored.",
      "Trash - The output contains characters (letters) removed from the account number before the start \n\t          of processing.",
      "Out Bank Id - If recognized, this output will contain the bank ID number in standardized form, which is\n\t          a 4-digit bank ID number filled with zeroes from the left if needed. This output value\n\t          does not depend on the validation of the foundbank idin the lookup file.",
      "Bank Id Lookup File Name - Lookup to use for validation of the determinedbank id. \n\t          It is a simple string lookup containing single values representing thebank id.\n\t          For more information see thedetailed description.",
      "Output Format - The format to be used for formatting the output. There are 3 predefined constants \n\t          representing the parts of the account number:{PREFIX},{BASE}and{BANKID}. If filling with zeroes to the maximum length of the component \n\t          is required, it is possible to use the constants{0PREFIX}and{0BASE}.{BANKID}always means thebank idstandardized to the \n\t          4-digit length, so a related 'zeroed' constant is not defined.Examples for the input00123-000456-234:{PREFIX}-{BASE}/{BANKID}- outputs123-456/0234{0PREFIX}-{0BASE}/{BANKID}- outputs000123-0000000456/0234Note that using special characters inside the component name will make it a 'static text' \n\t          component: for example{:PREFIX}will be represented as string \"{:PREFIX}\" \n\t          instead of identifying theprefixpart of the number.",
      "Invalid Data Definitions - A collecting tag for invalid data definitions processed by this step.",
      "Scorer - Element which stores basic scoring settings."
    ]
  },
  {
    "Step Name": "Validate Birth Number UA",
    "Step Details": "Detailed Description This step validates birth numbers used in Ukraine. For now, the number is tested to comply\n            with the following constraints: The number's date part must comply with the given date hint (only when the hint is \n                      present) The number's gender identification must comply with the gender hint (only when \n                      the hint is present) The number format must comply with the following rules: The date part must contain at least one non-zero digit The number after normalization must be at maximum 10 digits long.\n                              Normalization means that the number is converted to the 10-digit form by\n                              filling missing zeroes from the left or stripping redundant zeroes \n                              from the left This step also generates a fake birth number when the output fake binding property is set and \n            sufficient info to generate the number is present (the gender hint and date hint \n            are both connected and filled with valid values). The generated fake number consists of the date \n            part relevant to the given date hint, the gender part relevant to the given gender and the \n            'filling' character defined by the property Fake Substitute Character . The format of the generated fake number is the following: 00000XXXGX where 0 - the date part related to the given date hint (the number of days since 1.1.1900) X - the value defined by the Fake Substitute Character property G - the gender value: 1 for male, 2 for female - related to the given gender hint Note that generated fake numbers are not valid, since it is not possible to \n                calculate the valid control digit at present and therefore make the generated number valid \n                (fulfilling the Ukrainian rule for the control digit). Also the 'filling' character\n                may have a non-numerical value and therefore break the 'numerical-digits-only' constraint.",
    "Step Properties": [
      "Birth Date - Expression defining theDAYvalue to be taken as the date hint.",
      "Birth Date Out - Name of theDAYtype output where the birth date read from the birth \n                number should be written to.",
      "Birth Number - Expression defining theSTRINGvalue to be taken as the birth number.",
      "Birth Number Out - Name of theSTRINGtype output where the number validation result \n                should be written to.",
      "Gender - Expression defining theSTRINGvalue to be taken as the gender hint.",
      "Gender Out - Name of theSTRINGoutput where the gender read from the birth number \n                should be written to.",
      "Trash - Name of theSTRINGtype output where the characters from the birth \n                number trashed during validation should be written to.",
      "Fake Birth Number Out - STRINGtype output where the generated fake number should be written to. \n                A fake number is generated only when this property is set and there are valid \n                gender and date hints in the input.",
      "Fake Substitute Character - Defines the character that will be used as the substitution character in the generated \n                fake number. The default value is '0'. This value is required if the \n                fake number should be generated (i.e., whenFake Birth Number Outis \n                connected).",
      "Female Definition - Defines how the female gender is encoded in theGender/Gender Outinput/output",
      "Male Definition - Defines how the male gender definition is encoded in theGender/Gender Outinput/output",
      "Invalid Data Definitions - Collector tag for invalid data definitions",
      "When Condition - Expression that must be satisfied for this step to be executed.",
      "Id - Step identification string.",
      "Scorer - Element which stores basic scoring settings."
    ]
  },
  {
    "Step Name": "Validate Email",
    "Step Details": "Detailed Description This step checks the validity of an e-mail address according to a valid e-mail address format.\n\t\t\tThe step supports verification of the top level domain (TLD). The TLD can contain characters\n\t\t\tand numbers and must start with a character. Lower level domains respect letter digit hyphen (LDH) syntax \n\t\t\tand can start with a number. A hyphen is allowed within the domain portion only.\n\t\t\tIf an error is detected ( EML_ADDRESS , EML_TLD or EML_NULL ),\n\t\t\tan empty value is stored in both outputs Out Name and Out Email .",
    "Step Properties": [
      "When Condition - Expression that must be satisfied for this step to be executed.",
      "Id - Step identification string.",
      "In - Column that contains the input e-mail.",
      "Out Name - Column that stores the output owner of the e-mail. Note: This value is NOT equal to\n\t\t\t\tthe name of the mailbox on front of the at (@) mark. E.g. Support [customer_support@ataccama.com], Out Name is Support.",
      "Out Email - Column that stores the output e-mail address (username@domain).",
      "Tld Lookup File Name - Dictionary file that contains valid top level domains (TLD).\n\t\t\t\tFor more information about the dictionary seethe detailed description here.",
      "Scorer - Element which stores basic scoring settings."
    ]
  },
  {
    "Step Name": "Validate IC CZ",
    "Step Details": "Detailed Description Tests the input string as an ICO (Czech identification number for legal persons) and verifies its validity.\n\t\t\t\tOnly valid ICO values are stored in the output column \"out\". If the ICO value\n\t\t\t\tis invalid then a scoring flag indicating an error in the ICO value is set and an empty\n\t\t\t\tvalue is written to the output. The property Allow Cleaning allows execution\n\t\t\t\tof ICO cleansing before the validation. The property Preserve Input Value determines\n\t\t\t\twhether the original or cleansed ICO value is written to the output (after successful validation).",
    "Step Properties": [
      "Allow Cleaning - Flag which specifies whether special characters (other than letters and digits)\n\t\t\tcan be removed in verificiation.Default value:True.",
      "In - Column that contains ICO values.",
      "Omit Invalid IC - Flag which specifies whether a null value (flag set totrue) is written to the output column \"out\" in case\n\t\t\tany of the scoring flags IC_NULL, IC_BAD_FORMAT or IC_BAD_CHECKSUM is set.\n\t\t\tIf flag is set tofalsethen the propertypreserveInputValueinfluences the output.\n\t\t\t\n\t\t\tI.e. if the input value is valid then either the original (not cleansed value)\n\t\t\tor the cleansed and corrected (appended by 0) value is written to the output.",
      "Out - Column that stores valid output ICO values.",
      "Preserve Input Value - Flag which determines whether the original input value (flag set totrue)\n\t\t\tor the cleansed value (flag set tofalse) should be written to the the output column \"out\".\n\t\t\tThe flag is ignored ifallowCleaning = false.Default value:False",
      "When Condition - Expression that must be satisfied for this step to be executed.",
      "Id - Step identification string.",
      "Scorer - Element which stores basic scoring settings."
    ]
  },
  {
    "Step Name": "Validate ID Card",
    "Step Details": "Detailed Description This step checks validity of a card number specified in the Cn data input. In case the card\n\t\tnumber satisfies the conditions of validity, the input card number will be copied\n\t\tto the data output Cn Out . If the card number fails to satisfy some of the conditions,\n\t\tthe corresponding error flags will be set (see description of the flags) and the output will contain an \n\t\tempty value. Recording of an invalid number to the output can be enforced by\n\t\tthe parameter Omit Invalid ID Card and setting its value to false . Validation procedure: first, the characters '0' are removed from the beginning\n\t\tof card number (from the left).  The remaining number has to satisfy the following conditions\n\t\tof validity (to be recognized as valid): number length has to be 8 – 19 characters (inclusive) number does not contain non-digit characters check digit in the last position is valid contains at least one nonzero digit in the positions 2-6 \n\t\t\t\t\t(this position identifies card issuers) The last digit of the card number is a check digit. The algorithm used to validate the \n\t\tcheck digit is called the Luhn algorithm. When validated, the number\n\t\tmay be padded with zeroes from the left without affecting the check digit value. This step outputs standardized number formats, i. e. without added initial zeros.",
    "Step Properties": [
      "Id - Step identification string.",
      "When Condition - Expression that must be satisfied for this step to be executed.",
      "Cn - Column that contains the input card number.",
      "Cn Out - Column that contains the output card number.",
      "Omit Invalid ID Card - Property that\n\t\t\tspecifies whether the output value of the card number is shown in case of an error\n\t\t\t(for further details see the detailed description and appropriate scoring keys):true- NULL value is written;false- the input (original) value is written.Default value:True.",
      "Scorer - Element which stores basic scoring settings."
    ]
  },
  {
    "Step Name": "Validate In RES",
    "Step Details": "Detailed Description Tests the existence of the the input IČO (property ico ) in the Czech Register of Economic Subjects (RES) by obtaining\n\t\t\tthe business name. The obtained business name is compared with the input business name (property businessName ).\n\t\t\tThe obtained name is stored in the property realNameOut if this property is defined. A dictionary file\n\t\t\twhich contains all economic subjects is defined by the property databaseFile (for a detailed\n\t\t\tdescription of the dictionary file see the section Files - Register of Economic Subjects . If the input IČO is not filled in, then [branding:product.name.abbreviation] reports an error and the step ends. If the input IČO is filled in, then the step reads the input\n\t\t\tbusiness name and compares it to the business name found in the dictionary file by the IČO value. Matching values of \n\t\t\tboth the input and dictionary file names are compared. Rules for creating matching bussines name\n\t\t\tare defined by the matchingValueGeneratorConfig property.\n\t\t\tIf the values differ, then the scoring flag IC_EXT_NOT_FOUND is set. If there is no record matching the input IČO\n\t\t\tvalue in the dictionary file, then the scoring flag IC_EXT_NOT_FOUND is set. For best matching of company names, the step Transform Legal Forms should be applied\n\t\t\tto this name (stored into file in first column) and the same transformation must be applied before this step.",
    "Step Properties": [
      "Id - Step identification string.",
      "When Condition - Expression that must be satisfied for this step to be executed.",
      "Business Name - Column that contains the input business name.",
      "Database File - String that contains the name of the file keeping the lookup table of the records IČO - business name - other info (RES).",
      "Ico - Column that contains the input IČO.",
      "Matching Name Generator Config - Configuration of the matching generator which creates values from input values and the dictionary that can\n\t\t\t\tmatch each other.",
      "Real Name Out - Column that stores the output business name.",
      "Scorer - Element which stores basic scoring settings."
    ]
  },
  {
    "Step Name": "Validate Phone Number",
    "Step Details": "Detailed Description This step verifies the validity of a phone number. The phone number format is specified using parsing rules.\n\t\t\tFor more accurate verification, a dictionary of international area codes is used.\n\t\t\t(IDC - International Destination Code -\tthe dictionary specified in Idc Lookup File Name )\n\t\t\tAlso, a dictionary of local providers (e.g., Czech Republic) can be used when specified using Prov Lookup File Name ). This step uses a parser to examine the input string. For more details about\n\t\t\tit please see the description of Pattern Parser . This step is capable of processing more phone extension numbers\n\t\t\tso that these are grouped together and separated by the given character\n\t\t\t(the Ext Separator parameter). Besides standard components there are the following predefined ones available.\n\t\t\tThese components are verified against corresponding dictionaries. PHONE - phone number: 9 digit number with a space as possible separator, i.e. {NUMBER:acceptSeparators=true,separatorChar=' ',minLength=9,maxLength=9} . EXT - phone extension: at most 3 digit number, i.e. {NUMBER:minLength=1,maxLength=3} . PROV - number present in the dictionary of providers given by Prov Lookup File Name . IDC - international destination code,\n\t\t\t\t\ta number starting with '+' or '00' and present in the dictionary of the codes given by Idc Lookup File Name . In order to recognize components correctly, the number parts must be divided by a \n\t\t\tseparator. For example to recognize a number \n\t\t\twith the pattern {AREACODE}{PHONE8} (where {AREACODE} is a  \n\t\t\tcomponent consisting of 1 or 2 digits and {PHONE8} is a component \n\t\t\tconsisting of 8 digits) the two parts of the number must be divided by separator, \n\t\t\tfor example, in the telephone number '12 12345678' where space is the \n\t\t\tseparator.",
    "Step Properties": [
      "When Condition - Expression that must be satisfied for this step to be executed.",
      "Id - Step identification string.",
      "Components - Definition of user components.",
      "Full Trash Scope - Specifies whether text parsed by components which don't the defineStore Parsed Intoparameter is\n\t\t\t\tstored into the column defined by the bindingTrash. Text not parsed by any component is stored in any case.\n\t\t\t\tThis parameter has effect only if the bindingTrashis defined.Default value:False.",
      "Ext Separator - A single character (only) used as a delimiter between\n\t\t\t\ttwo extension numbers in theOut Extoutput binding.\n\t\t\t\tDefault value:,(comma).",
      "Idc Lookup File Name - Dictionary file with known IDCs.\n\t\t\t\tFor more information about the dictionary seethe detailed description here.",
      "Idc Prefix - A string used for conversion of an international area code into a uniform output format.\n\t\t\t\tAllowed values are \"+\" or \"00\".Default value is an empty string.",
      "In - Column that contains the input phone number.",
      "Invalid Data Definitions - Collector tag for invalid data definitions",
      "Number Part Length - Length of a part of the phone number into which the number is split in the output format.Default value: 3.",
      "Number Separator - Separator used for phone number parsing in the output format.",
      "Out Ext - Column that stores is identified extension-line.",
      "Out Ext Orig - Column that stores is original extension-line.",
      "Out Idc - Column that stores is identified international area code.",
      "Out Idc Orig - Column that stores is original international area code.",
      "Out Phone Number - Column that stores is final standardized (output) phone number.",
      "Out Phone Number Orig - Column that stores is original phone number.",
      "Out Prov - Column that stores is identified phone provider number.",
      "Out Prov Orig - Column that stores is original phone provider number.",
      "Out Rule Name - Column that stores is used parsing rule that best describes the input structure.",
      "Tokenizer Config - Definition of the tokenizer.",
      "Pattern Groups - This section associates individualPatternGrouptags representing one specified group of patterns.",
      "Prefix - String attached to the beginning of the phone number in the output.Default value is an empty string.",
      "Prov Lookup File Name - Dictionary file with known phone providers.\n\t\t\t\tFor more information about the dictionary seethe detailed description here.",
      "Trash - Column that stores trash information. Trash information is the part\n\t\t\t\tof the input text which was not recognized\n\t\t\t\tas a known component or, when theFull Trash Scopeflag is true,\n\t\t\t\ttext parsed by the component not having its own output columnStore Parsed Into.\n\t\t\t\tThe binding is not required, thus the propertyFull Trash Scopeis effective only when this binding is set.",
      "Scorer - Element which stores basic scoring settings."
    ]
  },
  {
    "Step Name": "Validate RZ CZ",
    "Step Details": "Detailed Description Validates RZ (czech car registration number) and performs checks against: car type (only for RZs where it is possible to determine the vehicle type from the RZ format) registration date (whether the form of the RZ complies with the era: since 2001 the new form \n\t\t\t\t  of registration number is used) This algorithm does not contain any internal cleaning mechanism  (removing invalid characters) - and expects \n\t\tcleansed data on input. In this case, cleaning means removing special characters (pause, spaces, etc.). The only\n\t\taction performed by the algorithm is conversion of the input to the uppercase. This algorithm implements validation of the RZ forms as defined by the following czech laws: vyhl. č. 243/2001 Sb. vyhl. č. 291/2004 Sb. Recognized RZ types: Standard RZ numbers For standard RZ values, the following rules are applied: input's length must be from 5 to 7 characters characters are formed by Latin alphabets ( L ) capitals and Arabic digits ( D ) the first character must be a letter of the region ( R ) the next characters can be alphanumeric characters ( X ) the input must contain at least one letter ( L ) and one digit ( D ) where: L = { ABCDEFHIJKLMNPRSTUVXYZ } R = { ABCEHJKLMPSTUZ } D = { 0123456789 } X = L+D Exceptions to the standard format (special RZ types) The step recognizes vehicle registration numbers of the following types (Note: the quoted characters define specific letters in particular positions): historical: 'V'DDDD (old form), DD'V'DDDD , DD'V'DDD , DD'V'DD (new form) racing: 'R'DDDD (old form), DD'R'DDDD , DD'R'DDD (new form) trial: 'F'DDDD handling/operating: min 5 characters, max 6 characters: RXXXX , RXXXXX diplomatic 2001: 'DD'DDDDD , 'DD'DDDD , 'DD'DDD , 'XX'DDDDD , 'XX'DDDD , 'XX'DDD diplomatic 2004: XXX*XX , XX*XX - where * = 'CD', 'XX', 'XS', 'HC' time limited: 7 characters, it begins with a region code + digits ( RDDDDDD ) Old RZ formats (SPZ) with the start of the new numbering system in 2001, the old SPZs were not discarded but are still valid (though no longer \n\t\t\t  assigned). Therefore the algorithm also supports and recognizes the following formats of the old SPZs: OODDDD OOLDDDD DDOODD DDOOLDD where: O = two letter code of the former czech districts: { BE,BI,BO,BK,BM,BS,BZ,BN,BV,CB,CE,CH,CK,CL,CR,CV,DC,DO,FI,FM,HB,HK,HR,HO,JE,JH,JI,JN,KA,KI,KD,KL,KH,KM,KO,KR,KV,KT,LB,LI,LN,LT,\n      \t\t\t\t\tMB,ME,MO,NA,NB,NJ,OC,OL,OM,OP,OS,OT,OV,PA,PU,PB,PC,PZ,PE,PH,PY,PI,PJ,PM,PN,PR,PS,PT,PV,RA,RK,RO,SM,SO,ST,SU,SY,TA,TC,TP,TR,TU,\n      \t\t\t\t\tUH,UL,US,UO,VS,VY,ZL,GT,ZN,ZR } L = { ABCDEFGHIJKLMNOPQRSTUVWXYZ } D = { 0123456789 } Algorithm accepts also combinations which probably were never assigned due to the vulgar or politically incorrect meaning \n\t\t\t  (e.g. PRC , PRD or STB - see. SPZ 1960-2001 info ). RZ patterns As the extension to the formats desribed above, the algorithm allows to define custom RZ patterns : a set of patterns\n\t\t\tto be used first when validating the input. If the input value matches any of these patterns, it is considered valid\n\t\t\tand no further checks are performed. The output Out Rz Type is then set to the CUSTOM_PATTERN value. Custom patterns are processed as the first ones, so they override any other patterns used by the algorithm (e.g. \n\t\t\tinternal patterns used to determine special RZ forms as defined by law etc.). The primary reason of the rzPatterns is to provide a way how to accept inputs which wouldn't be normally accepted\n\t\t\tbecause they do not meet the other rules (e.g. some new exception to RZ in the future etc.). RZ patterns syntax N - positive integral number { 123456789 } D - integral number { 0123456789 } R - letter of the region (see law from 2004): { ABCEHJKLMPSTUZ } A - alphabet - the pack of allowed characters that can be used in RZ: { ABCDEFHIJKLMNPRSTVXYZ } L - letter - { ABCDEFGHIJKLMNOPQRSTUVWXYZ } X - alphanumeric value - any value A or D The user's pattern can also contain the exact-character-rule enclosed in apostrophes such as 'X' , \n\t\t\twhere X is any character. Description of the input processing First phase - validation against the new RZ forms: This phase consists of the following checks (in this order): check user patterns check special forms of RZ as defined by law check common form of the RZ as defined by law No character fixing is performed in this phase, since the new RZ format is too generic. It is not possible to reliably recognize\n\t\t\tpossible typos from the correctly spelled characters. Second phase - validation against the old SPZ forms: This phase is run when: input string does not pass any of the checks in the first phase xthe input string only complies to the too generic checks of the first phase so there's possible ambiguity\n\t\t\t     (e.g. new RZ \"manipulative\" vs. old SPZ form). The part of the checks in this phase is also an attempt to fix incorrectly spelled characters. The fixing operation can be performed\n\t\t\there since the old SPZ had fixed format so it is possible to reliably determine what character (letter vs. digit) should be at\n\t\t\twhich position (in order to comply with the SPZ rules).\n\t\t\tFixing consists of transforming letters to the digits and vice versa using these transformation pairs: B <-> 8 I <-> 1 O <-> 0 S <-> 5 Z <-> 2 If the input passes just one of the phases, then it is assigned to the phase's era (new format vs. old format). If there's ambiguity when deciding whether given input represents old or new format, the input year hint is used. \n\t\t\tIf no input hint is\tavailable then the old SPZs are preferred (since the number of old SPZ \"in the wild\" is probably \n\t\t\tmuch higher then the amount of special RZs they can be confused with).",
    "Step Properties": [
      "Alphabet - Defines the character set\n\t\t\t\tconsidered to be an alphabet for RZ. The character set defined\n\t\t\t\tin the law from 2004 is used as the default value set: {ABCDEFHIJKLMNPRSTVXYZ}.",
      "Id - Step identification string.",
      "Omit Invalid RZ - When set tofalseand\tthe verification of RZ value has failed, the input\n\t\t\t\tRZ will be copied to the data output property. If the flagsRZ_UNKNOWN,RZ_BAD_FORMAT,RZ_YEAR_MISMATCH,RZ_TYPE_MISMATCHare set during the verification process, the input value is incorrect.\n\t\t\t\tIf the flagRZ_NULLis set, the output value is incorrect and empty.Default value:True.",
      "Out Rz - Where the verified RZ should be saved to.",
      "Out Rz Type - Sets out the type of the RZ as determined by the algorithm. The output value is just one of the following:GENERIC- the input corresponds to the common (standard) RZ formatHISTORICAL- the input corresponds to the historical RZ formatRACING- the input correspondes to the racing RZ formatTESTING- the input corresponds to the testing RZ formatDIPLOMATIC- the input corresponds to the diplomatic RZ formatMANIPULATIVE- the input curresponds to the permanantly manipulative RZ formatTIME_LIMITED- the input corresponds to the time limited RZ formatUSER_PATTERN- the input corresponds to the some user patternOLD_FORMAT- the input corresponds to the old SPZ format",
      "Registration Year - This determines the year of the vehicle registration. If the year of the registration is after 2001,\n\t\t\t\tthe set vehicle registration number must comply to the new RZ form. Checks only if the mentioned RZ is verified as valid.",
      "Rz - Specifies the vehicle registration number data source.",
      "Rz Patterns - AList of propertieswhich associates a set ofrzPatterntags.",
      "Vehicle Type - Vehicle type for more\n\t\t\t\taccurate verification of RZ. Meanings of single values are as follows:0- Type is not defined1- Moped2- Motorcycle3- Other types of vehiclesChecks only if the mentioned RZ is verified as valid and the set vehicle type is different from 0.\n\t\t\t\tIf the set vehicle type is different than above, the RZ is assignedRZ_BAD_TYPE.",
      "When Condition - Expression that must be satisfied for this step to be executed.",
      "Scorer - Element which stores basic scoring settings."
    ]
  },
  {
    "Step Name": "Validate RZ SK",
    "Step Details": "Detailed Description This step checks the validity of a vehicle registration number\n\t\t(formerly known as SPZ in Czech Republic). If the registration number\n\t\tis determined to be invalid, the step attempts to identify possible typing\n\t\terrors and, if successful, returns the corrected number. While correcting, the\n\t\tstep attempts to switch the following pairs of characters:\n\t\t'O' <-> '0', 'S' <-> '5', 'Z' <-> '2' and 'I' <-> '1'. The step does not contain any internal cleaning mechanism (removing invalid\n\t\tcharacters), and expects cleansed data on input. In this case, cleaning\n\t\tmeans removing special characters (pause, spaces, etc.) and character conversion\n\t\tto uppercase form. First, the step tries to verify uncorrected (not repaired) data as a new RZ and then\n\t\tas an old SPZ.  If no match is found, first step tries to find any corrections\n\t\tfor the format of new RZ and then for the format of an old SPZ. Any RZ which is corrected is given the flag FLAG_FIXED . The following rules are checked in the RZ: 1. a minimum of 6, and a maxium of 7, characters 2. characters are formed by Latin alphabet capitals and Arabic digits 3. a matching with some of the possible forms of Slovak RZ or SPZ If a match is found with some of the RZ or SPZ forms, then the step validates the production year\n\t\tacquired from the Prod Year property with the specified format. If the number is in old format of vehicle number\n\t\tand the production year is after 1997, the step will set the BAD_DATE flag.\n\t\tThis comparison does not have an influence on the\n\t\tvalidity of RZ, it only serves as supplemental information. If no production year information exists,\n\t\tthe step will set the DATE_MISSING flag. If the property of production year is not used\n\t\t(not connected), the 'missing' flag will not be set and validation of the production year will not be performed. If the vehicle registration number is recognized as valid, the original form (eventually the corrected form)\n\t\tis recorded to the output rzOut . For recording an invalid RZ to the output, it is necessary to specify\n\t\tthe Omit Invalid RZ parameter and set its value to false .",
    "Step Properties": [
      "Id - Step identification string.",
      "District Lookup File Name - Dictionary file that contains\n\t\t\tcodes of Slovak regions for RZ. Defined index value is a region code. For more information see theDictionary files.",
      "Omit Invalid RZ - Property that influences the output\n\t\t\tvalue of RZ if the RZ number is invalid. 'True' means that the invalid RZ is not written, 'False' means that the input (original)\n\t\t\tvalue is written.Default value: True.",
      "Out Rz - Column that stores the RZ output value.",
      "Prod Year - Determines the production year of the vehicle. If the production year is after 1997, \n\t\t\t\tthe set vehicle registration number must be valid. Checks only if the mentioned RZ is verified as valid.",
      "Rz - Column that contains the RZ input value.",
      "Vehicle Type - Not used. Defined for backward compatibility with theValidate RZstep.",
      "When Condition - Expression that must be satisfied for this step to be executed.",
      "Scorer - Element which stores basic scoring settings."
    ]
  },
  {
    "Step Name": "Validate DIC",
    "Step Details": "Detailed Description This step recognizes new and old VAT ID (DIČ) values. The step also cleans incorrect characters within the VAT ID,\n\t\t\t verifies checksum (if possible) and VAT ID against a dictionary file containing registered tax offices.\n\t\t\tUsing the preserveInputValue property, the final output value can be selected (original or\n\t\t\tcleansed). NOTE: VAT ID = DIČ.",
    "Step Properties": [
      "When Condition - Expression that must be satisfied for this step to be executed.",
      "Id - Step identification string.",
      "Cn Lookup File Name - Dictionary file with list of companies.\n\t\t\t\tFor more information about the dictionary seethe detailed description here.",
      "Convert Old Vid - Specifies whether VAT ID number with the format valid before 2004/05/01 should be converted to\n\t\t\t\tthe format valid after 2004/05/01.Default value:False",
      "Fo Lookup File Name - Dictionary file with a list of tax offices.\n\t\t\t\tFor more information about the dictionary seethe detailed description here.",
      "In - Column that contains the input VAT ID value.",
      "Out Code - Column that stores company (Register of Economic Subjects) code if successfully verified.\n\t\t\t\tAcceptable codes are IC (company identification number), RC (birth number) or any other\n\t\t\t\tnumber which can play role of code within a VAT ID (e.g. the number of the company's cash register).\n\t\t\t\tApplies to old VAT ID values only.",
      "Out Fin Off - Column that stores the tax office code for possible VAT ID verification.\n\t\t\t\tApplies to old VAT ID values only.",
      "Out Vid - Column that stores the output VAT ID value.",
      "Preserve Input Value - Specifies whether the original(true) or the cleansed(false) value is stored\n\t\t\t\tin the output (when cleansing is performed)Default value:False.",
      "Treat Old Vid As Invalid - Specifies whether a VAT ID number given the format valid before 2004/05/01 should be considered as invalid.Default value:False",
      "Scorer - Element which stores basic scoring settings."
    ]
  },
  {
    "Step Name": "Validate VIN",
    "Step Details": "Detailed Description This step validates a VIN (Vehicle Identification Number) and returns its verified value. NOTE: Using VIN as a resource for vehicle identification/validation can be misleading when additional\n\t\t\tinformation is missing. Currently, VIN is used by different manufacturers with more or less strict rules.\n\n\t\t\tThe functional capabilities of this step are strongly dependent on the information of manufacturer codes, which are\n\t\t\tsupplied using World Manufacturer Identifier (WMI) dictionaries. Furthermore, the same manufacturer may use different VIN formats when developing different model series'.\n\t\t\tFor example, The Czech company Skoda uses a check-sum number for its Octavia model series,\n\t\t\thowever no check-sum is used for the Felicia model series. The step considers a VIN as valid only if it satisfies the following conditions/properties: The VIN has the correct length and contains only allowed characters. The VIN check-sum number (CRC) is correct (only if the VIN contains a CRC). The manufacturer's name (if available in the input data) conforms to the one recognized by WMI. Whether the VIN type was successfully and unambiguously identified using: unique WMI unique combination of WMI and input manufacturer's name If the manufacturing year derived from the VIN agrees with the input year value hint.\n\t\t\t\t(Applies only if the position of the year digit within the VIN is known and the input year hint value is available) This step checks the characters within the input VIN and, if the invalid characters I , O and Q are found, they are replaced with the numbers 1, 0 and 0 respectively. NOTE: The input value must be preprocessed to the appropriate format (i.e., transformed to uppercase and with no diacritics\n\t\t\t(accents) - a sample of the configuration needed for cleansing the data before using this step is in the example). The step checks the validity of the CRC number, but only if VIN type format is successfully identified.\n\t\t\tRecognition of the manufacturer is possible according to the manufacturer's name and model series. In addition, the CRC check-sum information\n\t\t\tcan be derived. If such information is not discernible (the VIN has CRC marked as unknown in the dictionary Vin Info File Name ), \n\t\t\tthe VIN is checked to see if: it conforms to the US form of VIN, which must contain a CRC in the 9th position (US VINs starts with a digit from range 1-4) there's CRC at the default position (9th position) which agrees with the potential CRC VIN calculation Based on the value of the Do Repair parameter, the step can repair an incorrect CRC (when used with setting of \n\t\t\tthe VIN_CRC_FIXED flag). The repair is allowed only if the algorithm is sure that such VIN contains CRC digit (the value of the Out Has Check Digit = TRUE ). If the manufacturer's name hint is available on input, it is compared with the name(s) derived from the WMI code of the given VIN -\n\t\t\tthis comparison is diacritics (accents) insensitive and case insensitive.\n\t\t\tIf the output manuOut is defined, the value of the ascertained/verified manufacturer's name is stored here - if\n\t\t\tit is unique for a given WMI. If it is not unique for a given WMI (more manufacturers exists for a given WMI), an empty value\n\t\t\tis stored, meaning that it was not possible to choose the proper manufacturer. If validation is not successful, the appropriate flags are set/scored (see \n\t\t\tflags descriptions). The step checks also the production year. The production year processing depends on the production-year-code-position defined in the VIN lookup: If the VIN type is known to have production year in the VIN, the production year is decoded from the position defined by the VIN lookup,\n\t\t\t\t\t  the TRUE value is written to the output Out Has Prod Year and decoded production year value is\n\t\t\t\t\t  written to the Out Year .\n\t\t\t\t\t  Then the year-value decoded from the VIN is compared to the Year input hint. If the hint is not empty and\n\t\t\t\t\t  the hint value does not match the value decoded from the VIN, the VIN_PY_MISMATCH flag is set and VIN is considered invalid. If the VIN type is known to not have production year - then all production year checking mechanism is skipped since there's no value in\n\t\t\t\t\t  the VIN which can be validated. This situation is not marked by any flag.\n\t\t\t\t\t  In this case the FALSE value is written to the output Out Has Prod Year and null value\n\t\t\t\t\t  is written to the Out Year property. If it is not known whether the VIN contains the production year or not, the null value is written to the output Out Has Prod Year .\n\t\t\t\t\t  Then the algorithm checks if there's CRC defined in the VIN. If there's no CRC in the VIN, it is invalid or the guessed CRC is invalid, \n\t\t\t\t\t  the algorithm skips further production-year checks and writes null to the Out Year property. If the CRC validation was successful - then the algorithm tries to read the production year code from the\n\t\t\t\t      default (10th) position in the VIN (it assumes that if the VIN supports the CRC digit, there's a good chance that it supports production\n\t\t\t\t\t  year either). If the value in that position is a valid production year code, it is decoded, flag VIN_PY_GUESS is set and the value is written to the Out Year . Then the decoded year value is compared to the input year hint (if defined) and in case of the mismatch the VIN_PY_MISMATCH flag is set. \n\t\t\t\t      But in this case the flag does not invalidate whole VIN since we are not absolutely sure that the code at 10th position is really represents the \n\t\t\t\t      production year code. If the value at 10th position does not represent valid production year value, the further production year checking is skipped. Since the production year is encoded in the VIN using just single position (which may have 30 different values), each 30 years a new \n\t\t\tcycle starts. Therefore the same value in the VIN may be interpreted as multiple production years. E.g. value 'A' stands either for \n\t\t\tthe year 1980 or 2010, 2040 etc. When algorithm detects this ambiguity, the input production year is considered and the closes matching production \n\t\t\tyear is selected. If there's no input hint specified, the newest possible value is chosen. Due to the production year vs. real year difference, \n\t\t\teven currentYear +1 value is returned as valid. Manufacturers often use their own \"model year\" when marking the year within the VIN, which does not\n\t\t\tconform to the calendar year: it begins on the first of September in the previous year and ends on the 31st of August\n\t\t\tin a year that is nominally the same as model year. At the same time, it may occur that the value of year included in VIN is less\n\t\t\tthan the input year. To eliminate such differences the step can, when validating the VIN, accept a year range\n\t\t\tplus or minus one year. This is considered as valid, and the flag VIN_YEAR_AROUND is set (if the attribute Allow Year Around is set to true (which is the default value)). Based on the verifications listed above, the VIN is written to the output in accordance with the setting of the Omit Invalid VIN parameter. This parameter determines whether only VINs which have satisfied all tests\n\t\t\t(true) or VINs which did not satisfy all tests, are written to the output.",
    "Step Properties": [
      "Id - Step identification string.",
      "When Condition - Expression that must be satisfied for this step to be executed.",
      "Vin - Column that contains the input VIN.",
      "Year - Column that contains the input production year hint.",
      "Manu - Column that contains the manufacturer's name hint (the value compared with the value determined from the VIN).",
      "Out Vin - Column that stores the final output VIN.",
      "Out Year - Contains year production information determined from the input VIN.Since this information is more manufacturer-specific, the determined value is reliable only if there's noVIN_PY_GUESSflag set. \n              \tOtherwise it is a best-guess based on the state of the VIN (see more detailed description below).if this binding is non empty thenVIN_PY_GUESSis not set: algorithm has successfully detected the production year information in the VINVIN_PY_GUESSis set: the algorithm does not reliably detected the production year value, so the value written\n\t\t\t\t\t\t\t\t  to the output is just a guessif this binding is empty thenVIN is known not to have production year informationor the algorithm does not have any information about the production year presence in the VIN (typically when manufacturer \n\t\t\t\t\t\t\t\t\t  does not provide any information about its VIN structures)",
      "Out Manu - Since the manufacturer information is encoded directly in the WMI part of the VIN, this information (when present) is pretty reliable.if this binding is non empty then algorithm successfully detected the manufacturer of the vehicle (manufacturer is \n\t\t\t\t\t\t  defined in the WMI/VIN lookup)if this binding is empty thenmanufacturer is unknownor there are multiple manufacturers using this type of VIN and the algorithm cannot choose the correct one. \n\t\t\t\t\t\t\t\t  In such caseVIN_MANU_MULTIPLEflag is set",
      "Out Has Check Digit - Reliably determines whether given type of VIN contains CRC information or not.\n              \tThis value reliably says whether given VIN type contains CRC or not. It works as a three-state boolean\n              \twhere individual values represents the following states:TRUE- we know that this type of VIN contains CRC informationFALSE- we know that this type of VIN does not contain CRC informationnull(empty) - we don't know anything about CRC information in this type of VINNotes:this output value is not related to the overall validity of the VIN. In other words: this output binding remains same \n              \t\t\t   for all VINs of given type regardless on the result of the particular VIN validation (e.g. CRC check result).even if we do not know anything about the CRC in the VIN we try to check it. In such case this output binding containsnull(as explained above), but flags may report some more information:NO_CRC= CRC check was not successful, but since we are not sure whether there's CRC in the VIN,\n                    \t\t\t\t\tthis is not considered an errormissingNO_CRC- we tried to check the CRC and the check passed. So there's probably CRC in\n                 \t\t\t\t\t  default CRC position but since we can't be sure about it, the hasCrc binding remains empty.",
      "Out Has Prod Year - This property reliably says whether given VIN type contains production year or not. It works as three-state boolean where individual values represents \n              \tthe following states:TRUE- we know that this type of VIN contains product year information . If this is the case then mismatch with the \n              \t      \t\t\t\t\t  potential input product year hint is considered an error.FALSE- we know that this type of the VIN does not have product year information. If this is the case then input product \n              \t      \t\t\t\t\t  year hint is silently ignored (since we have no information from the VIN to compare it with).null- we have no reliable information about this VIN, so we cannot decide whether this VIN contains product year or not. \n              \t      \t\t\t\t\t  If this is the case then algorithm tries to perform the production year guess - possible detected year mismatch is then\n              \t      \t\t\t\t\t  flagged asVIN_PY_MISMATCHbut it does not affect VIN validity (since we are not completely sure about the guess).",
      "Out Model - Column that stores the ascertained vehicle model, if it is present in the dictionary.",
      "Allow Year Around - When this parameter is set to \"true\", years with a difference of (maximum) 1 are considered as the same\n\t\t\t\t(disagreement between the vehicle's \"model year\" and production year, the calendar year and the car company's production year).\n\t\t\t\tIf this parameter is set to \"false\", only years with no difference will be considered as the same (ifVIN_YEAR_AROUNDis not set).Default value:True.",
      "Do Repair - Repairs incorrect value of checksum number (CRC). This correction is performed only if the error is detected\n\t\t\t\t(i.e., the step does not correct the CRC when it has been determined that no CRC is present within the VIN).\n\t\t\t\tThis repair is allowed only if the algorithm is sure that there's CRC in the VIN.",
      "Omit Invalid VIN - Property that influences the output value of the VIN if there is an error\n\t\t\t\t(a VIN with some of the errors described above).Truemeans that a NULL value is written,Falsemeans that the input (original) value is written.Default value:True.",
      "Vin Info File Name - Dictionary file that contains information about known VIN formats.\n\t\t\t\tThe dictionary contains records with individual VIN types.\n\t\t\t\tFor more information about the dictionary seethe detailed description here.",
      "Wmi File Name - Dictionary file that will be used for manufacturer's name\n\t\t\t\tverification.\n\t\t\t\tFor more information about the dictionary seethe detailed description here.",
      "Scorer - Element which stores basic scoring settings."
    ]
  },
  {
    "Step Name": "Versioned File System Component",
    "Step Details": "Detailed Description Defines a set of versioned filesystem folders. The component is used to monitor configuration file changes in order to enable reloading new configuration without a server restart. Modified files are detected on the refresh command execution and are replaced to the currently running configurations.",
    "Step Properties": [
      "Disabled - Specifies whether component should be disabled.",
      "Required Role - User role required to execute the refresh command.",
      "Versioned Folders - Paths (absolute or relative to the location of server configuration file) to filesystem folders that should be versioned."
    ]
  },
  {
    "Step Name": "Web Console Component",
    "Step Details": "Detailed Description Enables Admin Center.",
    "Step Properties": [
      "Cache Template - Enables caching of Admin Center page templates.",
      "Custom Menu Categories - Specifies custom menu categories. Each custom category corresponds to a section in the Admin Center navigation panel.",
      "Custom Menu Items - Specifies custom menu items (pointing to external servers and/or document folders) in the Admin Center.",
      "Disabled - Specifies whether component should be disabled.",
      "Listeners - Comma-separated list of names of HTTP listeners where the console should be accessible. If the attribute is missing, the console will be deployed on all listeners.",
      "Required Role - User role required to access the Admin Center."
    ]
  },
  {
    "Step Name": "Web Lookup",
    "Step Details": "Detailed Description This step downloads a web page for each input row. Optionally, it can\n\t\t\ttransform the page\n\t\t\tusing an XSLT transformation and extract\n\t\t\tinformation which can be stored\n\t\t\tin\n\t\t\tcolumns. The address of the\n\t\t\tdownloaded web page is constructed using data in\n\t\t\tthe input\n\t\t\trow. Various\n\t\t\tproblems can be scored, e.g. timeout, no response, wrong\n\t\t\ttransformation, etc. Parameters to be sent to the server are stored in Post Parameters (when using the POST\n\t\t\tmethod) and depend on the technology used. Some\n\t\t\tcommon ones are: ASPX Requires posting of the attributes that are involved in the creation\n\t\t\tof the VIEWSTATE value, the VIEWSTATE value itself and the form element being clicked.\n\t\t\tThe VIEWSTATE value is used by ASPX to keep the\n\t\t\tstate and values of the ASP form.\n\t\t\tValues that are involved in the\n\t\t\tVIEWSTATE value are controls\n\t\t\thaving\n\t\t\tsome value (text fields, combo boxes, etc.). To inform the server\n\t\t\tabout the clicked button it\n\t\t\talso needs to send the respective button\n\t\t\tID. Values that need to be sent\n\t\t\tcan be determined by\n\t\t\tanalyzing HTTP\n\t\t\tdata communication between the server and client. This can be\n\t\t\tdone\n\t\t\tusing a specialized tool, such as: Firefox page info (tab Form) - for version 3 this can be done\n\t\t\t\t\tusing, for example,\n\t\t\t\t\tthe Page Info Forms and Links plugin\n\t\t\t\t\t(https://addons.mozilla.org/cs/firefox/addon/7978) Ethereal network analyzer and others A simple example: a form has one entry text field, idValue , a button, cmdSend ,\n\t\t\twhich performs the form's submit action, a button, cmdReset , which performs the form's\n\t\t\treset action and labels, label1 and label2 . So, the values that\n\t\t\tindicate that the form was filled in and sent\n\t\t\tfor processing are: __VIEWSTATE (can be read from the HTML page containing the form) label1, label2 idValue cmdSend (may have any value, since the server only needs to know that cmdSend was clicked) JSP Requires sending attributes that determine the result plus the\n\t\t\tcontrol that was selected (clicked). So, using\n\t\t\tthe same example as for\n\t\t\tASP, the values that need to be sent are: idValue cmdSend (again with any value - the server only needs to know about this\n\t\t\t\t\tcontrol)",
    "Step Properties": [
      "Before Transformation Debug File - The downloaded page transformed into XML can be stored in a\n\t\t\t\tfile for\n\t\t\t\tdebugging purposes. Only the page for the last row is\n\t\t\t\tstored.",
      "After Transformation Debug File - The same asBefore Transformation Debug File, except that the transformed page is stored.",
      "Transformation File - The XSLT\n\t\t\t\ttransformation file.",
      "Cookies - Name of the column where to load/store cookies from/to. If this\n\t\t\t\tbinding is defined together with\n\t\t\t\ttheInit Url Patternattribute, thenInit Url Patternis used to retrieve cookies\n\t\t\t\tand these cookies are stored to this\n\t\t\t\tcolumn for later usage (in subsequent steps that needs to\n\t\t\t\tuse same\n\t\t\t\tcookies for example). WhenCookiesattribute is set andInit Url Patternis empty, then cookies are read from this column and send to the\n\t\t\t\tremote server.",
      "Init Url Pattern - Address that\n\t\t\t\tdetermines which URL to use to read cookie-related data. It needs to\n\t\t\t\tbe filled in if the server requires, for example, the session ID to\n\t\t\t\tbe sent with the HTTP request. This\n\t\t\t\tis the case for ASPX, JSP and\n\t\t\t\tother page types. Before processing individual rows, the\n\t\t\t\tstep\n\t\t\t\tconnects to this address using the GET method and retrieves all\n\t\t\t\tcookie-related\n\t\t\t\tdata. These data are then sent to the server with each\n\t\t\t\tHTTP request.",
      "Post Parameters - Defines parameters to be sent to the website as POST parameters.",
      "Use Post Method - Determines\n\t\t\t\twhether to use the POST method. It is set to false by default, which\n\t\t\t\trepresents the GET method.",
      "Url Pattern - The URL where\n\t\t\t\tweb pages are downloaded from. The URL is constructed\n\t\t\t\tdynamically.\n\t\t\t\tThe column values can be substituted into the pattern as\n\t\t\t\t\"{column_name}\". Example:\n\t\t\t\t\"http://www.web.com/?id={column_name}\".\n\n\t\t\t\tValues inserted to \"{column_name}\" are automatically URL encoded\n\t\t\t\t(percent-encoding). Use\n\t\t\t\toption \"{column_name|noEscape}\" to disable\n\t\t\t\ttranslation of special\n\t\t\t\tcharacters.",
      "Timeout - The time in\n\t\t\t\tseconds in which the page must be downloaded for each row.\n\t\t\t\tIf the\n\t\t\t\tdownloading time is longer than this, nothing is stored in the\n\t\t\t\toutput columns,\n\t\t\t\tthe row\n\t\t\t\tis scored and processing continues to the next\n\t\t\t\trow.",
      "Response Debug File - The responses can be written into the file. If the file name contains a percent character\n\t\t\t\tthen it is replaced by the number of the record for which the response was generated.\n\t\t\t\tIf a file with the same name exists then it is rewritten.",
      "Request Debug File - The same asResponse Debug Filebut for the request.",
      "Columns - Each column maps a text extracted from downloaded and\n\t\t\t\toptionally\n\t\t\t\ttransformed page to\n\t\t\t\ta column in the input row.",
      "Scorer - Element which stores basic scoring settings.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Word Analyzer",
    "Step Details": "Detailed Description This step identifies words by parsing input column values\n\t\t\tusing a specified separator and replaces them with symbols -\n\t\t\tnames of dictionaries specified by step properties. Furthermore these\n\t\t\tcreated strings can be matched against patterns. If some pattern is\n\t\t\tmatched then this pattern goes to the output instead of the created\n\t\t\tstring. If the string is not found, there are three possible actions and\n\t\t\tthree corresponding output columns (plus a trash column): output column dest - not found strings are removed output column maskWithUnknownLeft - not found strings\n\t\t\tare left as they were in the input if no pattern is found output column maskWithUnknownReplaced - not found strings\n\t\t\tare replaced by the string defined in the symbolForUnidentifiedWords property Unknown strings can be copied into the trash column.\n\t\t\tThe step is repeated for each pair of source and destination columns.",
    "Step Properties": [
      "Analyzed Columns - List of analyzed columns<analyzedColumn>containing source and destination column pairs.",
      "Trash Separator - If unidentified words are found then these words are stored in a column optionally defined on each analyzed\n\t\t\tcolumn and these words are separated by this property value.",
      "Copy Separators - Specifies whether the data input separator characters should be copied into the\n\t\t\tdata output. Default behavior is 'true'.",
      "Separator - Definition of characters used to divide/parse the input data into\n\t\t\t\ttokens (words).\n\t\t\t\tSeecharacter set definition syntax.",
      "Lookups - List of<slListOfValue>elements containing filenames\n\t\t\t\twith String Lookup dictionaries and their symbols (names).\n\t\t\t\tNote: At least one dictionary must be specified within this step - at least\n\t\t\t\tone of either<slListOfValue>or<mlListOfValue>must be supplied (and be nonempty).",
      "Symbol For Unidentified Words - Specifies a character or character sequence that should be used to\n\t\t\treplace words not present in the specified dictionaries.",
      "When Condition - Expression that must be satisfied for this step to be executed.",
      "Id - Step identification string."
    ]
  },
  {
    "Step Name": "Word Tokenizer",
    "Step Details": "Detailed Description Step creates a new record for each word token (word, punctuation mark, abbreviation etc.)\n                in the input string. For optimal performance, one input record should be one sentence. Prepare large documents\n                with SentenceTokenizer . OpenNLP model files are necessary for this step. Model files can be trained with the WordTokenizerTrainer step.",
    "Step Properties": [
      "Id - Step identification string.",
      "Input Column - Column containing input sentences.",
      "Model File - OpenNLP TokenizerModel file.",
      "Output Column - Column for output tokens.",
      "Record Descriptor Column - Record descriptor column."
    ]
  },
  {
    "Step Name": "Word Tokenizer Trainer",
    "Step Details": "Detailed Description Step creates the model for WordTokenizer . Tokens are either separated by a whitespace or by a special <SPLIT> tag. At least one <SPLIT> tag is necessary in the input stream, otherwise the model training will fail. The following sample shows the correct input format: Pierre Vinken<SPLIT>, 61 years old<SPLIT>, will join the board as a nonexecutive\ndirector Nov. 29<SPLIT>. Mr. Vinken is chairman of Elsevier N.V.<SPLIT>, the Dutch\npublishing group<SPLIT>. Rudolph Agnew<SPLIT>, 55 years old and former chairman of\nConsolidated Gold Fields PLC<SPLIT>, was named a nonexecutive director of this British\nindustrial conglomerate<SPLIT>. More information about training can be found at OpenNLP website ( http://opennlp.apache.org ).",
    "Step Properties": [
      "Abbreviations - Known abbreviations column.",
      "Cutoff - The minimal number of times a feature must be seen, otherwise it is ignored.",
      "Id - Step identification string.",
      "Input Values - Training input column.",
      "Iterations - Number of training iterations.",
      "Model File - Output model file."
    ]
  },
  {
    "Step Name": "Workflow Server Component",
    "Step Details": "Detailed Description Adds Workflows section to Admin Center and enables running workflows on the server via the Admin Center, HTTP Request and OnlineCtl.",
    "Step Properties": [
      "Disabled - Specifies whether component should be disabled.",
      "Http Listener - TheHTTP Dispatcherlistener on which workflows can be executed via HTTP requests.",
      "Log Level - Maximum level of events that will be logged into server console. The levels are:-1: Log everything.0: Log nothing.1: Log everything related to Workflow Server Component.2: Log starts and ends of workflows, state changes of workflow tasks.3: Log activities of workflow tasks.4 and higher: Log activities of other subloggers (e.g., DQC plans started by workflows). Workflows that log on the level 4 or higher are rare.",
      "Resources Folder - Relative (to the server configuration file) or absolute location where to store supporting and resource files. Currently, there are stored:File containing ID generation state for running task instances.Log files created by the workflow for individual workflow instances.",
      "Show Runtime Configuration - Specifies whether the runtime configuration is displayed in the ONE Runtime Server Admin when using the Run DQC Process task.",
      "Sources - Defines named sources of the workflow configurations.",
      "State Storage Provider - Defines which backend should be used for storing the workflow execution states. Currently, there are two storage provider implementations: DB State Storage Provider and File State Storage Provider."
    ]
  },
  {
    "Step Name": "Xml Parser",
    "Step Details": "Detailed Description This step is similar to the Xml Reader step .\n\t\tIt can create several output streams from input stream containing xml strings. Rows of one output stream can be\n\t\tlogically children to rows from another stream of rows. For example let's assume input xml contains set\n\t\tof clients and each client has several addresses. Such record can be read as a stream of clients\n\t\tand another stream of addresses, where each address can have an id taken from the client stream (for\n\t\tfurther identification of the corresponding client). Each stream can also use shadow columns and referencing\n\t\tvalues from other columns of the input stream for further identification of particular records.\n\n\t\tThe input stream passes through to the default output \"out\", with the possibility to specify a column that will\n\t\tbe filled with explanation in case of an error parsing the associated xml.\n\n\t\tThe namespaces official specification can be a little tricky and therefore if they are used it is good to read namespace specification .",
    "Step Properties": [
      "Input Column - The input column containing xml strings to be parsed.",
      "Id - Step identification string.",
      "Namespaces - When the xpaths contain namespaces then the namespaces must be used by prefixes\n\t\t\t\twhich are defined here. Eg. for xpath expression \"pr:el\" the prefix \"pr\" with its\n\t\t\t\tnamespace must be defined here.",
      "Data Streams - Output end points defined on the root level.",
      "Data Format Parameters - General parameters for data formatting. This configuration is applied to\n\t\t\tall specified columns unless the column defines its owndataFormaParameterssection.\n\t\t\tFor more information, seeDataFormatParameters"
    ]
  },
  {
    "Step Name": "Xml Reader",
    "Step Details": "Detailed Description It can create several output streams from one xml file where rows of one stream can be\n\t\tlogically children to rows from another stream of rows. Eg. file can contain records\n\t\tof clients and each client have several addresses and then the xml file can be read\n\t\tas a stream of clients and for each client there can be read addresses where addresses\n\t\tare put to another stream. The streams can also be independent. The namespaces official\n\t\tspecification can be a little tricky and therefore if they are used it is good to read namespace specification .",
    "Step Properties": [
      "File Name - The source xml file.",
      "Id - Step identification string.",
      "Namespaces - When the xpaths contain namespaces then the namespaces must be used by prefixes\n\t\t\t\twhich are defined here. Eg. for xpath expression \"pr:el\" the prefix \"pr\" with its\n\t\t\t\tnamespace must be defined here.",
      "Data Streams - Output end points defined on the root level.",
      "Data Format Parameters - General parameters for data formatting. This configuration is applied to\n\t\t\tall specified columns unless the column defines its owndataFormaParameterssection.\n\t\t\tFor more information, seeDataFormatParameters"
    ]
  },
  {
    "Step Name": "XmlReaderWrap",
    "Step Details": "",
    "Step Properties": []
  },
  {
    "Step Name": "Xml Writer",
    "Step Details": "Detailed Description The data output is a standard XML file containing records stored as XML nodes.\n\t\t\tMultiple data flows (called data streams in the step configuration) can be stored\n\t\t\tto a single file, each in different format. The individual records are stored as\n\t\t\ta single element with arbitrarily complex inner structure (attributes and subelements)\n\t\t\twith the column values stored either as text nodes or attribute values. Note: endpoints this step exposes depend on data streams defined in the step\n\t\t\tconfiguration.",
    "Step Properties": [
      "Id - Step identification string.",
      "Encoding - Encoding of the output file.",
      "File Name - Name of the output file.",
      "File Template - The overall structure of the resulting XML file, the fragment must be\n\t\t\t\ta valid XML file. The individual data streams are referenced by<dataStream name=\"data stream name\" />elements. These\n\t\t\t\tspecial elements will be replaced by the actual data when the step is run.\n\n\t\t\t\tEvery whitespace character (space, tab, new line break, etc.) defined in the template will be\n\t\t\t\twritten to the output XML in the same way unlessindentproperty is set.",
      "Data Streams - Definition of data streams. Each stream adds a new input endpoint and must have its\n\t\t\t\tcounterpart infileTemplate. Data read from the input are written\n\t\t\t\tat the place where the stream is referenced infileTemplate",
      "Indent - Indicates to generate indented XML. In that case content of textual nodes is trimmed\n                (i.e. whitespace characters removed from both sides).\n                Indenting character is a tab character.Default value:true."
    ]
  }
]