{
  "title": "Introduction to writing steps in Java",
  "headers": [
    "Introduction to writing steps in Java",
    "Contents:",
    "Introduction",
    "Requirements",
    "Concepts",
    "Configuration class",
    "Runtime class",
    "End point",
    "RecordFormat",
    "Validation",
    "Access to the file system",
    "Record",
    "Column Binding",
    "End Point Listener",
    "Plugin",
    "Messages",
    "Distribution",
    "Examples",
    "Simple filter step",
    "Less simple filter step",
    "Complex step",
    "Adapting the step for Spark processing",
    "Introduction",
    "Spark Driver and Spark Runtime classes",
    "Spark examples"
  ],
  "content": "This document briefly describes how to write new steps in Java. It is not \n     a complete guide; it only presents basic information on how to write steps.\n\nThere are two basic types of steps:Filter Stepshave one input end point and one output end point and can only\n    take rows one by one from the input end point; They change some values in the\n    row and return it to the output end point; There are some exceptions to this behavior, but this rules applies to most cases.Complex Stepsare general steps which can have as many input and\n    output end points as the developer chooses.\n\nThe properties are Java properties returned by getters and set by setters. The properties\ncan be of simple Java type, arrays and classes.\n\nValidationis processed in thevalidateStepmethod.\n\nThe predecessor of filter steps containsinandoutend points. \nEach complex step can have as manyend pointsas you like.\nHowever, unlike filter steps, allend pointsmust be created and must registerlisteners, \nwhich listen to changes in inputend points(which will likely affect subsequent outputend points).\n\nThe filter class implements thefilterRowmethod which is called on each row from the input end point and \nperforms some operation(s) above the row.\n\nThe complex class implements therunmethod which is called only once.\n\nThis section describes how to adapt your steps in order to run them via Big Data Engine in Spark mode.\n     Apache Spark is an open-source project that allows running a distributed processing on clusters (e.g. Hadoop or Spark clusters).\n     It is highly recommended to know the basics of Spark before reading this section.\n\nBig Data Engine is using mainly the RDD-based approach in Spark mode. Each step input and output is an RDD that can be transformed, filtered, merged or split.  \n    If you prefer to work with Dataframe API, it is possible to use inside steps, however, step connections strictly use only RDD. \n    In case if you want to implement a Reader or Writer step, it is possible to read data using Dataframe API and then convert it to RDD to pass the data in the plan.\n    If you want to use Dataframe API within a dataflow, it is required to convert RDD to Dataframe on Input, and then convert it back before specifying the output.\n\nIn order to adapt a step for the Spark engine, you will need to implement several interfaces in the step configuration.\n    We will start with the most generic interface for DQC Stepcom.ataccama.dqc.tasks.common.spark.ISparkStep.\n    The interface will require a developer to implement two functions:\n\nISparkDriver createSparkDriver(IAlgorithmContext ac)This is the function that should return a class that contains logic about RDD behavior, \n\ton this level, you work directly with RDD and not with separate records.\n\nISparkRuntime createSparkRuntime(IAlgorithmContext ac) throws ExceptionThis is the function that should return a class responsible for implementing internal record logic, e.g. changing values inside rows/records.\n\tIt is possible to reuse the same DQC logic if it is not required to adapt it for Spark, \n\te.g. record-level transformation is valid for both DQC and Spark engines, \n\tbut for complex join or aggregations operation, it is required to re-implement the logic for Spark engine.\n\nTo implement ISparkDriver interface, it is suggested to use an abstract helper class that implements \n\tall the required functionality -com.ataccama.dqc.tasks.common.spark.SparkDriverBaseor its simplified version forcom.ataccama.dqc.tasks.common.spark.SimpleSparkDriverfor steps that only has 1 input and 1 output.\n\nFirst, let's start with adapting our Sample Complex step example that redirects two streams on input into a single output endpoint.\n    Luckily for us, this operation already exists in Spark and is calledUnion, \n    it takes two RDDs on input and merge them into a new single RDD on output.\n    Since we validate that the record format for both inputs is the same and we do not need to implement any logic on single records in RDD specifically,\n    this step does not require any implementation if theISparkRuntimeclass.//First, lets declare that our step implements interface ISparkStep\npublic class SampleComplexStep extends ComplexStepConfigBase implements ISparkStep\n\n\n\n\t\t\n@Override\npublic ISparkDriver createComplexStepSparkDriver(IAlgorithmContext ac) {\n\treturn new ComplexStepSparkDriver();\n}\n\n//Actual implementation of the Union operation via Spark RDD API which extends a class helper SparkDriverBase\nprivate static class ComplexStepSparkDriver extends SparkDriverBase {\n\t@Override\n\tpublic void init(ISparkStepRuntimeContext ctx, ISparkConnection[] inps, ISparkConnection[] outs) {\n\t\tIRdd<SparkRecord> rddA = inps[0].getRdd();\n\t\tIRdd<SparkRecord> rddB = inps[1].getRdd();\n\t\tIRdd<SparkRecord> unionRdd = rddA.union(rddB);\n\t\touts[0].setRdd(unionRdd);\n\t}\n}\n\t\n//As mentioned before, we do not need to implement runtime class for this step\n@Override\npublic ISparkRuntime createSparkRuntime(IAlgorithmContext ac) throws Exception {\n\treturn null;\n}That was quite easy example, now let's imagine that we want to join two streams as in the previous example,\nbut also add a new column to the output that will specify whether the record was from the left data stream\nor right data stream. As you can see, in this example we would need to add and calculate a new column \non the record level which will require to use theISparkRuntimeclass.@Override\npublic ISparkDriver createSparkDriver(IAlgorithmContext ac) {\n\treturn new ComplexStepSparkDriver();\n}\n\nprivate static class ComplexStepSparkDriver extends SparkDriverBase {\n\t@Override\n\tpublic void init(ISparkStepRuntimeContext ctx, ISparkConnection[] inps, ISparkConnection[] outs) {\n\t\tIRdd<SparkRecord> rddA = addColumn(ctx, inps[0].getRdd(), \"left stream\");\n\t\tIRdd<SparkRecord> rddB = addColumn(ctx, inps[1].getRdd(), \"right stream\");\n\n\t\tIRdd<SparkRecord> unionRdd = rddA.union(rddB);\n\t\touts[0].setRdd(unionRdd);\n\t}\n\t\t\n\tprivate static IRdd<SparkRecord> addColumn(ISparkStepRuntimeContext ctx, IRdd<SparkRecord> rdd, string inputStreamName) {\n\t\t//here we are using a map function in order to apply changes on record level and sending the function to be used\n\t\treturn rdd.map(new FunctionCtxWrap<SparkRecord, SparkRecord, ComplexStepSparkRuntime, string>(ctx, inputStreamName) {\n\t\t\t@Override\n\t\t\tprotected SparkRecord call(SparkRecord record, ComplextStepSparkRuntime rtm, string inputStreamName) throws Exception {\n\t\t\t\treturn rtm.apply(record, inputStreamName);\n\t\t\t}\n\t\t});\n\t}\n}\t\n\n//Function returns the newly created Spark Runtime for this step\n@Override\npublic ISparkRuntime createSparkRuntime(IAlgorithmContext ac) throws Exception {\n\tIRecordFormat outFmt = out.getRecordFormat();\n\treturn new ComplexStepSparkRuntime(outFmt, createConverts(ac.getProcessContext()));\n}\t\n\t\nprivate static class ComplextStepSparkRuntime implements ISparkRuntime {\n\tprivate final IRecordFormat outFmt;\n\tprivate final ConvertEntry[] converts;\n\tprivate ComplextStepSparkRuntime(IRecordFormat outFmt, ConvertEntry[] converts) {\n\t\tthis.outFmt = outFmt;\n\t\tthis.converts = converts;\n\t}\n\n\tprivate SparkRecord apply(SparkRecord rec, string inputStreamName) {\n\t\t//SparkRecord implements the IRecord interface as it was seen in the previous examples\n\t\tSparkRecord output = new SparkRecord(rec.getRecordId(), new Object[outFmt.getColumns().length], rec.getOuterRecord());\n\t\t//Here, you can work with SparkRecord directly and modify it as you wish.\n\t\t//e.g. applying transfromations or adding new columns.\n\t\t//you can reuse the same logic as you had done for DQC or implement a new one\n\t\t\t\n\t\t//<Complex logic for converting a SparkRecord and adding a new column>\n\t\t\t\n\t\treturn output;\n\t}\n}In this example we have shown how to create a new runtime class with specific Spark logic. \nHowever, if you are sure that the logic for Spark records and DQC records is exactly the same, \nyou can reuse the same DQC runtime, just make sure that implement ISparkRuntime interface.private class TheInstance extends FilterStepBase Implements ISparkRuntime {\n\t// DQC logic\n}"
}