[
  {
    "title": "Changing component name",
    "url": "https://community.ataccama.com/data-quality-catalog-94/changing-component-name-1806",
    "question": {
      "author": "JTH",
      "timestamp": "[No timestamp]",
      "content": "Hi,\nIn ONE Desktop > Servers > OnePlatform > Monitoring Projects > Create new > Post Processing Plan > client_dataset_invalidrecords (as a name to the component) > a new component will be created accordingly.\nOnce created, is it possible to rename the component?"
    },
    "answers": [
      {
        "author": "ekaterina.ponomareva",
        "timestamp": "4 hours ago",
        "content": "Hi ​\n@JTH\n,\nThank you for your question!\nUnfortunately, there is no easy and straightforward way to rename the component. It would be a lot faster, easier and less error-prone to delete the component and recreate it with a new name.\nPlease let me know if you have any further questions.\nKind regards,\nEkaterina"
      }
    ]
  },
  {
    "title": "Power BI Lineage Scanner - Lineage for 'Internal Use' Reports",
    "url": "https://community.ataccama.com/data-quality-catalog-94/power-bi-lineage-scanner-lineage-for-internal-use-reports-1818",
    "question": {
      "author": "cmagnano",
      "timestamp": "[No timestamp]",
      "content": "Hi All,\nAt Bupa, within our Power BI environment we have a number of reports that have been tagged with a sensitivity label of ‘Confidential - Internal Use Only’ or ‘Highly Confidential: Internal Use Only’ using Microsoft’s sensitivity labels.\nhttps://learn.microsoft.com/en-us/purview/sensitivity-labels\nWhen reports have one of these labels applied, the data returned by the Export Report API in Power BI is encrypted. This is the main API call that the Ataccama scanner uses to extract metadata/lineage information for ingestion into the catalog. As the data returned is encrypted, the scanner is not currently processing it and skipping to the next report.\nReports - Export Report In Group - REST API (Power BI Power BI REST APIs) | Microsoft Learn\nHas anyone come across scenario in their own environment and been able to identify any workarounds/ways forward to capture lineage for those reports with a higher sensitivity label? Either on the Ataccama side or the Microsoft side?\nBelow is what is currently being returned by the API."
    },
    "answers": []
  },
  {
    "title": "Best Practices for Data Quality Rules based on joins",
    "url": "https://community.ataccama.com/data-quality-catalog-94/best-practices-for-data-quality-rules-based-on-joins-1758",
    "question": {
      "author": "bobparry",
      "timestamp": "[No timestamp]",
      "content": "Hello!\nWondering if anyone has experience with creating data quality rules for values in joined tables.\nFor example, we have 2 tables (Table A and B), related to each other through a shared attribute. I want a data quality rule where the value of a column in table A must be lower than the value of a column in Table B. My only idea for this is to create a SQL catalog item joining the two tables, and creating a dq rule on the SQL catalog item. This works, but has flaws, I can’t have this data quality rule contribute to the overall data quality of Table A, nor will users who find this table in the catalog know of all the identified data quality issues.\nDoes anyone have a better way of doing this, or is there a built-in tool I’m not familiar with that anyone has used?\nThanks!"
    },
    "answers": [
      {
        "author": "OGordon100",
        "timestamp": "25 days ago",
        "content": "Hello ​\n@bobparry\n!\nThis is a very good question. Unfortunately due to product limitations, you are exactly correct - if you need a rule involving multiple tables you must create a new Catalog Item (either with SQL, ONE Desktop Components, or the Transformation Editor in v15+), and attach your rule to it. It then has no impact on your original table’s Data Quality.\nWe are, however, actively tracking this exact use case, as it is a very common one. Could you please create an idea for this on the community? Thank you!"
      },
      {
        "author": "Lisa Kovalskaia",
        "timestamp": "13 days ago",
        "content": "​\n@bobparry\nHi! To add to what ​\n@OGordon100\nwrote, for the time being you might make use of catalog item relationships to help users discover SQL CIs related to source CIs. With a bit of training users can get used to checking those relationships whenever they are browsing the catalog. Of course if you want to build some automations based on DQ scores, you'd need some logic to translate DQ issues from the SQL CI to the source CI - that's something you could potentially achieve with the metadata model customizations and an automation workflow that would export DQ results from an SQL CI asset and import it to the source CI asset. Let me know if you are interested and have questions.\nThat said, please do raise a feature idea here on the community too! Hearing from our customers helps our product team tremendously to prioritize and refine their plans. Thank you!"
      },
      {
        "author": "bobparry",
        "timestamp": "13 days ago",
        "content": "Thanks Gordon and Lisa, I’ve created the idea here:\nEnable Data Quality Rules based on Joins | Community\n​\n@Lisa Kovalskaia\n, can you elaborate on what you mean about the automations based on DQ scores? Do you mean having a metadata field, and populating that with the Data Quality score from the SQL catalog item, and then have users look at that field rather than the Overall score?\nTo your point about the item relationships, is there any way to automate the creation of those? Especially for SQL Catalog items it feels like the Source catalog items they are related to should automatically appear in the Relationships pane, given the environment knows exactly how they are related. It’s a great idea to encourage users to check the relationships in cases like this, it would be great if it was a little easier to have those relationships populated."
      },
      {
        "author": "Lisa Kovalskaia",
        "timestamp": "1 day ago",
        "content": "​\n@bobparry\nregarding automations, yes, what you’re describing is close to what I had in mind - although I think this would be more useful for triggering some automatic actions on the source item, such as DQ alerts or data remediation tasks or DQ reporting. I wouldn’t necessarily highlight that new field in the UI. I feel like for end users that would add more confusion than benefit, since the metrics in the additional DQ metadata field and the standard Data Quality tab would not always align.\nRegarding relationships, I agree it would be a nice improvement to see them created automatically based on the SQL CI query. Currently they have to be created manually. Depending on the size of your catalog you may also consider configuring an orchestration workflow  to automate that, e.g. parsing the SQL CI query, looking for the CI with the same name and path, and creating the relationship. Then again, if there’s say one new SQL CI per week or per month, it might be easier for the data stewards to keep setting these relationships. What do you think?"
      }
    ]
  },
  {
    "title": "Why can’t I select multiple schemas under a single source with multiple connections in Data Observability?",
    "url": "https://community.ataccama.com/data-quality-catalog-94/why-can-t-i-select-multiple-schemas-under-a-single-source-with-multiple-connections-in-data-observability-1764",
    "question": {
      "author": "Gerson",
      "timestamp": "[No timestamp]",
      "content": "I'm facing an issue with selecting schemas to observe for schema change detection in Observability for one of our sources(Sybase).\nContext:\nIn our SQL database source setup, we have a single connection that includes multiple schemas, and Observability allows us to select multiple schemas without issue, as shown below.\nHowever, for the Sybase source, we're unable to select multiple schemas under a single connection. I only get the option to choose one connection and one schema from the dropdown. (See screenshot below.)\nIdeally, I’d like to select all dbo across all connections to monitor for structural changes.\nQuestion:\nHas anyone experienced a similar limitation?\nIs there a recommended approach for configuring Observability for schema checks on sources that have multiple connections and schemas under a single source — especially in cases like a Sybase database, where multiple connections are used?\nAny guidance or best practices would be greatly appreciated."
    },
    "answers": [
      {
        "author": "Lisa Kovalskaia",
        "timestamp": "1 day ago",
        "content": "Hi ​\n@Gerson\n, you should be able to select multiple schemas when configuring data observability on a source with multiple connections. There shouldn’t be any difference between technologies in that regard -- but to select multiple schemas you first need to pick a connection. There isn’t a way to browse all schemas across all connections in one screen.\nIs it possible that the dropdown on your screenshot is the list of connections, not schemas? Once you select one of the connections, can you pick some schemas, then select another connection and pick more schemas there?"
      }
    ]
  },
  {
    "title": "How to schedule profiling of specific schema in the database Or subset of that schema if schema is too big.",
    "url": "https://community.ataccama.com/data-quality-catalog-94/how-to-schedule-profiling-of-specific-schema-in-the-database-or-subset-of-that-schema-if-schema-is-too-big-1816",
    "question": {
      "author": "Ojaswini",
      "timestamp": "[No timestamp]",
      "content": "Hello,\nIs this possible now in latest version? I am using 15.4.0. Requirement is to schedule profiling of specific schema in the database. Or subset of that schema if schema is too big (set of tables).\nThanks in advance."
    },
    "answers": []
  },
  {
    "title": "Business glossary - synonyms (part 5)",
    "url": "https://community.ataccama.com/data-quality-catalog-94/business-glossary-synonyms-part-5-1512",
    "question": {
      "author": "Albert de Ruiter",
      "timestamp": "[No timestamp]",
      "content": "Hi Community,\nThe first posts about the business glossary dealt about applying more\nterm types\n(attribute and entity terms), and how they get context by adding\nrelationship types\n. Following posts described an implementation for\nmultilingualism\nand how to apply\ncomputed content\n. Especially the last one was rather tough to digest, so for my last post about the glossary I promise a soft landing.\nI think one thing needs some further attention and that regards synonyms.\nDefining a synonym for a term is existing functionality: you can define a synonym by pointing to another term.\nThis means though that when you want to define a synonym for a term, you would have to define a new term first. For that term you would also have to define its metadata. In fact the new term including metadata would be redundant with the original term. Or would you leave out the additional metadata?\nWe chose to use a different approach: for a synonym only its name is relevant, as the other metadata would be the same.\nSo our implementation is\nCreate a new entity, named for instance\nglossarySynonym\n(initially I used\ntermSynonym\n, but when upgrading to version 14.5 we discovered that it came with a new entity with that name).\nIn entity\nterm\ndefine a new property of type embedded object array from entity\nglossarySynonym\n.\nAfter adding this new property in the page layout of the terms in the glossary you can add synonyms that are not terms in itself, but simply synonyms. For instance for term Loan you could define synonyms Credit and Moneylending, which would show as follows in the glossary.\nWell, this is the last part about possible customizations for your glossary. I hope they inspired you to consider customization of the glossary yourself.\nFeel free to share your ideas or comments as well, so that together we can ensure that all the members of the Community benefit from it!\nKind regards,\nAlbert"
    },
    "answers": [
      {
        "author": "mpersons",
        "timestamp": "4 days ago",
        "content": "Thanks for the info about Synonyms.  Is there a way in Ataccama to discover potential synonyms automatically?"
      },
      {
        "author": "Albert de Ruiter",
        "timestamp": "1 day ago",
        "content": "Hi ​\n@mpersons\n, not that I know of. But I think my approach would be (1) discuss internally which synonyms are actually used in your organization or (2) use AI to discover synonyms. The latter approach will likely give you synonyms faster, but also present synonyms that are irrelevant (so simply not used) in the organization.\nKind regards,\nAlbert"
      }
    ]
  },
  {
    "title": "Configure Export plan  for DQ Lookup Items",
    "url": "https://community.ataccama.com/data-quality-catalog-94/configure-export-plan-for-dq-lookup-items-1813",
    "question": {
      "author": "Susan24us",
      "timestamp": "[No timestamp]",
      "content": "Hello,\nI have two questions I would love to inquire about:\n1. Exporting Lookup Items in Ataccama\nI would like to know if it's possible to configure exports for lookup items in Ataccama.\nWhen I try to export my lookup items, the export option is greyed out. When I hover over it, I see a message saying:\n\"No export plan exists for the selected lookup items.\"\nUnder\nGlobal Settings > Import and Export\n, I see an option to create a new export plan. I tried using this, but got to a stage where I need to input a code for the export plan. I found the following JSON online and was wondering if this would work as-is or if it needs any tweaks:\n{\n  \"\nname\n\":\n\"Export_All_Lookup_Items\"\n,\n  \"\ndescription\n\":\n\"Exports all lookup items for backup or migration\"\n,\n  \"\ntype\n\":\n\"LOOKUP_ITEM\"\n,\n  \"\nformat\n\":\n\"JSON\"\n,\n  \"\nfilters\n\":\n{\n    \"\nlookupTypes\n\":\n[\n\"ALL\"\n]\n,\n    \"\ntags\n\":\n[]\n,\n    \"\nprojects\n\":\n[]\n}\n,\n  \"\noutput\n\":\n{\n    \"\ncombineIntoSingleFile\n\":\ntrue\n,\n    \"\nfileName\n\":\n\"lookup_export.json\"\n,\n    \"\nincludeMetadata\n\":\ntrue\n}\n}\nWould this JSON-based export plan enable the export feature for lookup items? If not, what modifications would be required? Also, is there any official documentation or reference material available for creating export plans in Ataccama?\n2. Visibility of DQ Rules\nWe have some Data Quality rules that are not visible to all users.\nIs there a way to configure DQ rules so they are visible across the platform for all users ? do i have to manually share each rule before its visible to all users ? Any guidance on rule sharing, visibility settings, or permission configuration would be greatly appreciated.\nThank you in advance!"
    },
    "answers": [
      {
        "author": "Rianna",
        "timestamp": "4 days ago",
        "content": "Hello ​\n@Susan24us\n,\nFor your point 2, we have a solution using GraphQL. We created a group in Ataccama Web and set up in graphQL an automated process which shares the DQ rules with this group on a regular basis so that all users can see all DQ rules.\nExample of the end result:\nEvery DQ rule created will be shared with a group and all users in that group can view the DQ checks (select the share button in top right hand corner and the sharing with the group will automatically be completed with the below solution).\nFirst set up a Group in Ataccama Web\nSet up a new group in “Global Settings” of Ataccama ONE Web. Then in the Group roles, assign the Keyclock role profiles in the “Identity provide role” column. For example for us, all the Ataccama users have the Keyclock role “MMM_read-only” assigned to their user access accounts so assigning this role to the group means everyone will be able to see the DQ rules.\nFor the next steps I recommend you reach out to Ataccama to help you set up the below as it is a bit complex. They helped us set it all up previously. Manhar Seewooth was the superstar who helped us with this <manhar.seewooth@ataccama.com>.\nYou will need to set up a GraphQL query and a component in Ataccama ONE Desktop which utilises a JSON call and when run, it will share the DQ rules with the group you created. Ataccama can provide you all the code/details required as they did with us.\nThen finally you will need to set up a workflow to automatically run the Ataccama ONE Desktop component on a regular basis. For example, we set it up to run every few hours, so any rules created in the previous few hours will then be automatically be shared with the group you created when this workflow runs."
      }
    ]
  },
  {
    "title": "How to schedule a profiling job for specific schema vs all schemas within the data source?",
    "url": "https://community.ataccama.com/data-quality-catalog-94/how-to-schedule-a-profiling-job-for-specific-schema-vs-all-schemas-within-the-data-source-492",
    "question": {
      "author": "Catherine",
      "timestamp": "[No timestamp]",
      "content": "Use Case;  to configure a scheduled profiling job daily for a\nspecific schema of a data source\nwithin Ataccama Web application and not via Desktop?"
    },
    "answers": [
      {
        "author": "Daniel M",
        "timestamp": "2 years ago",
        "content": "Currently it’s not possible to create a scheduler for a specific schema via the web UI.\nI will provide this as a feature request to the product team."
      },
      {
        "author": "Ojaswini",
        "timestamp": "4 days ago",
        "content": "Hello, Is this possible now in latest version? I am using 15.4.0. Requirement is to schedule profiling of specific schema in the database. Or subset of that schema if schema is too big.\nThanks in advance."
      }
    ]
  },
  {
    "title": "DQ Monitoring project UAT/Test deployment",
    "url": "https://community.ataccama.com/data-quality-catalog-94/dq-monitoring-project-uat-test-deployment-503",
    "question": {
      "author": "Ojaswini",
      "timestamp": "[No timestamp]",
      "content": "Hello,\nI am new to DQ and Ataccama. Our code is in development environment and soon to be deployed in Test/UAT for UAT testing and later to Prod once we get UAT signoff.\nMy question is what is the way to export DQ monitoring project and its related components (like rules, etc) from Dev environment to UAT/Test environment? Is there any export capability? I have also heard that we may need to recreate it again in UAT environment.\nReally appreciate any direction in this.\nThanks in advance."
    },
    "answers": [
      {
        "author": "Maxim Kim",
        "timestamp": "2 years ago",
        "content": "I think if it is a single (or several DQ monitoring projects) it would be easier to re-create them in respected environments.\nOtherwise you can try Import & Export feature, although it probably doesn’t have what you need out of the box:\nIt probably is possible to add additional export plan that would be able to export DQ monitoring projects with all connected entities (Rules, Catalog Items, etc), but I am not 100% sure. Your environments might have different names/identifiers for datasource/connection that holds info about catalog items used in the project or even be absent at all.\nAnother option is to write a specific export and import plan in One Desktop that would do it, using metadata readers/writers."
      },
      {
        "author": "Ojaswini",
        "timestamp": "2 years ago",
        "content": "Thank you very much Maxim Kim,\nI have 2 monitoring projects to deploy. Seems like recreation of DQ Monitoring project is the easiest way.\nAlso is there any recommended practice from Ataccama for such kind of deployments."
      },
      {
        "author": "Susan24us",
        "timestamp": "12 days ago",
        "content": "I’m in a similar situation and would love to know how you resolved it. Did you have to recreate the monitoring project along with its associated rules in the test environment? Or did you create a deployment plan that included the monitoring project as well as the entities it was applied to in the test environment? Kindly clarify."
      },
      {
        "author": "Rianna",
        "timestamp": "6 days ago",
        "content": "I raised this with Ataccama back in February 2024 and they added exporting/importing monitoring projects across environments to some of their feature requests and they advised it likely wouldn’t be available by late v15/v16 so hopefully from v17 onwards it will be available but not guaranteed 😬\nI have found it easy exporting/importing DQ rules across environments though using the Export plan Maxim included in the screenshot above"
      },
      {
        "author": "Ojaswini",
        "timestamp": "6 days ago",
        "content": "Yes, DQ Rules can be exported, and I have tried before. (although I am still facing some issues in import). But for Monitoring projects, we need to recreate then in TEST and Prod environments. I guess no shortcuts :D"
      },
      {
        "author": "Rianna",
        "timestamp": "6 days ago",
        "content": "Are the issues with your importing related to DQ rules which contains either a lookup, a group by (aggregated rule) or a convert variable (e.g. convert from string to integer)? If so, I had these issues too and the bug was fixed in v14.5.3 and v15.2.0. If you have an older version I have some manual workarounds which aren’t ideal but helps a little"
      },
      {
        "author": "Ojaswini",
        "timestamp": "6 days ago",
        "content": "Mine is already v15.4 Dev and TEST both. But you are right. One of the rule is accuracy rule with lookup table. But before importing DQ rule I already prepared Lookuo table in advance to avoid any issue. But still Import gave errors and then I recreated them in TEST and then Prod. But still your workaround can be helpful for future. Thanks in advance.\nAnd to\nSusan24us\nYes DQ rules can be exported and imported if they are simple as\nRianna\nmentioned above.\nRegards,\nOjaswini"
      },
      {
        "author": "Rianna",
        "timestamp": "6 days ago",
        "content": "Hello ​\n@Ojaswini\n, here are my manual workarounds to see if they work for you 🙂\nI recommend exporting the “problem” rules you have on their own for these solutions. So only 1 DQ rule per Export/Import.\nSolution 1\nExport the DQ rule as usual and in the downloaded ZIP folder, open up the text file called “metadata00001rule”.\nIn the text file, find the code which looks like the below:\nAmend the green highlighted part to the ID of the\nnew\nlookup item you created in the other Ataccama environment you want to import the rule to.\n(You can find the ID in the web URL when you have the lookup open - example below).\nSave the text file changes you made and import as per the usual process you follow.\nSolution 2\nExport the DQ rule as usual and this time don’t make any changes to the downloaded zip folder.\nIn the import upload, select “import as draft and publish”.\nBecause the rule has an error, it should still be imported but not published. Then find the DQ rule in the “unpublished” area and update the rule implementation to select your newly created lookup item.\nAs I mentioned they are very manual solutions ☹️"
      },
      {
        "author": "Ojaswini",
        "timestamp": "5 days ago",
        "content": "Wow that’s interesting\nRianna\n. Thank you for this. Yes, I assumed that the issue might because of Id diff between DEV and TEST but was not sure where and how to correct that. Because Ataccama objects metadata is all about their respective IDs that we see in browser.\nI will definitely try this. Thanks again."
      }
    ]
  },
  {
    "title": "Profiling Date or Datetime",
    "url": "https://community.ataccama.com/data-quality-catalog-94/profiling-date-or-datetime-1763",
    "question": {
      "author": "jdordregter",
      "timestamp": "[No timestamp]",
      "content": "Hi!\nI am profiling tables and am looking at the profiles. I noticed that for date and datetime the profile does not show the MIN or MAX values. That is something I had expected though. Working on the data quality of this data, I really would like to know the MIN and MAX values for these attributes. Is there a way to change the configuration and include these profile statistics for these datatypes?\nkind regards, Jur Dördregter"
    },
    "answers": [
      {
        "author": "ekaterina.ponomareva",
        "timestamp": "24 days ago",
        "content": "Hi ​\n@jdordregter\n!\nThank you for your question!\nAs you are correctly pointing out, numerical statistics are calculated only for numeric attributes.\nHowever, you can have a look at the\nQuantiles -\nThey display the values that occur at designated intervals in the\nordered data set\n, with the\nfirst value on the list shown at 0% and the last value at 100%.\nIf your date type attributes have standardized format, like YYYY-MM-DD, it will work for you, as well.\nHope this will help you!\nKind regards,\nEkaterina"
      },
      {
        "author": "jdordregter",
        "timestamp": "22 days ago",
        "content": "Hi ​\n@ekaterina.ponomareva\n,\nI tried your solution, however it does not show the quantiles in my profiling, is this due to our version or the source used?\nkind regards, Jur Dördregter"
      },
      {
        "author": "ekaterina.ponomareva",
        "timestamp": "22 days ago",
        "content": "Hi ​\n@jdordregter\n,\nI can see that your data source is Hadoop, so it is indeed because of that.\nKind regards,\nEkaterina"
      },
      {
        "author": "Cansu",
        "timestamp": "15 days ago",
        "content": "Hi ​\n@jdordregter\nI’m closing this thread for now. If you have any follow up questions please feel free to share the in the comments or create a new post 🙋🏻‍♀️"
      },
      {
        "author": "jdordregter",
        "timestamp": "8 days ago",
        "content": "​\n@ekaterina.ponomareva\nGood for you to know: in the backend this information is stored. Should not be rocket science to show this information also in web application."
      },
      {
        "author": "Cansu",
        "timestamp": "6 days ago",
        "content": "Hi ​\n@jdordregter\n, ONE Desktop and Ataccama ONE does have some differences and thank you for taking the time to share your feedback. I’ve passed this on to our team."
      }
    ]
  },
  {
    "title": "Write business terms using graphQL",
    "url": "https://community.ataccama.com/data-quality-catalog-94/write-business-terms-using-graphql-1815",
    "question": {
      "author": "Hanish N",
      "timestamp": "[No timestamp]",
      "content": "Is there any mutation to create terms in Ataccama web using API."
    },
    "answers": [
      {
        "author": "Albert de Ruiter",
        "timestamp": "6 days ago",
        "content": "Hi ​\n@Hanish N\n,\nIn the graphQL Playground you can apply mutations like:\nmutation createBizTerm {\nbusinessTermCreate(\nnew: { _type: \"businessTerm\", abbreviation: \"MT\", name: \"Mutation Term\" }\nparentGid: \"00000000-0000-0000-0000-000000000001\"\nparentNode: \"metadata\"\nparentProperty: \"terms\"\n) {\nsuccess\nresult {\ngid\npublishedVersion {\nname\n}\ndraftVersion {\nname\n}\n}\n}\nMyself I have a different approach by using OneDesktop: reading the term details from a text file and writing with a Metadata Writer. This allows you to load new terms in bulk, which is not possible using the Playground.\nKind regards,\nAlbert"
      }
    ]
  },
  {
    "title": "How to connect to SAP HANA using jdbc driver",
    "url": "https://community.ataccama.com/data-quality-catalog-94/how-to-connect-to-sap-hana-using-jdbc-driver-1811",
    "question": {
      "author": "Vani",
      "timestamp": "[No timestamp]",
      "content": "Are you looking to connect to SAP HANA using jdbc driver? Here are the simple steps\nDownload and copy SAP HANA jdbc driver file into /opt/ataccama/one/dpe/lib/jdbc/ folder\nAdd a set of properties for SAP HANA driver in /opt/ataccama/one/dpe/etc/application.properties.( Check the attached document for properties)\nRestart DPE &  check logs of DPE service for any errors\nGo to ONE Web UI and new driver “SAP HANA” is available under connections “Select Connection type “drop-down menu"
    },
    "answers": []
  },
  {
    "title": "is catalog item overview page customizable?",
    "url": "https://community.ataccama.com/data-quality-catalog-94/is-catalog-item-overview-page-customizable-1804",
    "question": {
      "author": "NormanThink",
      "timestamp": "[No timestamp]",
      "content": "on v15.4, can we customize the snowflake table overview page after cataloging, to show or hide certain properties? I don’t see edit page option in top right 3 dot.\nI do see the “edit page template” option for MDM data, but not for snowflake tables.  is this intended product feature?\nThanks."
    },
    "answers": [
      {
        "author": "Albert de Ruiter",
        "timestamp": "8 days ago",
        "content": "Hi ​\n@NormanThink\n, I suppose it is not intended, but there is a workaround to get to the edit page-option after all: by pressing Ctrl-Alt-t you wil also find the edit page option.\nOne big warning though: you will not have the preview option now. You can save your changes with Ctrl-s, but if you made a mistake, there is a risk that you can no longer access the page anymore (speaking from experience I'm afraid).\nKind regards,\nAlbert"
      },
      {
        "author": "Lisa Kovalskaia",
        "timestamp": "8 days ago",
        "content": "​\n@NormanThink\nhi, when you open the three-dots-menu could you check if you have a scroll bar on the right? I do have the edit page template option on Snowflake tables, but it's at the very bottom of the list and I have to scroll there. Definitely not super obvious and I will flag that to our product designers!\nThere shouldn't be any difference in front end configuration options you have with different catalog entity subtypes, so if you can edit the page layout somewhere in the catalog you can usually assume it's possible for other subtypes, too."
      },
      {
        "author": "NormanThink",
        "timestamp": "8 days ago",
        "content": "Thanks ​\n@Albert de Ruiter\nfor your answer.  Will look into the workaround."
      },
      {
        "author": "Albert de Ruiter",
        "timestamp": "8 days ago",
        "content": "No problem ​\n@NormanThink\n. By the way, I use this workaround if I want to change the page layout of ‘Edit’-pages. Make sure then, because the prompt will be inside a field, to click outside that field first."
      }
    ]
  },
  {
    "title": "Handling Large Dataset while data profiling & Data Quality",
    "url": "https://community.ataccama.com/data-quality-catalog-94/handling-large-dataset-while-data-profiling-data-quality-1775",
    "question": {
      "author": "akshayl09",
      "timestamp": "[No timestamp]",
      "content": "Ataccama One Web:\nHow to handle large dataset with 1 B rows in the data set while working on data profiling & Data quality evaluation? (Best Practices)\nWhat is expected Infrastructure configuration required with respect to infrastructure in case such huge datasets?"
    },
    "answers": [
      {
        "author": "ekaterina.ponomareva",
        "timestamp": "11 days ago",
        "content": "Hi ​\n@akshayl09\n!\nThank you for your question.\nYou would indeed need to consider several things in the architecture when working with large data sets.\nScalability:\nDPE (Data Processing Engine) can be scaled horizontally to handle increasing data volumes.\nResource Allocation :\nMake sure memory is scaled appropriately.\nMonitor running tasks, identify performance issues or failures. Review, change, and adjust the configurations.\nThere is no one perfect configuration, you need to tweak for your process.\nIt will also depends heavily on the data source, e.g. it is possible to use ‘Pushdown’ feature for some of the data sources. It means executing data quality operations (profiling, rule evaluation) directly within the Big Data source (e.g., Snowflake, Databricks, Synapse). Ataccama ONE Gen2 translates data quality configurations into native SQL (or equivalent) and executes them on the platform.\nBenefits:\nReduced Data Transfer: Minimizes data movement, saving bandwidth and time.\nLeverages Platform Resources: Utilizes the processing power and scalability of the Big Data platform.\nImproved Performance: Faster execution due to optimized processing within the data source.\nAlso, in case you wanted to run Monitoring Project only on top of some part of the data set, you could take advantage of data slicing feature -\nhttps://docs.ataccama.com/one/latest/catalog-items/create-data-slice.html\nHope this helps! Let me know if you have any further questions.\nKind regards,\n​​​​​​​Ekaterina"
      },
      {
        "author": "Cansu",
        "timestamp": "8 days ago",
        "content": "Hi ​\n@akshayl09\n, I’m closing this thread for now. If you have any follow up questions please feel free to share them in the comments or create a new post 🙋🏻‍♀️"
      }
    ]
  },
  {
    "title": "Where are lookup tables stored?",
    "url": "https://community.ataccama.com/data-quality-catalog-94/where-are-lookup-tables-stored-1799",
    "question": {
      "author": "JohnY",
      "timestamp": "[No timestamp]",
      "content": "In Ataccama One web we can download Canada.lkp but on Ataccama Desktop we could not find this file (searched on drive C: as well).\nWhere are these lookup tables stored?\nHow to get a complete list of all of them?\nThanks!"
    },
    "answers": [
      {
        "author": "BrianF",
        "timestamp": "12 days ago",
        "content": "Hi JohnY,\nIf you download the .lkp files from the web application by clicking on the filename, they will be placed in your usual downloads folder. They can be copied from there into your ONE Desktop workspace, either through the file system, or directly into the folders in the ONE Desktop.\nIt is also possible to browse lookup files directly on the web application from the ONE Desktop, using the ONE Explorer pane in the ONE Perspective view, as per the attached image. They can be dragged from here into a plan or component, to either perform a lookup, or to extract the data via a lookup reader step.\nI hope that helps,\nBrian."
      },
      {
        "author": "JohnY",
        "timestamp": "11 days ago",
        "content": "Hi Brian,\nThank you for your detailed response!\nForgot to mention, we are using Ataccama v14.5.3 and our GUI differs from your screenshot. However I am able to create a DQ plan to get the list of lookup tables.\nMy new question is, how can I show the contents of a given lookup table?\nThanks!\nJohn"
      },
      {
        "author": "BrianF",
        "timestamp": "11 days ago",
        "content": "Hi John,\n14.5.3 can have this view as well. It is in the ONE Perspective, which enables you to browse metadata and explore assets on the server.  To access it, click the Ataccama logo in the top right of your desktop:\nLkp files are not human readable in normal text file viewers as they are indexed and optimised for Ataccama, but you can view the files by opening them in the desktop itself.\nThey are ‘physically’ stored in the Ataccama back end bucket system, MinIO, but that is not generally directly accessible to users.\nThanks,\nBrian."
      },
      {
        "author": "JohnY",
        "timestamp": "11 days ago",
        "content": "Hi Brian,\nThank you for your reply!\nDO these lookup files (e.g. canada.lkp) need to be refreshed by Ataccama customers?\nThanks!"
      },
      {
        "author": "BrianF",
        "timestamp": "11 days ago",
        "content": "Hi John,\nThe files that come with Ataccama DQ&C are there as examples of what is possible. They can be used as-is if they meet requirements, or they can be used  as a basis to create your own versions of lookup files if you so wish.\nThe expectation is that you’ll generally create your own if you need something more than what is available out of the box, either from a column in a Catalog Item, or by using the Lookup Builder step in the ONE Desktop, and uploading them to the web application.\nFurther documentation is here:\nhttps://docs.ataccama.com/one/latest/data-quality/lookup-items.html\nAnd you can access the more technical ONE Desktop documentation on this by clicking the circled ? icon in the bottom left of the Lookup Builder step.\nThanks,\nBrian."
      }
    ]
  },
  {
    "title": "How to get started: Snowflake x Ataccama ❄️⚙️",
    "url": "https://community.ataccama.com/data-quality-catalog-94/how-to-get-started-snowflake-x-ataccama-1803",
    "question": {
      "author": "Cansu",
      "timestamp": "[No timestamp]",
      "content": "Heard about Ataccama x Snowflake pushdown and wondering how to leverage it?\nYou’re in the right place. In this guide, we’ll walk through what Snowflake pushdown is, why it matters, and how to get started with it using Ataccama.\nWhat is Snowflake pushdown?\nRun processing directly in Snowflake by enabling pushdown. Pushdown allows Ataccama to offload data processing tasks directly to Snowflake—right where your data lives. That means faster performance, better security, and less infrastructure overhead.\nWhy use pushdown processing?\nEnabling pushdown unlocks multiple benefits:\n🚀 Performance\n: Evaluate millions of records in seconds. No need to pull large datasets into external servers for processing.\n🔐 Security\n: Processing is executed inside Snowflake. Results (and invalid samples, if configured) are returned—no external data transfers required, all processes run on secured and governed Snowflake warehouses.\n🖥️ Infrastructure\n: Leverage your existing Snowflake environment—no need to set up or maintain new processing servers.\n📈 Scalability\n: Pushdown enables efficient scaling by using Snowflake’s compute capabilities without additional Ataccama infrastructure.\nHow to start using the Snowflake pushdown?\nStep 1: Set Up Your Snowflake Environment\nIf you haven’t set up Snowflake yet, here are a few helpful resources to guide you through the basics:\nWarehouse Setup Walkthrough\nUsers, Roles, and Privileges\nDatabases and Schemas\nSnowflake Quick Start Guide\nStep 2: Create a Working Database in Snowflake Once your environment is ready, the next step is to create a working database. This will allow Ataccama to transfer the functions it needs to process your data.\nNote: This step should be done directly in Snowflake before you proceed with Ataccama configuration.\nWe recommend using Snowflake Worksheets for this. Simply click on + Worksheet in the UI and run the following SQL script:\nSnowflake + Worksheet\nHide content\nShow content\nCreate working database and stage\nCREATE DATABASE IF NOT EXISTS <working_db>;\nCREATE STAGE IF NOT EXISTS _ATC_ONE_STAGE;\nGRANT ROLE <read_data_role> TO ROLE <pushdown_role>;\nCreate role\nCREATE ROLE IF NOT EXISTS <pushdown_role>;\nAssign role to user\nGRANT ROLE <pushdown_role> TO USER <sample_user>;\nGRANT ROLE <pushdown_role> TO USER <another_user>;\nGrant access to database\nGRANT USAGE ON DATABASE <working_db> TO ROLE <pushdown_role>;\nGrant access to schema\nGRANT USAGE ON SCHEMA public TO ROLE <pushdown_role>;\nGRANT CREATE TABLE ON SCHEMA public TO ROLE <pushdown_role>;\nGRANT CREATE SEQUENCE ON SCHEMA public TO ROLE <pushdown_role>;\nGRANT CREATE FUNCTION ON SCHEMA public TO ROLE <pushdown_role>;\n*GRANT CREATE STAGE ON SCHEMA public TO ROLE <pushdown_role>;\nGrant access to stage\nGRANT READ ON STAGE _ATC_ONE_STAGE TO ROLE <pushdown_role>;\nGRANT WRITE ON STAGE _ATC_ONE_STAGE TO ROLE <pushdown_role>;\n*If there are permissions issues, it is recommended to add this grant.\nThis is our recommended approach based on best practices with using worksheets within Snowflake.\nIn Snowflake, select\n+ Worksheet\n, and follow these instructions. The following script creates the working database and grants access to defined user roles.\n✅ Best Practices for Setting Up Your Working Environment\nWorking Database Name\n: Replace\n<working_db>\nwith a meaningful name. This database temporarily stores Ataccama domain lookups and other operational data. It can be deleted when no longer needed. Multiple users can share the same working database, as long as they have access.\nSchemas (from v16.1+)\n: You can now define a custom schema for pushdown processing—no more relying on the default\npublic\nschema.\nRoles\n: Assign specific Snowflake roles (e.g.,\n<pushdown_role>\n) to define access control.\nUsers\n: Use\n<sample_user>\nand\n<another_user>\nplaceholders to represent actual Snowflake user accounts.\nAtaccama will use a database/schema for operational needs. It is used for lookup storage and Ataccama requires access to a named stage in schema which is used as a storage for Ataccama UDFs (needed for DQ evaluation). Snowflake SQL are missing functional pieces and Ataccama uses these UDFs to supply those missing parts.\nOnce you’ve completed this setup, you’re ready to integrate pushdown processing into your Ataccama workflows.\nGot questions or tips of your own? Share them with us in the comments below 👇\n!-->"
    },
    "answers": []
  },
  {
    "title": "data freshness support for databricks",
    "url": "https://community.ataccama.com/data-quality-catalog-94/data-freshness-support-for-databricks-1801",
    "question": {
      "author": "manidhar",
      "timestamp": "[No timestamp]",
      "content": "Hi Ataccama Community, Is the data freshness feature is supported for data bricks database now? In the documentation it says Snowflake, PostgreSQL, BiqQuery, and Oracle are supported. I am using Ataccama version 15.4"
    },
    "answers": []
  },
  {
    "title": "Delete dataset from report (Monitoring Project)",
    "url": "https://community.ataccama.com/data-quality-catalog-94/delete-dataset-from-report-monitoring-project-1308",
    "question": {
      "author": "JTH",
      "timestamp": "[No timestamp]",
      "content": "Hi,\nONE Web > Monitoring Project\nI have a MP A which contains 7 CIs, I want to copy this MP to create MP B. For MP B I only want to use 6 out of the 7 CIs as the last CI is unnecessary. I use the import configuration (great qol btw) and for the 7th CI I use same CI.\nAfter the import configuration has been completed I delete the 7th CI from the MP. Inside the Report tab I still see that same CI appear which is not ideal. You'd think clicking the three dots besides it lets me Delete it from the page, but to my surprise that option is greyed out. How do I get rid of this CI from the Report page?\n​​​"
    },
    "answers": [
      {
        "author": "ivan.kozlov",
        "timestamp": "9 months ago",
        "content": "Hi JTH,\nCould you please clarify which version of the product is this?\nThere are quite a few improvements available in 15.2 in comparison with what is available in v14 which allows much more granular control over things you’re trying to import.\nI think I've encountered the same issue as the one you’ve reported with “ghost CI’s appearing under the report tab.\nThe solution was do delete those items manually using GraphQL queries.\nFirst you need to run a query to identify the id of problematic CI aggregation and next query should delete it.\nPlease test these queries on some development system before running anything in production!\nYou can access GraphQL API here\nhttps://customer.envrionment.ataccama.online/playground/\nhttps://docs.ataccama.com/one/latest/one-apis/one-api.html#work-with-graphql\nquery {\n  monitoringProject(gid:\n\"PUT_PROJECT_ID_HERE\"\n) {\n    draftVersion {\n      configuration {\n        storedVersion {\n          aggregation {\n            storedVersion {\n              aggregations {\n                edges {\n                  node {\n                    storedVersion {\n                      aggregations {\n                        edges {\n                          node {\n                            gid\ntype\nstoredVersion {\n                              catalogItem {\n                                storedVersion {\n                                  _displayName\n                                }\n                              }\n                            }\n                          }\n                        }\n                      }\n                    }\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\nYou’ll need to use ID’s from the output of the first query in the 2nd one:\nmutation { \n  catalogItemAggregationDelete(gid:\n\"PUT_CI_AGGREGATION_ID_HERE\"\n) { \n    success \n  }\n# catalogItemAggregationPublish(gid: \"PUT_CI_AGGREGATION_ID_HERE\") {\n# success\n# }\n}\nI hope this helps.\nIvan"
      },
      {
        "author": "JTH",
        "timestamp": "9 months ago",
        "content": "Hi\n@ivan.kozlov\n,\nCurrently on 13.9.4 and also I don't have access to the first link, however I'll look into this with the team 🙂 thank you for the respone"
      },
      {
        "author": "Cansu",
        "timestamp": "9 months ago",
        "content": "Hi\n@JTH\nplease don’t hesitate to let us know if there is anything we can help you with further. I’m closing this thread for now, if you have any other questions please feel free to share them in the comments or create a new post 🙋‍♀️"
      },
      {
        "author": "Marcel-Jan",
        "timestamp": "9 months ago",
        "content": "I have tried this and it worked. The dataset is gone. The only weird thing is that the thing under which it was grouped (ISHS Sybase) remains.\nThe monitoring project ran successfully and did not profile the removed dataset though. So that's good."
      },
      {
        "author": "Cansu",
        "timestamp": "9 months ago",
        "content": "Thanks for the feedback\n@Marcel-Jan\n, I’ve shared the issue with the team 🙌"
      },
      {
        "author": "ivan.kozlov",
        "timestamp": "14 days ago",
        "content": "​\n@Marcel-Jan\nsorry, for such a lot time with reply. Just recently had to deal with cleanup of those Source aggregations for one of our customers and managed to find the right queries for that:\nYou can use this query to identify those blank source aggregations:\nquery q3a {\nmonitoringProjects(filter:\"configuration.aggregation.aggregations.any(aggregations.count() = 0)\",versionSelector:{draftVersion:true}){\ntotalCount edges { node { gid draftVersion {\nname\nconfiguration {storedVersion { aggregation { type storedVersion {\nname\naggregations(filter:\"aggregations.count() = 0\") {edges { node { type gid storedVersion { name\ndataSource { storedVersion { name }}\n}}}}\n}}}}\n}}}\n}\n}\nAnd this mutation to remove them (gid in this case is the gid of the aggregation you can get from the previous query):\nmutation m1b {\ndataSourceAggregationDelete(gid:\"79263f41-0000-7000-0000-0000110d5f50\") {\nsuccess\n}\n}\nThe delete mutation will put the project into draft state so you can review the change before publishing it."
      }
    ]
  },
  {
    "title": "🔐 Logging into Keycloak via SSO: Best practices for admins",
    "url": "https://community.ataccama.com/data-quality-catalog-94/logging-into-keycloak-via-sso-best-practices-for-admins-1792",
    "question": {
      "author": "OGordon100",
      "timestamp": "[No timestamp]",
      "content": "Hi everyone 👋\nBy default, Keycloak admin access is limited to a single account with fixed credentials provided by Ataccama. But just\nas we recommend using\nSSO for logging into the Ataccama platform\n, it’s also best practice for\nKeycloak administrators\nto log in the same way—for better security, auditability, and ease of access.\nHere’s a quick guide to help you set up and enable SSO login for Keycloak admins 👇\n1️⃣ Set up admin access in Keycloak\nFirst, log in to your Keycloak instance using the default admin credentials at:\n<your_env>/auth\nOnce logged in:\nSwitch to the\nataccamaone\nrealm\nGo to the\nRealm Roles\ntab\nDecide whether to use an existing role (e.g.,\nadmin\n) or create a new one (e.g.,\nKeycloak_Admin\n) based on your security requirements\n2️⃣ Assign the right permissions\nOpen the role you've selected or created:\nNavigate to the\nAssociated Roles\ntab and click\nAssign Role\nUnder\nFilter by\n, select\nClients\nNow assign the appropriate permissions. At a minimum, these should include:\nmanage-account\nview-applications\nview-profile\nview-realm\nFor full admin control, we recommend including all relevant permissions for identity providers, clients, users, groups, and events. We often see first-time clients selecting all Keycloak related roles for their admins. This aligns with the personas of administrators, and makes setup/ownership easier, and is therefore a reasonable approach.\n⚠️\nNote:\nThe\nimpersonation\nrole allows an admin to act as another user. While this is logged in the Keycloak Events log, it’s good to review whether this permission is necessary in your environment.\n3️⃣ Map the role to active directory\nUse your existing identity provider setup to\nmap the selected role to your AD group\n. Follow your standard process or\ncheck our documentation\nfor guidance.\n4️⃣ Logging in via SSO\nTo access the Keycloak admin console using SSO,\ndon’t use\n<env>/auth\n. Instead, go to:\n👉\n<env>/auth/admin/ataccamaone/console/#/\nThis will redirect users to the\nstandard Ataccama login page\n, where they can log in using SSO credentials.\nHave questions or need help with configuration? Let us know in the comments below! 👇"
    },
    "answers": []
  },
  {
    "title": "How to fetch Rule and Catalog item metadata from a monitoring project using Graphql API ?",
    "url": "https://community.ataccama.com/data-quality-catalog-94/how-to-fetch-rule-and-catalog-item-metadata-from-a-monitoring-project-using-graphql-api-1778",
    "question": {
      "author": "Apurva Kapoor",
      "timestamp": "[No timestamp]",
      "content": "Can we fetch Rule and Catalog item metadata from a monitoring project using Graphql API ? If yes, can you share the Graphql query ? I could not find it on ONE API Documentation"
    },
    "answers": [
      {
        "author": "anna.spakova",
        "timestamp": "15 days ago",
        "content": "Hi,\nso if you are looking for the graphQL API, you can start with something like this:\nquery\nq{\n  monitoringProjects(versionSelector: {draftVersion: true}\n){\n    edges{\n      node{\n        gid\n        draftVersion{\n          name\n## project name\nconfiguration{\n            draftVersion{\n              items{\n## catalog item\nedges{\n                  node{\n                    gid\n                    draftVersion{\n                      target{  \n                        gid\n                        draftVersion{\n                          name\n## name of the catalog items\n}\n                      }\n                      dqChecks{\n## DQ rule\nedges{\n                          node{\n                            draftVersion{\n                              target{\n                                gid\n                                draftVersion{\n                                  name\n                                }\n                              }\n                              mappings{\n## mapping between attribute of table and rule\nedges{\n                                  node{\n                                    draftVersion{\n                                      ruleAttribute{\n##rule attribute\ngid\n                                        draftVersion{\n                                          name\n                                        }\n                                      }\n                                      catalogItemAttribute{\n## catalog item attribute\ngid\n                                        draftVersion{\n                                          name\n                                        }\n                                      }\n                                    }\n                                  }\n                                }\n                              }\n                            }\n                          }\n                        }\n                      }\n                    }\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\nThis will give you all projects with all their configurations (even past ones), all catalog items and the applied DQ checks. You can play with it using our Playground to add more properties, see more:\nhttps://community.ataccama.com/data%2Dquality%2Dcatalog%2D94/guide%2Dhow%2Dto%2Drun%2Dgraphql%2Drequests%2Dpart%2Di%2D1543\nOr using the GraphQL plugin in your browser to discover the queries right away in the application.\nhttps://chromewebstore.google.com/detail/graphql-network-inspector/ndlbedplllcgconngcnfmkadhokfaaln\nAll the above can be also extracted in IDE using the ONE Metadata Reader.\nPlease let me know if there is anything else I can help you with.\nKind regards,\nAnna"
      },
      {
        "author": "Lisa Kovalskaia",
        "timestamp": "14 days ago",
        "content": "​\n@Apurva Kapoor\nhi! Assuming this question below is related to the same use case, please also have a look at the example plan in my response:\nA\nExtracting Rules and other Metadata Information from a Monitoring Project ?\nWould you rather go for the API-based approach or use the ONE Metadata reader steps? Would love to hear your feedback on which option you prefer and why!"
      }
    ]
  },
  {
    "title": "Defintion/Decription Field for Business Terms",
    "url": "https://community.ataccama.com/data-quality-catalog-94/defintion-decription-field-for-business-terms-1766",
    "question": {
      "author": "sjettawi",
      "timestamp": "[No timestamp]",
      "content": "Is anyone able to provide guidance on how to make the businessDefintion a required field when creating new terms?"
    },
    "answers": [
      {
        "author": "Robert Marinovic",
        "timestamp": "22 days ago",
        "content": "Hi ​\n@sjettawi\ndefault fields are not able to be modified. If you wish to do something similar you can create another attribute similar to businessDefinition and then modify as needed.\nIf you would like to make a attribute mandatory, inside of the Metadata Model you can set the flag for ‘Required’ to be on.\nSee screenshot attached.\nYou can find this in Metadata Model → Term(Or any entity you want to customize)"
      },
      {
        "author": "anna.spakova",
        "timestamp": "22 days ago",
        "content": "Hi ​\n@sjettawi\n, to add to Robert’s comment, it is possible to make the field mandatory even after it’s published in the model, however using custom Upgrade commands. You can use the following upgrade command:\n{\n  name: \"Make businessDefinition property mandatory\",\n  description: \"Sets property businessDefinition of node term as required\",\n  operations: [\n    {  \n\t\tname:\n\"AddConstraint\"\n,  \n\t\targs: {    \n\t\t\ttargetNode:\n\"term\"\n,\n\t\t\ttargetProperty:\n\"businessDefinition\"\n,\n\t\t\tconstraint: {                \n\t\t\t\tid:\n\"DEFINITION_REQUIRED\"\n,     \n\t\t\t\ttype:\n\"REQUIRED\"\n}\n\t\t}\n\t  }\t\n  ]\n}\nYou will save this as a json5 file and upload it through System changes:\nhttps://docs.ataccama.com/one/latest/metadata-model/system-changes.html#add-changes\n.\nHowever\n, this will cause that all your existing terms, that won’t have the business definition filled, will become\ninvalid\n(you will see an error in each of them). So I highly recommend to populate the field everywhere before uploading this. Also,\nit is recommended to do a DB backup before performing such change\n. For that reason it is much safer to create a new field as Robert described.\nYou can use a similar command to remove the constraint as well:\n{\nname\n:\n\"Remove required constraint from terms\"\n,\n  description:\n\"\"\n,\n  operations: [\n    {\n      name:\n\"RemoveConstraint\"\n,\n      args: {\n        targetNode:\n\"term\"\n,\n        targetProperty:\n\"businessDefinition\"\n,\n\"constraintId\"\n:\n\"DEFINITION_REQUIRED\"\n}\n    }\n  ]\n}\nKind regards,\nAnna"
      },
      {
        "author": "Cansu",
        "timestamp": "15 days ago",
        "content": "Hi ​\n@sjettawi\n, does either Anna or Robert’s solutions work for you? If yes, could you mark one as best answer please 🙏"
      }
    ]
  },
  {
    "title": "Export output location (Monitoring Project)",
    "url": "https://community.ataccama.com/data-quality-catalog-94/export-output-location-monitoring-project-1756",
    "question": {
      "author": "JTH",
      "timestamp": "[No timestamp]",
      "content": "Hi,\nIn ONE Web Monitoring Project there's an export tab with an export funcationality build within ONE Desktop. In ONE Desktop there's an output location.\nMight be an easy question, but how can I select an output location for the invalid records in this case to be send to a location inside my file directory? So basically what I want is the .csv/.xlsx file that come from invalidRecords to be sent automatically to a file directory folder I created. I understand the file can be downloaded from the Ataccama ONE Web export tab, but our customers have no use for this as the downloaded file will be downloaded in a file directory outside their working environment. Automatically moving the export invalid records to a file directory which they can access is a better solution in this case."
    },
    "answers": [
      {
        "author": "ekaterina.ponomareva",
        "timestamp": "28 days ago",
        "content": "Hello!\nThank you for your question!\nThere are 2 main options how to configure output location for Post Processing of Monitoring Projects results:\nConfigure it in a way that the file can be downloaded from Ataccama ONE - the file itself is physically stored on Minio.\nPlease see the details -\nHow to create Post Processing Plans? 🖋️\nSend the file to the specified location, e.g. S3 or ADLS. You  would need to configure the connection in the Global configuration and use that inside the post-processing plan. Please note that you need connectivity to this data source from the DPE that is running the monitoring project.\nhttps://docs.ataccama.com/one/14.5.x/dpm-admin-console/dpm-admin-console.html#configuration\nIf I understood your question correctly, you are interested in the 2nd option. Please let me know if you have any further questions.\nKind regards,\nEkaterina"
      },
      {
        "author": "JTH",
        "timestamp": "27 days ago",
        "content": "​\n@ekaterina.ponomareva\nthank you for your reply.\nI followed option 1 to the T. But ideally where is the file, the invalid records file, physically stored? So where does ONE Desktop generate the output to? In this case it's a .csv file."
      },
      {
        "author": "ekaterina.ponomareva",
        "timestamp": "27 days ago",
        "content": "Hi ​\n@JTH\n,\nIn option 1, the file is stored in MinIO. In option 2, the file would be stored according to your configurations.\nKind regards,\nEkaterina"
      },
      {
        "author": "Cansu",
        "timestamp": "15 days ago",
        "content": "Hi ​\n@JTH\n, I’m closing this thread for now. If you have any follow up questions feel free to share them in the comments or create a new post🙋🏻‍♀️"
      }
    ]
  },
  {
    "title": "Can we use reference data in case statement in Computed content?",
    "url": "https://community.ataccama.com/data-quality-catalog-94/can-we-use-reference-data-in-case-statement-in-computed-content-1738",
    "question": {
      "author": "sumisha",
      "timestamp": "[No timestamp]",
      "content": "Can we add reference data in case statement in Computed content?"
    },
    "answers": [
      {
        "author": "anna.spakova",
        "timestamp": "1 month ago",
        "content": "Hi ​\n@sumisha\n,\ncan you please elaborate a bit more on your usecase? The computed content is limited to the content of the mmm database, which means that it can only work with the metadata inside.\nKind regards,\nAnna"
      },
      {
        "author": "sumisha",
        "timestamp": "1 month ago",
        "content": "HI Anna, I have a table defined in One data\nI want to use this in my case statement in Context to get value in attribute field. LIke where sensitivity is PII, mark as confidential. List is very long, so I don;t want to hard code in case. Rather use this One data table or any other reference table here"
      },
      {
        "author": "sumisha",
        "timestamp": "1 month ago",
        "content": "My computed content looks like this:\nwith\nconnection_query as\n(select att.$attId$   as term_instance_id,\nti.$tiFrom$ as \"term_instance_from\",\nt.$tName$   as \"displayName\",\natt.$attPath$ as \"tiPath\"\nfrom $att$ att\ninner join $ti$ ti on  att.$attId$ = ti.$tiParentId$\ninner join $t$ t on ti.target_ri = t.$tId$ and\n(ti.target_rh is null or ti.target_rh = t.to_h)\njoin \"_MmdDictionary\" d on ti.$tiPath$ = d.id and d.name like '%/termInstances')\nselect j.term_instance_id as \"id_i\",\nj.term_instance_from as \"from_h\",\n$path(\"tiPath\")$ as \"path_i\",\n$type()$ as \"type_i\",\nj.term_instance_id as \"parent_id_i\",\ncase\nwhen j.\"displayName\"='DateOfBirth' then 'PII'\nwhen j.\"displayName\"='EmailAddress'  then 'PII'\nwhen j.\"displayName\"='FirstName'  then 'PII'\nwhen j.\"displayName\"='Mobile' then 'PII'\nwhen j.\"displayName\"='Landline' then 'PII'\nelse ''\nend as \"sensitivity\",\ncase\nwhen j.\"displayName\"='DateOfBirth' then '4 - High'\nwhen j.\"displayName\"='EmailAddress'  then '4 - High'\nwhen j.\"displayName\"='FirstName'  then '4 - High'\nwhen j.\"displayName\"='Mobile' then '4 - High'\nwhen j.\"displayName\"='Landline' then '4 - High'\nelse ''\nend as \"risk\"\nfrom connection_query j"
      },
      {
        "author": "anna.spakova",
        "timestamp": "1 month ago",
        "content": "Hi ​\n@sumisha\n,\nI understand your usecase, but this isn’t supported to my knowledge. ONE Data is a completely different module that is separated from the mmm backend database, thus it cannot see its values. I am thinking that maybe the Lists of values could be able to help you here instead of ONE Data but that has very limited functionality (no DQ monitoring etc.)\nAlso an alternative solution could be to use the orchestration server for this automation - you can export the ONE Data reference data into some DB and use this export in your workflow, that would populate the field via API. Together with some Notification handler you could potentially be able to make it “near real time” as well.\nhttps://docs.ataccama.com/runtime-server/latest/workflow-and-scheduler/notifications-handler.html\nKind regards,\nAnna"
      },
      {
        "author": "sumisha",
        "timestamp": "1 month ago",
        "content": "Thanks for getting back. Can you share some screen shot of an example of List of values mentioned in first solution?"
      },
      {
        "author": "anna.spakova",
        "timestamp": "28 days ago",
        "content": "Hi, apologies for the delay.\nThe Lists of values are covered by our documentation:\nhttps://docs.ataccama.com/one/15.1.0/metadata-model/lists-of-values.html\nIt’s basically just a space from which you can add new values to your custom entities - the main goal for those is to serve as small lookups for drop-down properties in other entities. But you can have multiple properties in those value list entities and you can also populate them via API. In general, you can use any custom entity for a drop-down, this is just a pre-built example of how it can be done.\nSince those lists are entities within the metadata model, they are reachable by the computed content.\nKind regards,\nAnna"
      },
      {
        "author": "Cansu",
        "timestamp": "15 days ago",
        "content": "Hi ​\n@sumisha\n, I’m closing this thread for now. If you have any follow up questions please feel free to share them in the comments or create a new post 🙋🏻‍♀️"
      }
    ]
  },
  {
    "title": "Create DQ rule to validate data in same column",
    "url": "https://community.ataccama.com/data-quality-catalog-94/create-dq-rule-to-validate-data-in-same-column-1517",
    "question": {
      "author": "Vaishali",
      "timestamp": "[No timestamp]",
      "content": "Hi,\nI want to create DQ rule to check data within same column. For example, from below table, I want to check if Value of Category B is >= Value of Category D.\nHow can I do this in the Ataccama One UI?\nThanks\nCategory\nValue\nA\n5\nB\n2\nC\n6\nD\n4"
    },
    "answers": [
      {
        "author": "ivan.kozlov",
        "timestamp": "5 months ago",
        "content": "Hi ​\n@Vaishali\n, dq rules in Ataccama ONE usually work on the row basis - rule condition would usually evaluate the column values in specific row of data. You can build aggregated rules that would compare specific row of data with aggregated data in the dataset but you need to have a clear idea how you would aggregate it based on what exactly you’re trying to see in the result.\nCould you please clarify what is the final goal of the rule?\nDo you need to compare the values only between specific categories or between all of them (basically identifying the maximum)?\nThank you!\nIvan"
      },
      {
        "author": "Vaishali",
        "timestamp": "5 months ago",
        "content": "Thanks ​@ivan.kozlov for your response.\nI am not looking for aggregate values. I want to compare some of the categories with each other. The comparison may be checking values less than or greater than each other. Or it can be checking if value of category B is x times value of category A."
      },
      {
        "author": "ivan.kozlov",
        "timestamp": "5 months ago",
        "content": "Hi ​\n@Vaishali\n, unfortunately, I don’t think what you’re trying to do is possible, especially using only rules created in web app UI. For the rule condition to be evaluated, all the input attributes should be within the same row of data and the data you need for your rule will never be in one place unless the catalog item structure would look completely different (values for A, B, C, D that have to be compared would be in the same row)."
      },
      {
        "author": "marella",
        "timestamp": "1 month ago",
        "content": "Going off of this, are there ways to apply rules on a column level? And is there any kind of AI generated automatic classification option when it comes to column level classifications? Thank you"
      },
      {
        "author": "Cansu",
        "timestamp": "15 days ago",
        "content": "Hi ​\n@marella\n, apologies for coming back a bit delayed here, not for the moment unfortunately. You can see the full list of ONE AI capabilities\nhere\nand if you’d like please don’t hesitate to share that as a feature request in our ideas section 🙋🏻‍♀️"
      }
    ]
  },
  {
    "title": "🔦 Community Spotlight Highlights: Glossary Customization in Ataccama ONE with Stater",
    "url": "https://community.ataccama.com/data-quality-catalog-94/community-spotlight-highlights-glossary-customization-in-ataccama-one-with-stater-1781",
    "question": {
      "author": "Cansu",
      "timestamp": "[No timestamp]",
      "content": "Hi everyone,\nWe're excited to share highlights from our very first\nCommunity Spotlight\nevent, where we heard directly from ​\n@Albert de Ruiter\n, Lead Data Management at\nStater\n, the largest mortgage service provider in the Netherlands.\nAlbert took us on a deep dive into how his team tackled\nglossary customization\nin Ataccama ONE to better reflect their business logic and metadata needs. With over 25 years of experience in data, Albert brought both expertise and practical insights to the session.\n💡\nHere are some key takeaways:\nWhy customize the glossary?\nStater needed different metadata for catalog items and their attributes—customizing allowed them to reflect these distinctions clearly.\nCustom term types:\nThey introduced\nentity terms\n,\nattribute terms\n, and\nbusiness terms\nto better align with their logical data model.\nReusability with relationships:\nUsing\nrelationship types (\"roles\")\n, they elegantly modeled attributes that appear in multiple entities—like keys, foreign keys, and descriptive attributes.\n📺\nMissed the session? No worries!\nAlbert created a guide with 5 articles you can follow through to get started with customizing your glossary:\nBusiness glossary - customization (part 1)\nBusiness glossary - adding relationship types (part 2)\nBusiness glossary - multilingualism (part 3)\nBusiness glossary – computed content (part 4)\nBusiness glossary - synonyms (part 5)\n💬 What did you think of the session? Have you tried customizing your glossary in a similar way? Share your thoughts and questions in the comments below!\nps. Stay tuned for our next\nCommunity Spotlight - coming soon\n!"
    },
    "answers": []
  },
  {
    "title": "Extracting Rules and other Metadata Information from a Monitoring Project ?",
    "url": "https://community.ataccama.com/data-quality-catalog-94/extracting-rules-and-other-metadata-information-from-a-monitoring-project-1733",
    "question": {
      "author": "Apurva Kapoor",
      "timestamp": "[No timestamp]",
      "content": "We need to extract the Rule and other Metadata from Monitoring project. Below is the set of Metadata Information that is required\nDQ Rule UID - Ataccama system generated unique identifier of Data Quality rule\nName of DQ Rule\nDefinition of DQ Rule\nThreshold\nDQ Dimension\nDQR Owner\nDQ Rule Start Date\nDQ Rule End Date\nDQ Rule Creation Date\nDQ Rule Modified Date"
    },
    "answers": [
      {
        "author": "Lisa Kovalskaia",
        "timestamp": "1 month ago",
        "content": "Hi ​\n@Apurva Kapoor\n, these you can extract quite easily via the\nONE Metadata reader step\n:\nDQ Rule UID\nName of DQ Rule\nDefinition of DQ Rule\nThreshold\nDQ Dimension\nDQR Owner\nMost of that information is stored in the properties of the\nrule\nentity, so you can start there. To get the threshold of each rule instance applied in a given project, you will need to dig into the embedded streams of\nmonitoringProjectConfiguration\n→\nprojectCatalogItemInstance\n→\ndqCheck\n→\ncheckAlertInstance.dqThreshold\nThe timestamps you're looking for are not directly available in entity properties out of the box. Have you added them into your metadata model, or do you want to export them based on rule or monitoring project version history?"
      },
      {
        "author": "Apurva Kapoor",
        "timestamp": "1 month ago",
        "content": "Can you please share the plan for the mentioned rule configuration setup to extract the metadata so that we can try to use and test it ?"
      },
      {
        "author": "Apurva Kapoor",
        "timestamp": "1 month ago",
        "content": "Is there any reference that you can share to extract the Rules Metadata from Monitoring projects ​\n@Lisa Kovalskaia\n?"
      },
      {
        "author": "Lisa Kovalskaia",
        "timestamp": "15 days ago",
        "content": "​\n@Apurva Kapoor\nhi! Apologies for the radio silence! Please see an example plan attached (built and tested on v.15.4) - you may enrich it further to collect details like attributes to which rules are applied, rule alert thresholds, etc.\nAs I mentioned, by default versions and dates are stored differently in the Ataccama DB, so rule start date/end date/modified date either could be added as custom fields in your rules, or that info could be exported with\nGraphQL\nqueries. Alternatively, if you run this metadata export regularly on a fixed schedule, you may use each individual export as a snapshot of the current state of rules, and calculate which rules are added/decommissioned/modified based on the differences between the snapshots.\nHope this helps! I'm curious to hear more about how you'll use this metadata - is that for reporting? For synchronisation with another system? Something else?"
      }
    ]
  },
  {
    "title": "'Deleted by' details via GraphQL",
    "url": "https://community.ataccama.com/data-quality-catalog-94/deleted-by-details-via-graphql-1779",
    "question": {
      "author": "Albert de Ruiter",
      "timestamp": "[No timestamp]",
      "content": "Hi all,\nSome time ago I posted a question (\n'Published by' details via GraphQL\n) about how to make visisble who published for instance a glossary term.\nI noticed that like this you can't show who may have deleted the asset.\nDoes anyone have a clue how to retrieve this details via graphQL (or in a different manner)?\nKind regards,\nAlbert"
    },
    "answers": []
  },
  {
    "title": "Rule instance details via API",
    "url": "https://community.ataccama.com/data-quality-catalog-94/rule-instance-details-via-api-1761",
    "question": {
      "author": "Hanish N",
      "timestamp": "[No timestamp]",
      "content": "Can we get the metadata of one rule instance. I have been using below query. It is giving all the dq checks. I want to pass only one DQ check ID and get the results.\nquery{\nruleInstances(versionSelector: {draftVersion: true}){\nedges {\nnode {\ngid\ndraftVersion {\nname\ndisplayName,\ntarget\n{\ngid\n}\n}\n}\n}\n}\n}\nCan anyone help me on this?"
    },
    "answers": [
      {
        "author": "Albert de Ruiter",
        "timestamp": "26 days ago",
        "content": "Hi ​\n@Hanish N\n, you can for instance add a filter or parameter like this:\nquery getDetails {\ncatalogItem(gid: “…. value of id ...\") {\nKind regards,\nAlbert"
      },
      {
        "author": "Hanish N",
        "timestamp": "21 days ago",
        "content": "for ruleInstance We must have versionSelector. It’s throwing some error if we use gid. Please find the error below."
      },
      {
        "author": "Albert de Ruiter",
        "timestamp": "19 days ago",
        "content": "Hi ​\n@Hanish N\n,\nSo a query like this, that runs well for me,\nquery getDetails {\nattributeTerm(gid: \"5877cd9a-0000-7000-0000-00000a7942c3\") {\ngid\ndraftVersion {\nname\n}\n}\n}\ndoesn't go well for a ruleInstance?\nIn that case I don't know if I can help you further. Maybe someone form Ataccama can jump in? Or another community member of course?\nKind regards,\nAlbert"
      }
    ]
  },
  {
    "title": "How to promote customized asset type/properties/relationship/valuelist to higher environment?",
    "url": "https://community.ataccama.com/data-quality-catalog-94/how-to-promote-customized-asset-type-properties-relationship-valuelist-to-higher-environment-1776",
    "question": {
      "author": "NormanThink",
      "timestamp": "[No timestamp]",
      "content": "After testing manually created asset type/properties/relation in DEV environment, how to promote these change to high environment?"
    },
    "answers": [
      {
        "author": "Albert de Ruiter",
        "timestamp": "19 days ago",
        "content": "Hi ​\n@NormanThink\n,\nIn the Maintenance Center → System changes select tab Applied changes. On each line, or grouped by date, you can find the changes that you have applied on your Dev environment. Via the 3 vertical dots on the right you can export the changes in json files (for each change one file).\nIn the higher environment also open  Maintenance Center → System changes but now select tab Pending changes. If you click Add change a windows open in which you can drag one json file at a time, starting with the first change.\nLike this you simply replay everything you did in the Dev environment. Downside of this approach is that if you have come to the end result with much of experimenting, applying corrections etc, you replay that as well. For easier changes it might be better then to just rebuild the solution in the higher environment.\nKind regards,\nAlbert"
      }
    ]
  },
  {
    "title": "Ataccama Desktop Iterate Tool Erros",
    "url": "https://community.ataccama.com/data-quality-catalog-94/ataccama-desktop-iterate-tool-erros-1670",
    "question": {
      "author": "bobparry",
      "timestamp": "[No timestamp]",
      "content": "Hello!\nDoes anyone have experience using the Iterate tool within the Ataccama Desktop, when creating a workflow? I’ve found a couple of old community posts referring to it, along with the files they’ve used, but I can’t get it to work in my environment. see the following:\nIterating rows of a table (Record-by-record - SQL Iterator) in a workflow\nBased on that post and my testing, it seems like the Iterate tool needs to call another workflow, so I’m calling another .ewf file that just has the Run DQC tool, which runs the .plan file. I’m able to succeed in running the .plan file from the Run DQC .ewf, however i can’t seem to get the Iterate tool to run, without getting Configuration errors, like the following:\n12.03.2025 15:30:51 [INFO]     Task 'Iterate' : failed with exception: com.ataccama.adt.internal.core.runtime.EwfConfigException: Configuration contains errors, exiting\njava.lang.RuntimeException: com.ataccama.adt.internal.core.runtime.EwfConfigException: Configuration contains errors, exiting\n\tat com.ataccama.adt.runtime.ProcessorWorkflowInvocationService.runWorkflow(ProcessorWorkflowInvocationService.java:55)\n\tat com.ataccama.adt.task.exec.EwfForEachTaskInstance$SerialExecutionStrategy.run(EwfForEachTaskInstance.java:143)\n\tat com.ataccama.adt.task.exec.EwfForEachTaskInstance.run(EwfForEachTaskInstance.java:84)\n\tat com.ataccama.adt.internal.core.runtime.EwfTaskProcessor$ThreadSlot.run(EwfTaskProcessor.java:372)\nCaused by: com.ataccama.adt.internal.core.runtime.EwfConfigException: Configuration contains errors, exiting\n\tat com.ataccama.adt.internal.core.runtime.EwfWorkflowTools.createWorkflowRunThread(EwfWorkflowTools.java:222)\n\tat com.ataccama.adt.runtime.ProcessorWorkflowInvocationService.runWorkflowAsync(ProcessorWorkflowInvocationService.java:83)\n\tat com.ataccama.adt.runtime.ProcessorWorkflowInvocationService.runWorkflow(ProcessorWorkflowInvocationService.java:53)\nIdeally i would be passing files through to iterate on, but I’ve been unsuccessful in getting that to work, or even work when not passing anything through the iterate, just iterating at all. I’ve tried iterating on Set and Files based on the tooltips available, but never been able to get this to work!\nIf anyone has had success with this, please let me know!"
    },
    "answers": [
      {
        "author": "Adrian Anderson",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@bobparry\n.\nIterating workflows can be a bit tricky to set up the first couple of times for sure.\nThere are some helpful example tutorials built into ONE Desktop.\nHave you had a chance to take a look at these?\nLet us know if this helps.\nKind regards,\nAdrian"
      },
      {
        "author": "bobparry",
        "timestamp": "2 months ago",
        "content": "Hi Adiran!\nThanks for this, I’d never seen that part of the Tutorials before! That clarifies where i was going wrong, my assumption was I could loop through a single .comp file many times with a range of inputs, but from how the tutorial is set up this is more like a batch file, limited to just iterating through a series of .comp files. Is that correct?\nThanks again, huge help!\nBest,"
      },
      {
        "author": "Adrian Anderson",
        "timestamp": "2 months ago",
        "content": "The workflows can be configured very flexibly, especially when parameters are used effectively.\nThese parameters can be used inside components as column values and even column names, allowing dynamic replacement inside expressions and most steps configurations. Note: this doesn’t work in plans.\nA couple of non-obvious hints:\nTo set parameters in workflow, double-click on an empty part of the workflow’s canvas or right-click and select Edit properties…\nTo use parameters in a step’s configuration in components, right-click on the name of the setting and select Map as Parameter.\nThere are other options as well but, hopefully, this helps for now.\nIf not, could you please provide more details about what you’re trying to achieve?\nKind regards,\nAdrian"
      },
      {
        "author": "bobparry",
        "timestamp": "2 months ago",
        "content": "Hi Adrian,\nThanks for this, i was able to replicate your steps, but I’m not sure how to iterate through values in the parameter.\nFor context, the overall goal is just to pass metadata (titles, descriptions, etc) from our source systems through to Ataccama One, allowing users to update their metadata at source, and see that same metadata reflected in the Data Catalogue. Our metadata is stored in XMLs, associated with every item in the catalogue. Using the methodology from this community post:\nimporting catalog item descriptions and attribute descriptions from dbT docs\n, i can join the metadata from the XML to the Data Catalogue item that already exists, and write to Ataccama One.\nHowever, i have thousands of catalog items with metadata stored this way, and didn’t want to manually create each plan, nor run each plan individually. The plan as of now, based on what you’ve said about iterating above, is to have a unique .comp file for each catalogue item, store all the comp files in the same folder, and iterate through them with the Iterate tool.\nTo make all of the .comp files, I’m just going to duplicate a template .comp file, and edit the JSON of each .comp file, replacing the name and .XML file it reads, to match each of the XMLs in a separate XML folder. I think we can do all of that using python, but that’s where I’m testing now.\nIf there is a way to iterate through the same plan, using the input .XML as a parameter, and passing the file name of each the XMLs through the parameter, and iterate the one plan for as many XMLs as there are in the XML folder, that would be easier, but based on the above instructions/what I’ve seen online I don’t know if that is possible.\nI’ve attached a zip file with the .ewf files, and a sample of 3 .comp files they refer to, and the XMLs they write to for your reference. It refers to ID’s for our workspace/is connecting to our instance of Ataccama One, but otherwise should work on your end. Once configured for an environment you can work with, just running the SDWMetadataIterator.ewf should run through each of the .comp files and update the metadata for them.\nLet me know if there’s a better or more efficient way of doing this!\nBest,"
      },
      {
        "author": "Adrian Anderson",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@bobparry\nThanks heaps for sharing the config files. I think I now understand what you're trying to achieve.\nHopefully, the answer is a bit simpler than it initially looked.\nBut first a few follow up questions/clarifications:\n1. The updates that we want to apply to the Catalog Items are in separate XML files. Is this correct?\n2. How are the XML files produced and how do they arrive into the filesystem?\n3. In the provided examples, the only difference between the three component (.comp) files is the Xml Reader step's File Name. Is this correct?\n4. In the lookup (.lkp) files, are the Code values coming from ONE? If so, it might be easier/more dynamic to use additional ONE Metadata Reader to retrieve the codes from ONE in real-time. This way the same plan can be used across multiple environments without needing to recreate any lookup files.\nTo process a folder of (XML) files, we can use a workflow to iterate over each XML input file and using\na single parameterized component\nthat accepts the XML filename as input.\nAttached (community_dq_1670.zip) is one way of doing this.\nDepending on how and where the XML files are generated, and how the lookup values are managed, this process could be simplified further.\nKind regards,\nAdrian"
      },
      {
        "author": "bobparry",
        "timestamp": "2 months ago",
        "content": "Hi Adrian,\nHuge improvements on mine, work on my end as well! Thanks so much!\nThe updates are all in XML files, however those files are just from a table in a MSSQL Database, so I think I can switch the iterator to a Sql Row iterator, and pull them from there? I’d only tried to do everything the XML’s in a folder as I thought I’d need to for the iterator to work, so that would save additional steps.\nThe XMLs are produced through a spatial data application called ArcGIS Pro, its how our organization manages and works with spatial data. They have a catalog interface that allows users to enter metadata for any table, and that metadata is then saved to an XML corresponding to the table.\nCorrect! The only change was the input file, using your technique it works. I had tried doing more or less what you had done but I was putting the parameter name in the File Name entry box in the XML Reader, rather than the .comp name, hadn’t occurred to me that would be the way this works.\nGreat idea on having the Lookups saved in ONE, and that means I only have to update the lookup in one location as we use the same lookup in a few dq rules as well.\nI won’t be able to share the connection string/credentials for the database, but if I can get that working I’ll share again with the database connection removed for others to use as a template. Any other Ataccama customer who store spatial metadata within an ArcGIS system should be able to use this!\nAs well I’ll be out of office for the next few weeks, but I’ll post an update when back. Thanks again Adrian!"
      },
      {
        "author": "Adrian Anderson",
        "timestamp": "1 month ago",
        "content": "Hi Bob.\nGlad to hear we're heading in the right direction with this.\nAs mentioned, depending on where the XML is sourced from, the entire process could be simplified further.\nIf the XML is being read from a DB, and the process executed from Desktop, then workflows aren't required at all.\nIn the component, simply replace the Xml Reader step with a Jdbc Reader step followed by an Xml Parser step (see example screenshot). The iteration now occurs in the Jdbc Reader step.\nNote: The Xml Parser step's config will be almost identical to the Xml Reader's, just needing to change the Data Stream name from 'out' to anything else, e.g., 'metadata'.\nEnjoy your break!\nKind regards,\nAdrian"
      },
      {
        "author": "bobparry",
        "timestamp": "20 days ago",
        "content": "Hi ​\n@Adrian Anderson\n,\nI’ve updated my plan to match what you have and it works great!! Thanks so much, it goes through each of the rows in SQL, finds the XML, parses it ,and then the flow works the same as it did before, without the need to set up the automation. If you ever have any customers pulling metadata from an ESRI ArcGIS environment, this would be the exact process to follow.\nFinal step for me is to schedule this process, so it automatically runs and keeps the metadata up to date. I’ve started with the advice in this help document:\nRun Plans from the Command Line :: Ataccama ONE\n, where I’ve created a .bat file which calls the runcif.bat, and passes this .plan through. Is that what you would recommend here as well, or is there a better method that I should explore?"
      }
    ]
  },
  {
    "title": "Business and Report Terms Bulk Uplaod",
    "url": "https://community.ataccama.com/data-quality-catalog-94/business-and-report-terms-bulk-uplaod-1774",
    "question": {
      "author": "Susan24us",
      "timestamp": "[No timestamp]",
      "content": "Hello ,\nplease advise on the steps required to perform a bulk upload of business or report terms to the Business Glossary page?\nAdditionally, is it possible to deploy the glossary content to a higher environment (e.g., from Dev to QA or Prod), or do we need to manually replicate the same steps in each environment"
    },
    "answers": [
      {
        "author": "Ilan.Boiangin",
        "timestamp": "20 days ago",
        "content": "Hi Susan,\nThank you for your question! This is actually something that Ataccama has an out of the box export for. if you are looking to export existing terms you can navigate to the Business glossary section and select the 3 dots next to “Create” this will bring up a feature that says “export list of items” which will allow you to export. Please be sure to select \"terms with dependencies” Once that is done you may go to Global configuration  > application settings > import and export to check you exported information but also it allows you to import the terms there as well! Note that this is mainly used for adding the exported data across the different environments such as promoting from dev to prod as mentioned in your original question. To import you newly exported terms you may go to the import and export tab and select import and add your file there. please see the attached images and please let me know if this answers your question! 🙂"
      }
    ]
  },
  {
    "title": "How to get PK viloation identification in Data Quality indicator in One Desktop ?",
    "url": "https://community.ataccama.com/data-quality-catalog-94/how-to-get-pk-viloation-identification-in-data-quality-indicator-in-one-desktop-1767",
    "question": {
      "author": "lokeshpotti",
      "timestamp": "[No timestamp]",
      "content": "in One Desktop we will get the summary of DQ checks like if the column is empty or if the column (salary) is having greater than defined. For each and every record it will show the record which got failed in the output of Data Quality indicator . My question here is How to achieve Primary key violated here ? i will show with an example."
    },
    "answers": [
      {
        "author": "Ilan.Boiangin",
        "timestamp": "21 days ago",
        "content": "Hello Lokeshpotti,\nThank you for your question. If I understand correctly the goal is to read a file and check to see if the primary key is violated by having a duplicate. This can be done in a couple of ways. One of the ways I can think of is to use a group aggregator step to check the count of the rows for each primary key and if the count is >1, then flag as a duplicate primary key. we could then bring the data back to the main flow so that we can see all of our data once again. Please see the images of how a flow like this would be created."
      },
      {
        "author": "lokeshpotti",
        "timestamp": "21 days ago",
        "content": "Thanks for quick response. i will check this and let you know."
      }
    ]
  },
  {
    "title": "Questions for Ataccama One Power BI connector",
    "url": "https://community.ataccama.com/data-quality-catalog-94/questions-for-ataccama-one-power-bi-connector-770",
    "question": {
      "author": "pdanpoonkij",
      "timestamp": "[No timestamp]",
      "content": "Hello community,\nI’m creating a Power BI source on Ataccama One and have a few queries:\nIs the built-in Power BI connection allow One to retrieve a Power BI datasets? Or this is strictly for only bringing the Power BI reports to One?\nThe imported catalog items from this connection shows “0” number of attributes and “-” number of records (screenshots below). This is also presents in the the screenshot from the Ataccama support page (\nlink\n). Is this behave as intended or am I missing anything here?\nThank you in advance.\nBen"
    },
    "answers": [
      {
        "author": "Cansu",
        "timestamp": "1 year ago",
        "content": "Hi\n@pdanpoonkij\n, I’ve checked with the team, and at the moment, it’s not possible to import the attributes for Power BI data sets. We have changed this for Tableau (so Tableau data sources actually display the attributes and we will be using them for lineage purposes). So it’s therefore expected behavior of the platform."
      },
      {
        "author": "pdanpoonkij",
        "timestamp": "1 year ago",
        "content": "Thank you for your response, Cansu 😀"
      },
      {
        "author": "hope.spenik",
        "timestamp": "2 months ago",
        "content": "Has this been updated within the last year?  Can Ataccama still not import attributes for Power BI data sets?"
      },
      {
        "author": "JohnY",
        "timestamp": "2 months ago",
        "content": "Or is it even in Ataccama’s roadmap?"
      },
      {
        "author": "Cansu",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@hope.spenik\nand ​\n@JohnY\n, I’ve checked with our team and while we are aware of the feedback this isn’t in the immediate roadmap. We’ll be sure to let you know if anything changes, in the meanwhile, could you please check\nthe idea post here\n, and upvote the idea so that we collect more upvotes regarding this suggestion 💡If this idea doesn’t cover the feedback fully, please feel free to create a new one."
      },
      {
        "author": "Monicazim",
        "timestamp": "24 days ago",
        "content": "​\n@Cansu\nthe Idea linked above is for APPS to be included versus including the underlying PowerBI dataset (semantic model) in the data catalog with its tables and attributes.  I’m searching Ideas for that request now.  Thanks!"
      },
      {
        "author": "Cansu",
        "timestamp": "22 days ago",
        "content": "Thank you for coming back to me ​\n@Monicazim\n, if there isn’t an existing suggestion please kindly create one for us on the community so that more users can upvote the request 🙌"
      },
      {
        "author": "Monicazim",
        "timestamp": "21 days ago",
        "content": "Thanks ​\n@Cansu\n- I created an IDEA for this ​\n@pdanpoonkij\nfor potential implementation.  Please vote it up :) Also if you have any workaround for now please share.  Thanks!\nImport attributes (tables and columns) for Power BI datasets | Community"
      }
    ]
  },
  {
    "title": "How to import terms from ONE Desktop 🟣🔎",
    "url": "https://community.ataccama.com/data-quality-catalog-94/how-to-import-terms-from-one-desktop-912",
    "question": {
      "author": "Cansu",
      "timestamp": "[No timestamp]",
      "content": "Hi everyone,\nThis week’s last best practice is on how to import terms from ONE Desktop 🖥️\nIn data governance, importing terms is a crucial step. Using\nONE Desktop\n, you can streamline this process by batch-importing terms into your\nONE Web Application\ninstance. This guide walks you through the process of importing terms from a CSV file into your\nONE instance\nusing the Text File Reader and ONE Metadata Writer steps. Please note that while we use the Text File Reader in this example, you can choose alternative input steps like JDBC Reader or ONE Metadata Reader.\nStep 1: Prepare Your Term File\nBegin by preparing your term file. In this example, we have a CSV file called\nparty.csv\n, which contains several terms:\nStep 2: Create a New Plan\nCreate a new plan in\nONE Desktop\n. Simply drag and drop your term file into the plan. This action will automatically generate a new Text File Reader step.\nStep 3: Edit Text File Reader Properties\nEdit the properties of the Text File Reader step:\nIn the\nColumns\nsection, add the name of the column from your data file (in this case, \"TERM\").\nSet the correct data type for your column.\nStep 4: Add a ONE Metadata Writer Step\nNow, add a new ONE Metadata Writer step and edit its properties:\nSelect your ONE Platform Server Name.\nChoose \"term\" in the entity types.\nSelect the Workflow State for the terms.\nChoose \"metadata\" as the Parent Type.\nSelect \"terms\" as the Parent Property.\nIn the Columns section, add a new Entity Column of the 'name' type and include the name of the column from your data file (e.g., TERM).\nEnter a Created GID Column Name (e.g., TERM).\nStep 5: Create Three More ONE Metadata Writer Steps\nTo assign rules to your imported terms in ONE, you'll need to add three more ONE Metadata Writer steps. These additional steps work as 'wrappers' for standardization, detection, and validation rules. Configure these steps as follows:\nFor the standardization step:\nSelect the same ONE Platform Server Name as in the previous steps.\nChoose \"termStandardizationRules\" in the entity type.\nSelect the Workflow State for the terms.\nSet \"term\" as the Parent Type.\nChoose \"standardizationRules\" as the Parent Property.\nIn the Columns section, for Standardization 'wrapper' step, Load Entity Columns and add an \"enabled\" column with its value set to\ntrue\n.\nFor the detection step:\nSelect the same ONE Platform Server Name as in the previous steps.\nChoose \"termDetectionRules\" in the entity type.\nSelect the Workflow State for the terms.\nSet \"term\" as the Parent Type.\nChoose \"detectionRules\" as the Parent Property.\nIn the Columns section, for the Detection 'wrapper' step, Load Entity Columns and add \"hasAIRuleDetection\" and \"operator\" columns with values of\ntrue\nand\n\"OR\"\n, respectively.\nFor the validation step:\nSelect the same ONE Platform Server Name as in the previous steps.\nChoose \"termValidationRules\" in the entity type.\nSelect the Workflow State for the terms.\nSet \"term\" as the Parent Type.\nChoose \"validationRules\" as the Parent Property.\nIn the Columns section, for Validation 'wrapper' step, Load Entity Columns and add an \"enabled\" column with its value set to\ntrue\n.\nIn the Columns section, for Detection 'wrapper' step, Load Entity Columns, add \"hasAIRuleDetection\" and \"operator\" columns with\ntrue\nand\n\"OR\"\n, respectively.\nEnter the name of the Parent ID Column Name (e.g., TERM).\nEnter \"termStandardizationRulesGid,\" \"termDetectionRulesGid,\" or \"termValidationRulesGid\" in the Created GID Column Name, respectively.\nStep 6: Run the Plan\nFinally, run the plan to efficiently import the terms from your data file into the selected\nONE instance\n.\nThis process allows you to maintain a structured and organized repository of terms across ONE Desktop to ONE Web App."
    },
    "answers": [
      {
        "author": "JohnY",
        "timestamp": "1 month ago",
        "content": "​\n@Cansu\nVery useful thread!\nDo you have the steps for Ataccama v14.5 as well?\nThanks!\nJohn"
      },
      {
        "author": "Cansu",
        "timestamp": "21 days ago",
        "content": "Hi ​\n@JohnY\n, steps should be similar but our documentation also has a\n14.5 version for ONE desktop including the connection.\nCould you let me know where do you see any difference?"
      }
    ]
  },
  {
    "title": "Single Post Processing Plan that is able to be used for all monitoring projects",
    "url": "https://community.ataccama.com/data-quality-catalog-94/single-post-processing-plan-that-is-able-to-be-used-for-all-monitoring-projects-1755",
    "question": {
      "author": "ThomasPro",
      "timestamp": "[No timestamp]",
      "content": "Hello!\nI am trying to think of a way through building a single post processing plan and or component that is able to be applied to every monitoring project in my ecosystem. It is a very simple plan, I will just have a filter for when invalid_rules_explanation is not null, and export to a default CSV location. The issue I am running into is that in all documentation, it is project / catalog specific when setting up a post processing plan. For a company with numerous projects and catalogs (around 500 unique configurations and more coming), I do not want to manually have to create the same post processing plan for every project and catalog. Is there a way to automate this, or have something within the UI of the web version that would allow me to do this more seamlessly?"
    },
    "answers": [
      {
        "author": "anna.spakova",
        "timestamp": "22 days ago",
        "content": "Hello ​\n@ThomasPro\n,\nthank you for your question. At this moment, there is no simple way how to automatically create multiple post-processing components. In theory, it should be possible via a combination of API calls and uploading components to Minio, but the process is quite compliated and can result in inconsisten state.\nDepepnding on your version, there is a new feature - Tranformation plans - that allows users to create simple post-processing plans via front-end application. There are some limitations, though, e.g. hybrid deployment has currently some limitations, but you could try on one example and see if it’s a viable option.\nhttps://docs.ataccama.com/one/latest/monitoring-projects/data-transformation-plans.html\nWhy I am suggesting this - from what I observed, it is using a standard graphQL API, so you should be able to get all the calls through e.g. the GraphQL inspector plugin in browser and prepare a script/plan that could automate it for all your projects. I haven’t tried myself, but might be worth a shot.\nAlso, it is one of the points our development team wants to address in the new cloud verions of Ataccama, so hopefully we will be able to resolve this issue with post-processing components going forward.\nKind regards,\nAnna"
      }
    ]
  },
  {
    "title": "Matching runtime error raised.[STEP Matching 2[Md Unify]]",
    "url": "https://community.ataccama.com/data-quality-catalog-94/matching-runtime-error-raised-step-matching-2-md-unify-1753",
    "question": {
      "author": "Raj K",
      "timestamp": "[No timestamp]",
      "content": "I am performing deduplication for which am using matching step. I have configured it using name attribute to find duplicate with jarowrinkler() but getting a runtime error.\nHere is the configuration details -\nHere the error message -\nNot getting any error warnings on any step, configure window within my plan file.\nI appreciate any assistance you can provide.\nThanks for taking the time to look into this.\nYour help is greatly appreciated."
    },
    "answers": [
      {
        "author": "Raj K",
        "timestamp": "29 days ago",
        "content": "Removing ID column from column binding solves these issues\nHope these helps,\nThanks."
      },
      {
        "author": "Cansu",
        "timestamp": "22 days ago",
        "content": "Hi ​\n@Raj K\n, thank you for coming back to share the solution, much appreciated!"
      }
    ]
  },
  {
    "title": "Can we change the catalog name in MP?",
    "url": "https://community.ataccama.com/data-quality-catalog-94/can-we-change-the-catalog-name-in-mp-1757",
    "question": {
      "author": "Hanish N",
      "timestamp": "[No timestamp]",
      "content": "Is there any way to give some logical name to the catalog item which is added in Monitoring Project. Actual catalog name and the catalog instance name present in MP should be different. Is that possible?"
    },
    "answers": [
      {
        "author": "anna.spakova",
        "timestamp": "22 days ago",
        "content": "Hello ​\n@Hanish N\n,\nthank you for your question. Unfortunately, this is not currently possible. You can potentially add a new field into the projectCatalogItemInstance entity (e.g.\nlabel\n), however this feild would be available only through API, never through the front-end as that isn’t ready for such a field.\nPlease feel free to raise this as an idea though, if we have more customers interested in this feature, we can try to prioritize this in the future development.\nKind regards,\nAnna"
      }
    ]
  },
  {
    "title": "how to get free trial license for Ataccama",
    "url": "https://community.ataccama.com/data-quality-catalog-94/how-to-get-free-trial-license-for-ataccama-1682",
    "question": {
      "author": "lokeshpotti",
      "timestamp": "[No timestamp]",
      "content": "i have installed ataccama one desktop but it is asking for license to execute anything. Can someone please help in providing free trial license."
    },
    "answers": [
      {
        "author": "Cansu",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@lokeshpotti\n, I’ve reached out to you directly via email 📧"
      },
      {
        "author": "sh.rohit.srivastava",
        "timestamp": "1 month ago",
        "content": "I'm facing the same issue you recently encountered with Ataccama Desktop—getting the error:\nDQC \"license not verified! Program will exit.\"\nCould you please let me know how you resolved this and how to get the free license version? Any help would be appreciated.\nThanks in advance!\nBest regards,\nRohit"
      },
      {
        "author": "Cansu",
        "timestamp": "1 month ago",
        "content": "Hi ​\n@sh.rohit.srivastava\nreplied to your PM 📥"
      },
      {
        "author": "Aatif",
        "timestamp": "1 month ago",
        "content": "Hi,\ni have installed ataccama one desktop but it is asking for license to execute anything. Can someone please help in providing free trial license."
      },
      {
        "author": "Cansu",
        "timestamp": "1 month ago",
        "content": "Hi ​\n@Aatif\n, I’ve raised this with your account representative they will be in touch as soon as possible 🙋🏻‍♀️"
      },
      {
        "author": "ritik",
        "timestamp": "1 month ago",
        "content": "I have downloaded ataccama 16.1.0 for Windows:\nAtaccama ONE Desktop\n, from where i can get license to download"
      },
      {
        "author": "Cansu",
        "timestamp": "22 days ago",
        "content": "Hi ​\n@ritik\n, we don’t offer free trials anymore, please reach out to your Ataccama representative to access a license. 🙋🏻‍♀️"
      }
    ]
  },
  {
    "title": "How to use Dynamic Expression Assigner?",
    "url": "https://community.ataccama.com/data-quality-catalog-94/how-to-use-dynamic-expression-assigner-1234",
    "question": {
      "author": "srija piratla",
      "timestamp": "[No timestamp]",
      "content": "Hi I'm Srija, Solutions Consultant here at Ataccama.\nIn this post, I’ll walk you through how to use a dynamic expression assigner.\nWhat is a dynamic expression assigner?\nThe Dynamic Expression Assigner in Ataccama allows you to define dynamic expressions or rules to determine how values are extracted or assigned to specific attributes based on conditions present in your data.\nUse Case:\nThe Monitoring project results will give the attributes , Invalid DQ rules and Invalid DQ Attributes for the failed rows. If we need to extract the DQ Error Attribute Value based on the Invalid DQ Attribute then Dynamic Expression Assigner can be used in the post processing plan.\nIn the below example, I have a DQ_Error_Attribute which is causing the issue for that record. If I need to extract the respective value of the DQ_Error_Attribute then, I can use the Dynamic Expression Assigner.\nWe can achieve this using a column assigner, but we must hardcode the attributes to extract the value. If any new attributes are added to the source or a column name change happens then a hardcoded expression in column assigner will throw an error. In this scenario, we can make use of the dynamic expression assigner.\nHow to use a Dynamic Expression Assigner in this scenario?\nCreate a plan and use the text file reader step and consider the Excel table above as your\nSource\n.\nAdd an\nAlter\nstep to add a new column DQ_Error_Attribute_Value.\nUse the Dynamic Expression Assigner step, In the column give the DQ_Error_Attribute_Value created above using the alter statement.\n4) In the expression, give the DQ_Error_attribute so that the Dynamic expression assigner extracts the value of the attribute and stores it in the DQ_Error_Attribute_Value.\nCreate a text file writer and store all the attributes.\nThe complete plan looks like below.\nRun the plan and see the output file. The excel file should contain a new attribute named DQ_Error_Attribute_Value.\n8. DQ_Error_Attribute value is extracted based on the DQ_Error_Attribute. In this scenario, DQ_Error_Attribute for ‘John’ is City. The actual value of the City is ‘Texas’. By using the Dynamic Expression Assigner step, The value of the city is extracted by passing the DQ_Error_Attribute in the Expression.\nIf you have any questions or thoughts please don’t hesitate to share them in the comments below!"
    },
    "answers": [
      {
        "author": "greglvaughan",
        "timestamp": "8 months ago",
        "content": "HI,\nIs there any way to parse through a list of attributes ( colA|colB|colC) and output (‘A’| ‘ ‘ | ‘’) .  I can split and recombine was just hoping I may be able to parse the list then combine the results into one column for multiattribute tests.\nI am just going to regex split → assigner → group .. no worries..\nthanks,\ngV"
      },
      {
        "author": "srija piratla",
        "timestamp": "8 months ago",
        "content": "Hi\n@greglvaughan\n,\nI believe using regex split and assigning using the dynamic expression and grouping the columns is the best solution in order to achieve this.\nRegards,\nSrija Piratla"
      },
      {
        "author": "greglvaughan",
        "timestamp": "26 days ago",
        "content": "Hi,\nThis works for string data but appears to be coming back blank for float, long , datetime data? Is there any workaround for that?\nthanks,\nGreg\nSolution.. just create data typed dynamic expression assigners (ie attribute_value_date, attribute_value_long, attribute_value) then convert to string and concatenate.\ngV"
      },
      {
        "author": "Cansu",
        "timestamp": "22 days ago",
        "content": "Thank you ​\n@greglvaughan\nfor the question..and the solution 🙌"
      }
    ]
  },
  {
    "title": "DQ Rule for missing values",
    "url": "https://community.ataccama.com/data-quality-catalog-94/dq-rule-for-missing-values-1759",
    "question": {
      "author": "akshayl09",
      "timestamp": "[No timestamp]",
      "content": "I want to find out missing values in the columns.\nUse case, column called colors which has values as [‘ORANGE’,’BLACK’,’BLUE’] and rule is if RED is not part of column values, then Rule should fail and alert that RED color is missing in table."
    },
    "answers": [
      {
        "author": "bobparry",
        "timestamp": "26 days ago",
        "content": "I would create a SQL Catalog Item based on your table, selecting a count of each colour (including values that aren’t listed), something like the following:\nSELECT\nColor,\nCOUNT\n(*)\nAS\nCount\nFROM\nColorsTable\nWHERE\nColor\nIN\n(\n'Black'\n,\n'Orange'\n,\n'Blue'\n,\n'Red'\n)\nGROUP\nBY\nColor;\nThen, build a DQ rule on the SQL Catalog Item, where if the count total is 0, record fails, if greater than 0 record passes. That way, if Red has no records, then Red fails. It would require you to know all the acceptable colours, and list them out specifically like I have in the WHERE clause.\nNot sure if there is a better way, there might be something in Data Aggregation rules that could do this maybe? Nothing I’m familiar with though."
      },
      {
        "author": "akshayl09",
        "timestamp": "26 days ago",
        "content": "Thanks ​\n@bobparry\nfor your suggestion.  I was thinking different way.\nCreate Aggregated rule like Maximum (find (‘RED’, <Attribute>), this way, you can if you find the RED anywhere in column values, it will true value. Maximum will return true. If RED doesn't available, all the values will be false. Hence it will return false.\nBut I am looking for any better approach where we can provide the list of value to find the same without creating additional catalog items.\nShould there be an option to calculate such thing by creating SQL statement under DQ rule section, instead of creating new catalog item for each scenario."
      }
    ]
  },
  {
    "title": "Exclude attributes from Reconciliaton Project",
    "url": "https://community.ataccama.com/data-quality-catalog-94/exclude-attributes-from-reconciliaton-project-1752",
    "question": {
      "author": "jdordregter",
      "timestamp": "[No timestamp]",
      "content": "Hi,\nwe are making use of reconciliation projects to compare two different dataset with eachother. Most of the data needs to be compared, but some fields we want to skip in the comparison.\nis it possible to skipp certain attributes in the comparison (we got some mutation date fields that show differences, but are no importance)\nis it possible to distinguish in notification between some attributes missing and data showing differences (Dataset A has a few extra attributes then Dataset B, but all other data should be exactly the same). I don't want daily notifications that I am missing these attributes, but I do want a notification when the data is showing differences.\nkind regards, Jur Dördregter"
    },
    "answers": [
      {
        "author": "ekaterina.ponomareva",
        "timestamp": "28 days ago",
        "content": "Hello!\nThank you for your question!\nDuring the mapping process, you can manually select which attributes from the origin and target datasets you want to map. By doing this, you can exclude attributes that you do not want to compare. Make sure you uncheck automapping option.\nIf the project is already created, please go to ‘Results’ page, get to the ‘Compare’ tab and delete/add attributes, as needed.\nI believe this would solve both of the questions you have.\nLet me know if you have any more questions.\nKind regards,\nEkaterina"
      },
      {
        "author": "jdordregter",
        "timestamp": "27 days ago",
        "content": "Thanks Ekatarina, that works! However this process also could have some improvements. I don’t want to map 65 attributes manually per table, that is quite time consuming. Removing attributes results seems to go faster, however the pop-up closes after every delete, the attribute page number is not remembered and I have to confirm every delete. So deleting 7 attributes requires me to open the pop-up 14 times and with 65 attributes, no sorting or search option I need to navigate trough the attribute pages a lot."
      },
      {
        "author": "ekaterina.ponomareva",
        "timestamp": "27 days ago",
        "content": "Hi ​\n@jdordregter\n,\nThanks a lot for your feedback. Please note, that you can also share your ideas and feedback here  -\nhttps://community.ataccama.com/ideas?\nKind regards,\nEkaterina"
      }
    ]
  },
  {
    "title": "Identifying data quality issue in monitoring project: DQ Issues Detected",
    "url": "https://community.ataccama.com/data-quality-catalog-94/identifying-data-quality-issue-in-monitoring-project-dq-issues-detected-1749",
    "question": {
      "author": "akshayl09",
      "timestamp": "[No timestamp]",
      "content": "Is there any global setting at Monitoring Project level to get DQ issue detected warning if DQ percentage is less than 95%.?\n14.5.3 version, warning seems to be given after below 60%.\nIt means that if for catalog item in Monitoring project shows overall data quality less than 95%, it should be shown amber warning. instead of green ok."
    },
    "answers": [
      {
        "author": "ekaterina.ponomareva",
        "timestamp": "28 days ago",
        "content": "Hello!\nThank you for your question!\nIn Ataccama ONE version 14.5.3, the default threshold for data quality (DQ) issue detection is set to 70%. If you want to change the threshold to 95% so that a warning is shown when the DQ percentage is less than 95%, you can adjust the alert threshold settings within the Monitoring Project.\nHere's how you can change the threshold:\nNavigate to the Monitoring Project: Go to the Configuration & Results tab within your Monitoring Project.\nSelect the Catalog Item: Open the Catalog Item for which you want to update the DQ check threshold level.\nSelect the Applied Rule for which you want to update the DQ check threshold level. Click on ‘Configuration’ tab of a Rule. Change the threshold to 95%\nIf you want to change the default threshold for all items at once, you will need to do it via GraphQL.\nLet me know if you have any more questions.\nKind regards,\nEkaterina"
      },
      {
        "author": "akshayl09",
        "timestamp": "28 days ago",
        "content": "​\n@ekaterina.ponomareva\nThanks for your reply. This is for notification, but I want to see warning sign when I directly go to monitoring project in amber if the data quality result is below 95%. I would like to set global default setting for below highlighted warning, so that we don't have set manually for each and every catalog item."
      },
      {
        "author": "ekaterina.ponomareva",
        "timestamp": "28 days ago",
        "content": "Hi ​\n@akshayl09\n,\nLet me elaborate a bit how these warnings work.\nYou would see the warning if\nat least 1 check\nis failing below the set threshold. The threshold is set on ‘applied rule’ level, which allows you to make certain checks more important than others, if needed.\nIf you want to set a different threshold, which will be the same for all checks at once, you can do it via GraphQL.\nPlease see documentation -\nhttps://docs.ataccama.com/one/latest/one-apis/one-api.html\nAnd a community post giving more details -\nHow to use graphQL queries in ONE Desktop\nTutorial\nHope it is more clear now.\nKind regards,\nEkaterina"
      },
      {
        "author": "akshayl09",
        "timestamp": "28 days ago",
        "content": "​\n@ekaterina.ponomareva\n, as threshold is set on applied rule level, do you know what is default threshold applied on all the rules? As I don't see any default configuration at rule level to set the threshold. Where can I find these settings in Ataccama One Web?\nI would like to change thresholds for all the rules with one setting instead of doing it for one by one."
      },
      {
        "author": "ekaterina.ponomareva",
        "timestamp": "28 days ago",
        "content": "Hi ​\n@akshayl09\n,\nThe default should be 70%.\nPlease see the first reply with the instruction.  I am also pasting additional screenshot pointing to a place where you would see such threshold.\nI would like to change thresholds for all the rules with one setting instead of doing it for one by one.\nYes, understood. Please see the above-shared links to understand how to use GraphQL calls to update the threshold for all rules at once.\nIf you would like to find out more about this topic, please also have a look at these posts:\n📚 Guide: How to run GraphQL requests - Part I\n📚 Guide: How to run GraphQL requests - Part II Working with MMM\nWorking with APIs via GraphQL\nBest Practice\nHope it helps now.\nKind regards,\nEkaterina"
      },
      {
        "author": "akshayl09",
        "timestamp": "28 days ago",
        "content": "Thanks ​\n@ekaterina.ponomareva\n. Do you have any exact article to change thresold settings?\nIf I can get this as metadata through Graphql, is there any way it can be done on Ataccama Web Portal?"
      },
      {
        "author": "ekaterina.ponomareva",
        "timestamp": "28 days ago",
        "content": "​\n@akshayl09\n, I do not think there is any article about this Use Case specifically. The above-shared articles and documentation pages can still be used, as an inspiration and guideline.\nIn order to update the threshold for all checks in all Monitoring Projects, you will need to first read certain metadata (DQ check and ProjectCatalogItemInstance) and then have a call with  POST Method. It can be done with Ataccama Desktop plan.\nOn Ataccama ONE Web portal you would have to do such an update 1 by 1.\nThere is also a very great and detailed tutorial about this topic -\nhttps://community.ataccama.com/master%2Ddata%2Dmanagement%2Dreference%2Ddata%2Dmanagement%2D92/using%2Dgen2%2Dgraphql%2Dapi%2Dand%2Dmetadata%2Dsteps%2D714?tid=714&fid=92\nKind regards,\nEkaterina"
      }
    ]
  },
  {
    "title": "Validate exact two digit after decimal point",
    "url": "https://community.ataccama.com/data-quality-catalog-94/validate-exact-two-digit-after-decimal-point-1714",
    "question": {
      "author": "akshayl09",
      "timestamp": "[No timestamp]",
      "content": "There is column called Sales Amount (Long Data type). It has values in decimal point. I wanted to create a DQ evaluation rule that It should have exact two places decimal point.\nI tried to create rule by converting long to string and validated length after decimal point as 2 but failed for values 1.00 or 2.10 or 0.00 as conversion to string change value from 0.00 to 0, 1.00 to 1 or 2.10 to 2.1\nIs there any way to implement this?"
    },
    "answers": [
      {
        "author": "Albert de Ruiter",
        "timestamp": "1 month ago",
        "content": "Hi ​\n@akshayl09\n,\nFirst question that pops up, are you sure about the decimals in combination with a Long data type, being an integer?\nIf it's supposed to be able to contain decimals, you could consider changing the column's data type to decimal or number (or whatever the options of your database are).\nThen let the application that displays the data define that the sales amount always shows with two digits.\nOther question, why define a DQ to check the format of data? A DQ rule would make more sense when it checks the content of the data right?\nKind regards,\nAlbert"
      },
      {
        "author": "akshayl09",
        "timestamp": "1 month ago",
        "content": "My Question is more about, when Long Data type/ Float Data type is converted to String data type, it trim zero after decimal point or before digit.\nIf figure is 1.00 (float datatype (32,2) or long type), after conversion to String format in Ataccama, it changes to 1 instead of keeping 1.00 similar other examples mentioned above and 001 is converted to 1.\nIt doesn't keep it as it."
      },
      {
        "author": "Albert de Ruiter",
        "timestamp": "1 month ago",
        "content": "If your column Sales Amount is of type Long, so an integer, it will never have decimals. Why even bother to make it show with two decimals, always being zero…??? After conversion to string it doesn't change from 1.00 to 1, because it was 1 already.\nBy the way, a Float datatype will not have a number of decimals specified, that would be the case with Number/Numeric/Decimal data types. A Float will have an undefined number of decimals.\nTo make a number with possible decimals show as you describe with always two decimals, you would have to examine the string: if no decimal sign (point or comma) exists, then add ‘.00', if length(string after decimal)=1 then add ‘0’. Something like that.\nOn this page you can find functions that you can apply:\nhttps://docs.ataccama.com/one/latest/common-actions/one-expressions.html\nI suppose that for instance you could make use of the decode, case, iif and length functions."
      },
      {
        "author": "akshayl09",
        "timestamp": "1 month ago",
        "content": "​\n@Albert de Ruiter\nLength function is not applicable for long type data type or float data type.\nI have another case to explain why it is important to retain the zero.\nConsider I have ID which should have 10-digit number, but it starts with 000, as zeros are getting trim. It doesn't validate the number correctly and in other programming language like python or java, this doesn't happen. I am not sure why zeros are getting trimmed while changing data type."
      },
      {
        "author": "Albert de Ruiter",
        "timestamp": "1 month ago",
        "content": "But you converted it to a string right? Then the length function is applicable.\nAnd again: a Long will never have decimals because it is an integer: it cannot have any functional relevance to make it show with decimals (otherwise it shouldn't have been defined as a Long).\nIf you have a 10-digit number of which the preceeding zeroes are relevant, then it must be implented as a string.\nIn my previous answer I gave you some guidance on how to proceed once a digit has been converted to a string. Unfortunately not all functions that we consider for granted, like rpad and lpad, are available in Ataccama. The functions that are available can be found via the link."
      },
      {
        "author": "akshayl09",
        "timestamp": "1 month ago",
        "content": "Thanks ​\n@Albert de Ruiter\nI understand, we are trying to resolve the issue by work-around. I think rpad or Lpad will add the zero, but it will eventually lead 100 % passing of the rule. In this case, no value will invalid as we are managing them through rpad or lpad. we will miss out invalid values through rule."
      },
      {
        "author": "Cansu",
        "timestamp": "28 days ago",
        "content": "Hi ​\n@akshayl09\n, I wanted to check whether you’ve found a solution you could also share here? If not, I’d like to suggest for you to open a support ticket for our team at support.ataccama.com 🙋🏻‍♀️"
      },
      {
        "author": "akshayl09",
        "timestamp": "28 days ago",
        "content": "Sure @Cansu. I will open a ticket for this."
      }
    ]
  },
  {
    "title": "Profiling comma separated file in webui",
    "url": "https://community.ataccama.com/data-quality-catalog-94/profiling-comma-separated-file-in-webui-1740",
    "question": {
      "author": "Delbert71",
      "timestamp": "[No timestamp]",
      "content": "Hi, This file is comma separated with header and “ text qualifiers and keeps failing as below with a long line error. The data is from an MSAccess table with specific column widths not the default 255 in length. I have tried a number of times to no avail.\nAny assistance would be most welcome.\n2025-04-10 16:27:04 [INFO]    com.ataccama.lib.securityutil.javaencryption.DefaultEncryptionEngine messageId=createDefaultKeyProvider reason=the 'properties.encryption.keystore' property not specified2025-04-10 16:27:04 [INFO]    com.ataccama.lib.securityutil.javaencryption.DefaultEncryptionEngine messageId=createDefaultKeyProvider reason=the 'internal.encryption.keystore' property not specified10.04.2025 16:27:04 [INFO]     Using following licenses:10.04.2025 16:27:04 [INFO]       /data/ataccama/one/dpe/license_axaUK_PROD-dpe-bde_v14_subscription_updated.plf10.04.2025 16:27:05 [INFO]     running job 2f1568e2-0000-7000-0000-00000026b0c8 (14-CRESTDSS POLICY AND POPULATION.txt)10.04.2025 16:27:05 [DEBUG]    metrics will be scraped to /data/ataccama/one/dpe-assembly-14.5.1.231205-13929-3b0b5777/storage/jobs/2f1568e2-0000-7000-0000-00000026b0c8/workspace/metrics10.04.2025 16:27:05 [INFO]     Ataccama DQC engine initialized.10.04.2025 16:27:05 [INFO]     Creating runtime...10.04.2025 16:27:05 [DEBUG]    Profiling(ProfilingAlgorithm): profiling.inMemoryTotal = 1,000,000 (default)10.04.2025 16:27:05 [DEBUG]    Profiling(ProfilingAlgorithm): profiling.inMemorySortPercent = 50 (default)10.04.2025 16:27:05 [DEBUG]    Profiling(ProfilingAlgorithm): profiling.inMemorySortWeight = 5 (default)10.04.2025 16:27:07 [INFO]     Starting runtime...10.04.2025 16:27:07 [INFO]     Running runtime...10.04.2025 16:27:07 [DEBUG]    Starting runnables...10.04.2025 16:27:07 [DEBUG]   [in] Reading...10.04.2025 16:27:18 [WARNING]  [EHS:LONG_LINE][step:2f1568e2-0000-7000-0000-00000011a43b][line:12531]10.04.2025 16:27:18 [FATAL]    Parse errors in file reader.[STEP Component/2f1568e2-0000-7000-0000-00000011a43b[Text File Reader]]First 1 detected problem(s) (out of 1)[EHS:LONG_LINE][step:2f1568e2-0000-7000-0000-00000011a43b][line:12531]"
    },
    "answers": [
      {
        "author": "anna.spakova",
        "timestamp": "1 month ago",
        "content": "Hi ​\n@Delbert71\n,\nis this the complete log? It looks like it is failing only on one line -\nline:12531\nAre you able to check the details of this specific record? This error usually means that there are more fields present than is the header (usually caused by extra delimiter).\nAlso, is this profiling done via Ataccama ONE? What version are you on? Can you provide a screenshot of the Import settings?\nhttps://docs.ataccama.com/one/16.0.0/sources/custom-file-import.html\nThank you.\nKind regards,\nAnna"
      },
      {
        "author": "Cansu",
        "timestamp": "28 days ago",
        "content": "Hi ​\n@Delbert71\n, did ​\n@anna.spakova\n’s solution help?"
      },
      {
        "author": "Delbert71",
        "timestamp": "28 days ago",
        "content": "Hi Anna,\nThanks so much for your reply. I was away last week on holiday so thanks for your patience. You are bang on, turns out there was a ‘\\’ on it’s own in one of the columns and this was shunting the data to the right. Not sure why it thought it was a delimiter but once I found that, the file loaded successfully.\nCheers Derek"
      }
    ]
  },
  {
    "title": "Last Friday or weekday",
    "url": "https://community.ataccama.com/data-quality-catalog-94/last-friday-or-weekday-1750",
    "question": {
      "author": "akshayl09",
      "timestamp": "[No timestamp]",
      "content": "How to calculate last Friday or current day of the week? Is there any weekday function available?"
    },
    "answers": [
      {
        "author": "AKislyakov",
        "timestamp": "28 days ago",
        "content": "Hi ​\n@akshayl09\n,\nYou can use\ntoString\nfunction to get the name/number of the current day of the week using\nSimpleDateFormat (Java Platform SE 8 )\nnotion. Naming and numbering are dependent on the locale used (e.g. 1 is Sunday in US, Monday in GB). And then create a conditional logic to calculate the last Friday.\nAlternatively, if your dates reliably lie within certain range, you can use some known Friday and calculate weeks from it. E.g. if valid dates are in this century, you can use following expression.\ndateAdd(toDate(\n'31-12-1999'\n,\n'dd-MM-yyyy'\n), (dateDiff(toDate(\n'31-12-1999'\n,\n'dd-MM-yyyy'\n), in_dt,\n'DAY'\n) div\n7\n) *\n7\n,\n'DAY'\n)"
      },
      {
        "author": "akshayl09",
        "timestamp": "28 days ago",
        "content": "Thanks ​\n@AKislyakov\n, It works."
      }
    ]
  },
  {
    "title": "Column Classification",
    "url": "https://community.ataccama.com/data-quality-catalog-94/column-classification-1754",
    "question": {
      "author": "marella",
      "timestamp": "[No timestamp]",
      "content": "Is there a way to manually or AI generate tags to classify columns based on the contents within columns of data? Also curious if there are any rule detection capabilities for column classification?"
    },
    "answers": [
      {
        "author": "ekaterina.ponomareva",
        "timestamp": "28 days ago",
        "content": "Hi ​\n@marella\n!\nThank you for your question!\nBased on the data or metadata of a column, detection rules identify catalog item attributes to which a particular business term should be applied. They are applied to terms themselves, and these terms are applied to attributes which in turn satisfy the condition of the rule. Multiple detection rules can be applied to one term. Detection rules run during profiling, to identify and classify attributes and catalog items according to the rules condition.\nPlease see more details about detection rules -\nhttps://docs.ataccama.com/one/15.1.0/data-quality/rules.html#more-about-detection-rules\nDetails how to create detection rules -\nhttps://docs.ataccama.com/one/15.1.0/data-quality/create-detection-rule.html\nHow to add/remove detection rules, enable/disable AI detection -\nhttps://docs.ataccama.com/one/15.1.0/data-quality/add-rules-to-terms.html#add-detection-rule-to-term\n.\nIf by classification, you mean applying data protection classification tags, please see this documentation page -\nhttps://docs.ataccama.com/one/latest/user-access-management/data-protection-classification.html#create-data-protection-classifications\nDoes this make sense? Please let me know if you need more information.\nKind regards,\nEkaterina"
      }
    ]
  },
  {
    "title": "How to get MP run status via API?",
    "url": "https://community.ataccama.com/data-quality-catalog-94/how-to-get-mp-run-status-via-api-1742",
    "question": {
      "author": "Hanish N",
      "timestamp": "[No timestamp]",
      "content": "I triggered MP using below query:\nmutation startMonitoring {\nrunMonitoringProject(projectId: \"89b662a3-8ece-409f-b523-01aeba8240a2\",\npartitionSettings: [\n{\ncatalogItemId: \"7c4993f7-bb8c-4885-a4e5-d3930d043f8e\",\npartitionType: LATEST\n}\n]\n) {\nloadMonitoringProjectId\n}\n}\nHow do I get the status of this MP whether it is failed, passed?\nI want to trigger another API query once this MP is finished. For this I need to get the status as response of this MP run.\nAny help on this?"
    },
    "answers": [
      {
        "author": "anna.spakova",
        "timestamp": "1 month ago",
        "content": "Hi ​\n@Hanish N\n,\nthank you for your question. There are multiple ways how to get the job status:\nthrough the dqEvalProjectJob mmm node - it return the ID of the entity (your MP, the status and other properties) - you can either use the graphQL API or you can use the ONE Metadata Reader step in Ataccama API\nthrough DPM API\nSince you say you want to trigger another job once the MP is finished, I recommend to use the Notification handler/Subscription - it will subscribe to the event of the\ndqEvalProjectJob\nand if there is any change, it will trigger a workflow that can then run another query. You can also use the Subscription API in other tools (like Python, etc.).\nhttps://docs.ataccama.com/one/15.1.0/one-apis/mmm-events-api.html\nhttps://docs.ataccama.com/runtime-server/latest/workflow-and-scheduler/notifications-handler.html\nLet me know if I can provide more information for any of those.\nKind regards,\nAnna"
      }
    ]
  },
  {
    "title": "Feature to check  change in size of a field in a schema as part of Data Observability check",
    "url": "https://community.ataccama.com/data-quality-catalog-94/feature-to-check-change-in-size-of-a-field-in-a-schema-as-part-of-data-observability-check-1746",
    "question": {
      "author": "abhiparam",
      "timestamp": "[No timestamp]",
      "content": "Data Observability currently looks for schema changes except field size change.  ‘Schema field size change detection’ should be added as a feature as we see this as Prioritized requirement from our stakeholders."
    },
    "answers": [
      {
        "author": "Albert de Ruiter",
        "timestamp": "1 month ago",
        "content": "Hi ​\n@abhiparam\n, sounds like a good suggestion. If you re-enter this as an ‘Idea’ instead of a question, the we can also upvote the idea.\nKind regards,\nAlbert"
      },
      {
        "author": "abhiparam",
        "timestamp": "1 month ago",
        "content": "Thanks Albert. Here is the link to the idea that talks about the Schema field size change detection.  Please upvote\nhttps://community.ataccama.com/ideas/data-observability-check-for-schema-is-not-capturing-field-size-change-can-this-be-added-along-with-other-checks-for-source-schema-changes-1747"
      }
    ]
  },
  {
    "title": "Data Catalog - Handling Deleted Objects in Source Systems",
    "url": "https://community.ataccama.com/data-quality-catalog-94/data-catalog-handling-deleted-objects-in-source-systems-1744",
    "question": {
      "author": "cmagnano",
      "timestamp": "[No timestamp]",
      "content": "Hi All,\nWe are periodically cataloguing our Databricks environment and have encountered a scenario where a table in the underlying system has been deleted, however the metadata still remains present in the catalog after a re-scan.\nI understand this is the expected behaviour of Ataccama’s existing functionality, however it is not an ideal scenario for end users as they be viewing metadata for an object that no longer exists. Ideally we would want to tag the catalog item to indicate it has been deleted, or soft delete the catalog item in some way and hide it from data consumers to avoid any potential confusion.\nI am interested to understand whether others have encountered this scenario before and what mechanisms (if any) they were able to implement to help with this, either with native Ataccama functionality or with a custom workaround.\nKind regards,\nCristian"
    },
    "answers": [
      {
        "author": "Albert de Ruiter",
        "timestamp": "1 month ago",
        "content": "Hi Cristian ( ​\n@cmagnano\n),\nI have not encountered (or noticed) this before, but I was wondering about the following. We have recently upgraded to version 14.5 and now have new functionality called Data Observability. With that schema changes like deleted tables can be noticed. If such a situaton occurs, then you can remove the obsolete catalog item yourself.\nOr, maybe even better, add a property to the catalog item entity in the metadata model that you can use to indicate the soft-delete, as you suggest. The you still have the old metadata available.\nKind regards,\nAlbert"
      },
      {
        "author": "cmagnano",
        "timestamp": "1 month ago",
        "content": "Thanks for the response Albert.\nI will try configuring the Data Observability module in one of our environments to detect schema changes and see if that gives us a list of tables/attributes deleted that can then be actioned."
      }
    ]
  },
  {
    "title": "Is there a way to extract user engagement insights from the platform?",
    "url": "https://community.ataccama.com/data-quality-catalog-94/is-there-a-way-to-extract-user-engagement-insights-from-the-platform-1330",
    "question": {
      "author": "Gloria",
      "timestamp": "[No timestamp]",
      "content": "I am looking to track how many users log onto Ataccama, how many new terms or rules are created. My objective is to track user engagement and activity. How can this be achieved on Ataccama?"
    },
    "answers": [
      {
        "author": "anna.spakova",
        "timestamp": "9 months ago",
        "content": "Hello\n@Gloria\n,\nthere is an Audit log that you can use for such information.\nhttps://docs.ataccama.com/one/latest/audit/audit.html\nTo make these statistics, it might be better to use the API:\nhttps://docs.ataccama.com/one/latest/one-apis/audit-api.html\nand aggregate the different logs. Ataccama logs everything and it might be overwhelming.\nThere is a comment under this article with an example of IDE plan extracting such data from the Audit log:\nHow to use graphQL queries in ONE Desktop\nTutorial\nAlso, by default the audit log is enabled only on source and Catalog items. If you want it also for other entities, you need to turn it on using a trait\naudit:auditEnabled\n(see the above documentation).\nLet me know if this helps.\nKind regards,\nAnna"
      },
      {
        "author": "may_kwok",
        "timestamp": "1 month ago",
        "content": "​\n@anna.spakova\nAm I reading this correctly that it only tracks changes made to the entities in ONE?\nHow about login history? Is there some way to extract that too?\nThanks!"
      },
      {
        "author": "anna.spakova",
        "timestamp": "1 month ago",
        "content": "Hi ​\n@may_kwok\n,\nit also tracks the Read operations, so it doesn’t require the actual editing of entities (so if user logs in and enters Catalog, it also creates audit logs),\nBut I think that we had a usecase internally when the PS was using the Keycloak logs to track the actual logins. I will try to find more details.\nKind regards,\nAnna"
      },
      {
        "author": "anna.spakova",
        "timestamp": "1 month ago",
        "content": "Hi ​\n@may_kwok\n,\nso Keycloak has GET /admin/realms/{realm}/events API call, that can give you all events for a specific client (e.g.\none-webapp-public-client\n) and type (=\nLOGIN\n)\nhttps://www.keycloak.org/docs-api/latest/rest-api/index.html\nThat is the other option our team investigated besides the Audit log.\nKind regards,\nAnna"
      }
    ]
  },
  {
    "title": "'Published by' details via GraphQL",
    "url": "https://community.ataccama.com/data-quality-catalog-94/published-by-details-via-graphql-1736",
    "question": {
      "author": "Albert de Ruiter",
      "timestamp": "[No timestamp]",
      "content": "Hi all,\nIn a JSON call-step in a Desktop component I use a query like this in order to retrieve the\nVersion\nand\nPublished on\ndetails.\n{\"query\":\"query getDetails {\nproduct(gid: \\\"#IDAta#\\\") {\ngid\ndraftVersion {\nname\n_effectiveFrom {\nid\ntimestamp\n}\n}\n}\n}\"}\nDoes anyone know how to expand the query so that also\nPublished by\nis retrieved?\nKind regards,\nAlbert"
    },
    "answers": [
      {
        "author": "SamWrigley",
        "timestamp": "1 month ago",
        "content": "Hi ​\n@Albert de Ruiter\n,\nTo see published by in that step you can add the following to the _effectiveFrom selection set and omit any details you don’t want.\nauthor {\nid\n... on Person {\nusername\nfirstName\nlastName\nemail\nuserId\n__typename\n}\n}"
      },
      {
        "author": "Albert de Ruiter",
        "timestamp": "1 month ago",
        "content": "Hi ​\n@SamWrigley\n,\nThat's great, I have theperson details included in my query now :-)\nThe ‘id’ was not interpreted well though, so I omitted that part. My query no looks like:\nquery getDetails {\nproduct(gid: \"5877cd9a-0000-7000-0000-000000a20de8\") {\ngid\ndraftVersion {\nname\n_effectiveFrom {\nid\ntimestamp\nauthor {\n... on Person {\nusername\nfirstName\nlastName\nemail\nuserId\n__typename\n}\n}\n}\n}\n}\n}\nKind regards,\nAlbert"
      }
    ]
  },
  {
    "title": "remove single quotation mark",
    "url": "https://community.ataccama.com/data-quality-catalog-94/remove-single-quotation-mark-1735",
    "question": {
      "author": "msabyasachi2005",
      "timestamp": "[No timestamp]",
      "content": "Hello everyone,\nHow can I  remove multiple characters, including the single quotation mark, for each row in the INPUT column  with a string data type in OneWeb (advance expression)  , e.g :-\nINPUT\nRETURN VALUE\n'Tom Smith' 'Laura Jones'\nTom Smith Laura Jones\nTom's\nToms\nNULL\nNULL\nThanks in advance.\nSabyasachi"
    },
    "answers": [
      {
        "author": "Adrian Anderson",
        "timestamp": "1 month ago",
        "content": "Hi Sabyasachi.\nThere are a few ways to do this in advanced expressions.\nProbably the easiest is to use the replace() function, e.g.:\nreplace( in_str, \"'\", \"\" )\nChange ‘in_str’ to whatever the input field is called.\nLet us know if this helps.\nKind regards,\nAdrian"
      },
      {
        "author": "msabyasachi2005",
        "timestamp": "1 month ago",
        "content": "Thanks Andrina . that works ."
      }
    ]
  },
  {
    "title": "Monitoring Project kept failing",
    "url": "https://community.ataccama.com/data-quality-catalog-94/monitoring-project-kept-failing-1741",
    "question": {
      "author": "Susan24us",
      "timestamp": "[No timestamp]",
      "content": "There are 98 rules implemented in the monitoring project, and it typically takes about 20 minutes for the project to run and complete successfully, which was fine. I then tried to add an export plan on the Ataccama ONE Desktop to export the results of invalid rules into Excel on the web, but it kept failing. After removing the export plan I created, I'm still encountering the same error code 137. What could be causing this issue?"
    },
    "answers": [
      {
        "author": "ivan.kozlov",
        "timestamp": "1 month ago",
        "content": "Hi ​\n@Susan24us\n,\nI’m afraid it’s hard to figure out what’s the problem in this case with just the information you shared.\nYou can try to check the “open related events” section for the failed DQ Evaluation Job in DPM Console and see if there might be some additional details.\nI’d suggest you to create a support request for this problem and my colleagues should be able to help you with investigation. Please provide the failed job reference, info about the product version you currently have, deployment type (on-prem, PaaS, Hybrid PaaS), info about datasource used in this project, and DPE logs (in case your deployment type is on-prem or this issue affects on-prem DPE), etc.\nThank you!\nIvan"
      }
    ]
  },
  {
    "title": "importing catalog item descriptions and attribute descriptions from dbT docs",
    "url": "https://community.ataccama.com/data-quality-catalog-94/importing-catalog-item-descriptions-and-attribute-descriptions-from-dbt-docs-1099",
    "question": {
      "author": "Prasad Rani",
      "timestamp": "1 year ago",
      "content": "We are using dBT for our data pipelines, and for the tables and fields, there is description maintained in dBT that we would like to export (json) and update Ataccama Data Catalog.\nIs there a known, trusted, supported way to automate this?\nI already do have a desktop plan i created that can update catalog objects and attributes based on a excel input file. I may be able to modify that if thats the only option. Iam reaching out to the community to see if there is a preferred way to get descriptions and documentation from dBT to Ataccama."
    },
    "answers": [
      {
        "author": "ivan.kozlov",
        "timestamp": "1 year ago",
        "content": "Hi Prasad,\nYou should be able to update the descriptions for catalog items and attributes using Metadata Writer step.\nFirst you would need to read the ID’s and names for all the existing CI’s and Attributes using Metadata Reader step, then you can join the metadata from the platform with another data stream from dbt which would contain the names of CI’s and attributes and relevant descriptions and then you would update existing CI’s and Attributes using Metadata Writer step.\nBelow are examples of the configuration of Metadata Reader\\Writer steps:\nAnd here’s a high level example of how the plan logic might look like:\nUpdating CI descriptions is rather straight forward task but in case of attributes you need to make sure that you reference the right parent catalogItemId for each attribute id.\nTo parse the data in json format coming from dbt you should be able to use something like Json Reader or Json Parser step in IDE.\nI hope this will be helpful.\nIvan"
      },
      {
        "author": "Cansu",
        "timestamp": "1 year ago",
        "content": "Hi\n@Prasad Rani\n, I’m closing this thread for now. If you have any follow up questions please feel free to share them here or create a new post  🙋‍♀️"
      },
      {
        "author": "bobparry",
        "timestamp": "2 months ago",
        "content": "​\n@ivan.kozlov\nthanks for your instructions above, our metadata operates with a similar structure to Prasads and I was able to follow what you have to work in my environment.\nQuestion for you though, I created a plan which reads metadata from an XML file, and successfully writes it to AtaccamaOne, through the logic you provided of joining based on the catalogueItemId and locationId. However, the plan only reads 1 XML file, and writes to 1 catalogue item. I have 1000+ XMLs related to 1000+ catalogue items. Is there a way to iterate through this workflow, for every XML (or DBT file). They all have the exact same formatting, so the plan i have created will work for any XML, i just cant find a way to provide multiple XMLs at input."
      },
      {
        "author": "ivan.kozlov",
        "timestamp": "1 month ago",
        "content": "Hi ​\n@bobparry\n,\nI apologize for delay with response on this topic.\nI guess there are 2 main ways how to read multiple files at once or iterate over the files.\n1) To read multiple files in the same directory you can try to set input file location to /PATH/TO/FILES/*.xml, in that case the reader step will read all the files that match that mask (all xml’s in directory) and then the plan will process the inputs from all the files. I should mention that this will work if all the files have the same structure.\n2) You can wrap the plan into a workflow (or rather 2 levels of workflows).\nTop level workflow will trigger the iteration over the files in defined directory using Iterate Task and then trigger the lower level workflow which should contain the Run DQC task and will trigger the actual component that will process the data. In this case you’ll need to pass the input file name\\location as parameter from the top level workflow, to bottom level workflow, to component. This approach is certainly more complex but adds more options in terms of flow control.\nYou should be able to find Workflow Tutorials project in your ONE Desktop application where you can see examples of Iterator in action.\nI hope this helps.\nIvan"
      }
    ]
  },
  {
    "title": "removing leading and trailing whitespaces in records",
    "url": "https://community.ataccama.com/data-quality-catalog-94/removing-leading-and-trailing-whitespaces-in-records-1737",
    "question": {
      "author": "DjB",
      "timestamp": "[No timestamp]",
      "content": "Hello Team,\nI am trying to create a rule for a column “account_number” in a dev table. We are using version 14.5 and “One Desktop” to create the rule.\nThe requirement is to create a rule which removes the leading and trailing whitespaces from the records of the column. I tried defining below\nexpressions in the Column Assigner step\nof the plan. The final step of the plan is the JDBC Writer step.\ntrim(account_number)\nsqueezeSpaces(account_number)\ntrimRight(account_number)\nOnce I make the changes, I save it, run the plan and the plan executes successfully.\nBUT\nwhen I check the records in the database…\nthe\nwhitespaces are still there in the database table records.\nCan you please help me understand how to fix this? Which expression should help remove the whitespaces from the records?\nThanks,\nDhruba"
    },
    "answers": [
      {
        "author": "AKislyakov",
        "timestamp": "1 month ago",
        "content": "Hi ​\n@DjB\n,\nCan you please share what database engine do you use and what are the types of the fields you store the data."
      },
      {
        "author": "DjB",
        "timestamp": "1 month ago",
        "content": "Hi ​\n@AKislyakov\n,\nThe data type of the column is CHAR(15) in the database and using in SQL server.\nIn Ataccama, the data type of the column is String.\nI even tried using the expression as\ntrim(toString(account_number))\nbut was of no use.\nThanks"
      },
      {
        "author": "AKislyakov",
        "timestamp": "1 month ago",
        "content": "Behavior of\nCHAR\ndatatype in MS SQL is dependent on the\nANSI_PADDING\nsetting. (\nSET ANSI_PADDING (Transact-SQL) - SQL Server | Microsoft Learn\n)\nBy default, the setting is set to ON, and as a result all values are forcefully padded with blanks when saved to the table\nPad original value (with trailing blanks for\nchar\ncolumns and with trailing zeros for\nbinary\ncolumns) to the length of the column.\nIf you need to store data without trailing blanks, I suggest you to opt for the\nVARCHAR\ndatatypes, which supports storage of variable-length strings."
      },
      {
        "author": "DjB",
        "timestamp": "1 month ago",
        "content": "Thank you ​\n@AKislyakov\n, that helps! I will check by changing the datatype of the column in the DDL.\nBut just wanted to know if there is any Ataccama function which can be used in the DQ rule to convert the char to varchar.\nSomething like CAST or CONVERT? I am checking this option so that I don’t have to keep changing the DDL in case we face similar issues with multiple columns, in future.\nThanks,"
      },
      {
        "author": "AKislyakov",
        "timestamp": "1 month ago",
        "content": "How are you creating tables? Is it using create table button in One Desktop? or manually via DDLs?"
      },
      {
        "author": "DjB",
        "timestamp": "1 month ago",
        "content": "We are mostly creating tables manually using the DDLs.\nBut we also change the datatype manually in One desktop if its a change in a single column."
      },
      {
        "author": "AKislyakov",
        "timestamp": "1 month ago",
        "content": "Both CHAR and VARCHAR are mapped to STRING datatype in Ataccama. But Once tables are created the only way to change datatype is through DDL (ALTER TABLE) command."
      }
    ]
  },
  {
    "title": "Where can i see the ataccama one version from the webapp",
    "url": "https://community.ataccama.com/data-quality-catalog-94/where-can-i-see-the-ataccama-one-version-from-the-webapp-1739",
    "question": {
      "author": "maneelloyds",
      "timestamp": "[No timestamp]",
      "content": "Where can i see the ataccama one version from the webapp/one ui?\nI cant see a about page? Please guide"
    },
    "answers": [
      {
        "author": "Ilan.Boiangin",
        "timestamp": "1 month ago",
        "content": "Hello ​\n@maneelloyds\n,\nTo determine your ataccama one version via ONE Webapp please select your icon on the bottom left and select “Version information”. once completed you will see a new page where it will bring up your one web application version as seen in the images below.\nHope this helps!\nThank you,\nIlan B"
      }
    ]
  },
  {
    "title": "How to use subset of records while executing and processing DQ results instead of the whole set of records ?",
    "url": "https://community.ataccama.com/data-quality-catalog-94/how-to-use-subset-of-records-while-executing-and-processing-dq-results-instead-of-the-whole-set-of-records-1732",
    "question": {
      "author": "Apurva Kapoor",
      "timestamp": "[No timestamp]",
      "content": "We want to evaluate Data Quality Score of Rule after applying the filter condition so that DQ Score gets calculated based on subset of records which are applicable for a rule rather on the whole set.\nFor example, we want to execute the DQ checks only on the records satisfying the filter condition such as ‘Product_Type’ = ‘DS’\nWe have taken the approach of SQL Catalog item to filter the records. Is there any other workaround for this problem ?\nWith the SQL Catalog option, we will have to create multiple SQL Catalog items to implement the different filter condition if needed for each rule. For example, if there are 2 rules, we need to use 2 SQL Catalog items and similarly would need to use 10 SQL Catalog items for 10 such rules.\nTherefore, we would like to understand if there is any workaround or alternate to deal with this situation ?\nPlease suggest."
    },
    "answers": [
      {
        "author": "ivan.kozlov",
        "timestamp": "1 month ago",
        "content": "Hello ​\n@Apurva Kapoor\n,\nI believe Data Slicing feature might the helpful in this case. It should allow you to run the DQ Evaluation  in MP’s on top of a subset of records instead of the whole dataset.\nHere’s a link to document:\nhttps://docs.ataccama.com/one/latest/catalog-items/create-data-slice.html\nThis functionality should be available starting from version 15.3.0.\nI hope this helps,\nIvan"
      }
    ]
  },
  {
    "title": "How to organize assets in ONE web app: adding columns and filters in Business Glossary",
    "url": "https://community.ataccama.com/data-quality-catalog-94/how-to-organize-assets-in-one-web-app-adding-columns-and-filters-in-business-glossary-556",
    "question": {
      "author": "Lisa Kovalskaia",
      "timestamp": "[No timestamp]",
      "content": "Adding more details to a list view and providing additional filtering options can help business users in their daily tasks as you scale Data Governance across multiple teams, geographies, or business functions.\nIn this article we’ll customize the Business Glossary to show\nadditional term attributes as columns\nin the list view and to enable\nfiltering\nby those attributes. This approach generally works across the Ataccama ONE Web app, so you can use this article as a guide to customize other pages, for example Rules, Catalog Items, or Monitoring Projects.\nYou don’t have to be a developer to follow this guide. If you come across any concepts you’re not familiar with, have a look at our\ndocumentation\n.\nAdding columns\nFirst, navigate to the Business Glossary’s Terms list. In the top right, find the 3-dot menu which features the option to\nEdit page\n.\nIn the side bar that opens up, go to the\nLayout\ntab. You’ll see the JSON that describes how the objects are shown on this page when the web app loads it.\nLocate\n“columns”\nand see how the columns on the Terms page correspond to the respective definitions in the JSON, e.g.\n“name”\n,\n“type”\n,\n“abbreviation”\nand\n“dqEvaltermAggr”\n.\n💡\nThe page layout references entity and property names defined by the\nMetadata Model\n. If you reference some property name that doesn’t actually exist, the app won’t be able to fetch anything to display in that column.You can lookup the\nMetadata Model\nfor the entity you’re interested in and see what its properties are called.\nNow, we want the\nowner\n, the\nsteward\nand the\nbusiness domain\nof each term to appear in the list view. At the same time, we’re not actively using\nabbreviations\n, so we might as well hide this column.\nHere’s what the changes are going to look like:\nHere we’ve added the parameter for the abbreviation column to disable it, and added the definitions for the 3 new columns we want to place in the list view.\nWe’ve also changed the value of\n“splitIndex”\nfrom 5 to 2. The “blank space” which serves as a visual separator within the grid will appear in the position of the 2nd column, after Name.\nHere’s what the result looks like once we Save and publish.\n💡\nYou’ll notice Business Domain isn’t a standard property that\nTerms\nhave out of the box. As an\nApplication Admin\n, you can add\ncustom properties\nto the Metadata Model - here’s how we’ve added Business Domain as a\ndropdown/picklist property type\n.\n💡\nWhen adding referenced properties to the screen layout of a list view, you may want to make them not only visible, but also sortable. In this case a different approach should be used. To make property columns sortable, the column definition should provide the details about the property of the target entity that's referenced from the current property. For example, for the owner property of a term, which references the username property of a person, this is what the column definition will look like:\n\"username\"\n: {\n              \"\nname\n\":\n\"Owner\"\n,\n              \"\ndataPath\n\":\n[\n\"owner\"\n,\n\"username\"\n]\n,\n              \"\nrenderer\n\":\n{\n                \"\n_type\n\":\n\"entity.scalar.tableCell\"\n}\n,\n              \"\nextraFetchRules\n\":\n[\n                {\n                  \"\npattern\n\":\n\"./owner/*\"\n}\n              ]\n},\nAdding Filters\nTo make it even easier to find items in the Glossary, let’s make an additional\nfiltering\noption from the property Business Domain.\nGo to\nGlobal Settings\n>>\nSearch Configurations\n>>\nterm\n. Notice that there’s already a number of nodes and their properties being indexed to power search in the app.\nFor\nbusinessDomain\n, the setup will be similar to what you’ll see for\nowner\n/\nsteward\nif you explore the search configuration. Just like\nowner\nand\nsteward\n,\nbusinessDomain\nis a referenced property that targets another standalone entity defined in the Metadata Model. Taking that into account, here’s how we’ll go about setting up search and filtering.\nFirst, we’ll add\nbusinessDomain\nas a new\nIndexed Node\nwith its\nname\nas the\nIndexed Property\n:\nThen, we’ll open the\nIndexed Node\nterm\nand add an\nIndexed Property\nbusinessDomain\n.\nFinally, we’ll\nAdd a Property Search filter\ncalled Domain, set as the property\nname\nof\nbusinessDomain\n(since we’ll want to be able to filter using the value of the business domain name).\nMake sure to also\nAdd Indexed Property Ref.\nIn the\nTarget field\n, once again look for the\nname\nproperty related to Search Configuration > term > businessDomain.\nOnce we’ve published the changes in the Search Configuration, the Terms listing will show a new filter:\nDid you follow along? If you have any questions or thoughts please share them in the comments below.\nP.S.If you liked this tutorial, keep an eye out for more articles on customizing the Ataccama ONE web app!"
    },
    "answers": [
      {
        "author": "Rianna",
        "timestamp": "1 year ago",
        "content": "Hello 😀 I have been trying to follow this article to implement the following in “Rules”:\nCreate a filter for dqDimension (dqDimension is already shown as a column in the table)\nCreate a new column and filter for dataType (I am referring to rule implementation > Rule input attributes > data type applied to the rule input attribute(s).) I would like the data types for all input attributes to be shown (a view similar to how multiple business terms linked to a catalog item show in 1 column).\nI have managed to get a Dimension filter showing, but it is greyed out and cannot click on it. And I have managed to get a data type column showing but it doesn’t show any data. (So not great progress!)\nI would like some help with this please where I can share the changes I have made and I would like some advise on how to fix it. Is it best to raise this as a JSM service ticket or as a new community article please?"
      },
      {
        "author": "Lisa Kovalskaia",
        "timestamp": "1 year ago",
        "content": "Hi\n@Rianna\n! Glad to hear you're making use of the customization options - I think this article is a good place to discuss adjacent use cases!\nLet's have a look at your questions. I'll assume you're on v.14.x.\nIf your Dimension filter is greyed out then I suspect something isn't quite right with the configuration of properties on which the filter should rely. You'll want to make sure that your Search Configuration for\nrule\nlooks as follows:\nindexed node rule has an indexed property dimension (should be there by default)\nyou've added one more indexed node,\ndqDimension ,\nwith the indexed property\nname\nwhen creating a filter, you've used the property\nname\nof the\ndqDimension\nnode (which is where we have the values you'll filter by - validity, accuracy, etc.)\nfinally you've added property\ndimension\nof the\nrule\nnode via Add indexed property ref under Referenced From (dimension name is displayed for a rule through a reference to the dimension node in the metadata).\nDisplaying datatypes of input attributes is trickier - I don't think it can be configured with the standard metadata model, because datatypes are too far removed in the model from the rule ( rule → ruleInputGroup → ruleInputAttribute → dataType). If you compare that to terms, termInstances are directly embedded into a rule as a property, which you can use to filter rules. If you're working with an Ataccama consultant on your implementation, this would be a good topic to raise to them.\nHope this helps!"
      },
      {
        "author": "Rianna",
        "timestamp": "1 year ago",
        "content": "Hello Lisa, thank you for your speedy detailed reply!\nThank you for confirming about the datatypes, I thought that would be a tricky one! We will pause on this until we can get some consultant assistance to help us.\nMany thanks for the steps for the dqDimension. I have followed these steps but unfortunately I still have the same issue ☹️\nI have repeated it a few times in case I made an error but it remains greyed out. I have attached a screenshot of each step I performed for reference.\nIs there anything incorrect you can spot in my attachment please? This one has me puzzled! 😵\nThank you very much"
      },
      {
        "author": "Rianna",
        "timestamp": "1 year ago",
        "content": "Sorry I forgot to also confirm I am using DEV environment version 14.2. Thanks!"
      },
      {
        "author": "Lisa Kovalskaia",
        "timestamp": "1 year ago",
        "content": "@Rianna\nthanks for the screenshots! Everything seems just right - let's just double-check a couple of things:\ndid you publish the changes in the search configuration before checking the rules list?\ncould you please try reindexing and recovering search, then checking the rules page again in a fresh tab?\nUsing DEV shouldn't have any impact at all. If the issue persists, could you please open a support ticket? Our team will investigate further. Thanks!"
      },
      {
        "author": "sumisha",
        "timestamp": "2 months ago",
        "content": "Hey ​\n@Lisa Kovalskaia\nis it possible to set a default value of filter?"
      },
      {
        "author": "Lisa Kovalskaia",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@sumisha\n, while not in the main list view, you can do that in a custom list view.\nFirst, you add a new view to the navigation panel:\nhttps://docs.ataccama.com/one/15.4.0/ui-customization/the-left-navigation-menu.html\nThen, in your new view, open up the 3-dots menu and add a filter using an AQL expression, for example:\nThat's it, your custom view will display a prefiltered list of assets that match the expected entity type and condition(s).\nI think you've already used AQL quite a bit, but referencing the doc for other readers :)\nhttps://docs.ataccama.com/one/15.4.0/common-actions/aql-expressions.html"
      },
      {
        "author": "sumisha",
        "timestamp": "2 months ago",
        "content": "Thanks for you reply ​\n@Lisa Kovalskaia\n. I am looking to set a default value in one of the filters Catalog Item tab. Can anything be set in below page?"
      },
      {
        "author": "Lisa Kovalskaia",
        "timestamp": "1 month ago",
        "content": "​\n@sumisha\nhi -- just noticed I overlooked your latest comment here, sorry! I answered that question under another post, so linking that for other readers’ benefit:\nS\nSetting of default value in catalog item in custom filter"
      }
    ]
  },
  {
    "title": "Policy Managment Logging Activities",
    "url": "https://community.ataccama.com/data-quality-catalog-94/policy-managment-logging-activities-1712",
    "question": {
      "author": "Catherine",
      "timestamp": "[No timestamp]",
      "content": "Audit logging should include all user activities within Policy Management."
    },
    "answers": [
      {
        "author": "anna.spakova",
        "timestamp": "1 month ago",
        "content": "Hello ​\n@Catherine\n,\ndid you try to enable the audit log for the policies? There is a trait called\naudit:auditEnabled\nthat should enable auditing any actions on that specific metadata node. More about traits:\nhttps://docs.ataccama.com/one/latest/metadata-model/traits.html\nDid this help? Did you try it?\nKind regards,\nAnna"
      }
    ]
  },
  {
    "title": "Catalog Items Associated with a Policy",
    "url": "https://community.ataccama.com/data-quality-catalog-94/catalog-items-associated-with-a-policy-1731",
    "question": {
      "author": "karine.davtyan",
      "timestamp": "[No timestamp]",
      "content": "Hello,\nIs there any way I can pull a list of catalog items that have been associated with a policy via graphQL given the policy gid? If not, is there another way I can access such a list ? The Policy is a data usage license agreement that is an array or references on ONE Desktop’s metadata reader.\nThanks,\nKarine"
    },
    "answers": [
      {
        "author": "anna.spakova",
        "timestamp": "1 month ago",
        "content": "Hello ​\n@karine.davtyan\n,\nI believe you can use a query like this:\nquery GetCatalogItems {\n  catalogItems(versionSelector: {draftVersion: true}, filter:\n\"policies.any($id='<your id>'\"\n) {\n    edges {\n      node {\n      gid\n      draftVersion{\n          name\n      \t}\n      }\n    }\n  }\n}\nThe “policies” should be the name of that Array of referenced object property in your metadata model. You can test your query first in the Catalog under All → AQL Search:\nYou can find more about the AQL filters for object properties here:\nhttps://docs.ataccama.com/one/latest/common-actions/aql-expressions.html#array-property-conditions\nLet me know if this helps.\nKind regards,\nAnna"
      }
    ]
  },
  {
    "title": "Case statement in computed content",
    "url": "https://community.ataccama.com/data-quality-catalog-94/case-statement-in-computed-content-1259",
    "question": {
      "author": "Albert de Ruiter",
      "timestamp": "[No timestamp]",
      "content": "Hi community,\nWe have a usecase to set a property's value automatically: based on several conditions that property gets a value. First we considered creating a desktop plan for this. Then we figured it would be preferrable to have this value set automatically, something that can possibly be accomplished by applying computed content.\nWe have used computed content before, mainly for concatenating values (like described in\nComputed Content: Show attribute values based on existing attributes (ataccama.com)\n). But in this case we would need to define a case statement to define the conditions and determine the value.\nSo the question is, does someone know if sql-like case stamenent can be implemented as computed content? And if so, can an example be provided?\nThanks for your attention and kind regards,\nAlbert"
    },
    "answers": [
      {
        "author": "abir",
        "timestamp": "10 months ago",
        "content": "Hello\n@Albert de Ruiter\n,\nYes, sql-like case stamenent can be implemented as computed content. See below a simple example of a computed content property for Catalog Item entity, serving a purpose of defining a status whether it is reviewed by Data Steward or not based on the description property.\nCatalog Item when description is null, custom property shows “PENDING”.\nCatalog Item when description is not null, custom property shows “READY”.\nConfiguration of Catalog Item entity in Metadata Model\nConfiguration of the computed content entity:\nHere is the SQL query:\nselect\na.id_i       ,\n        a.parent_id_i,\n        a.from_h     ,\n        a.path_i     ,\n        a.type_i     ,\ncase\nwhen\ncatalogitem_description\nis\nnull\nthen\n'PENDING'\nelse\n'READY'\nend\nas\nstatus,\n        concat(\n'https://mdm-server-one-fyg1u.worker-01-euc1.prod.ataccama.link/runWorkflow?id=WF1:export_all_instance.ewf&inputId='\n, a.id_i)\nas\nurl\nfrom\n(\nselect\nci.$ci_id$\nas\nid_i       ,\n                        ci.$ci_id$\nas\nparent_id_i,\n                        ci.$ci_from$\nas\nfrom_h     ,\n                        $path(ci.$ci_path$)$\nas\npath_i     ,\n                        $type()$\nas\ntype_i     ,\n\t\t\t\t\t\tci.$ci_name$\nas\ncatalogitem_name,\n\t\t\t\t\t\tci.$ci_def$\nas\ncatalogitem_description,\n                        ci.$ci_id$\nas\ncatalogitem_gid\nfrom\n$ci$ ci\n        )\nas\na\nHope this helps! If you have any questions, let me know in the comments below!"
      },
      {
        "author": "Albert de Ruiter",
        "timestamp": "10 months ago",
        "content": "Hi\n@abir\n,\nThanks for your example. I'll give it a try to make it fit our usecase. Likely I will have a follow-up question about joining tables, which works different as compared to regular SQL, because in our usecase the case statement will use logic involving several entities. But I will give it a shot myself first.\nKind regards,\nAlbert"
      },
      {
        "author": "abir",
        "timestamp": "10 months ago",
        "content": "Hi\n@Albert de Ruiter\nYou’re welcome! And joining multiple entities is also doable. Here is another example where entities are joined together.\nselect\nid_i, parent_id_i, from_h, path_i, type_i, \nconcat(\n'https://ataccama.atlassian.net/jira/secure/CreateIssueDetails!init.jspa?pid=10000&issuetype=10005&labels=AccessRequest&summary=Access%20Request%20for%20'\n,\ncatalogitem_name,\n'%20from%20'\n,\nconnection_name,\n'&customfield_10033='\n,\nschema_name,\n'&customfield_10034='\n,\ncatalogitem_name,\n'&customfield_10035='\n,\nowner,\n'&customfield_10037=https%3A%2F%2Fone.local.ataccama.pro%2Fcatalog%2Fdata%2FcatalogItem%2F'\n,\ncatalogitem_gid,\n'%2F&customfield_10038='\n,\nconnection_name,\n'&description=Hi%2C%0A%0ACould%20I%20please%20get%20access%20to%20'\n,\ncatalogitem_name,\n'%20from%20'\n,\nconnection_name,\n'%3F'\n)\nas\nurl\nfrom\n(\nselect\nci.$ci_id$\nas\nid_i,\n    ci.$ci_id$\nas\nparent_id_i,\n    ci.$ci_from$\nas\nfrom_h,\n    $path(ci.$ci_path$)$\nas\npath_i,\n    $type()$\nas\ntype_i,\n    l.$l_name$\nas\nschema_name,\n    c.$c_name$\nas\nconnection_name,\n    p.$p_username$\nas\nowner,\n    ci.$ci_name$\nas\ncatalogitem_name,\n    ci.$ci_id$\nas\ncatalogitem_gid\nfrom\n$ci$ ci\nleft\njoin\n$l$ l\non\nl.$l_id$ = ci.$ci_pid$\nleft\njoin\n$c$ c\non\nc.$c_id$ = ci.connection_ri\nleft\njoin\n$p$ p\non\np.$p_id$ = ci.owner_ri)\nas\na"
      },
      {
        "author": "Albert de Ruiter",
        "timestamp": "10 months ago",
        "content": "Hi\n@abir\n,\nI wanted to apply your input, but I ran into the following. One of the entities that I need in the computed content is an extension of term. When trying to add this it cannot be selected (not visible in the dropdownbox). I can see term itself, but none of the extensions of term that I created. Is that expected behaviour (version 13.9)? Otherwise I'll contact support.\nKind regards,\nAlbert"
      },
      {
        "author": "Albert de Ruiter",
        "timestamp": "10 months ago",
        "content": "I have contacted Support about the last question. I will update this post when I have more clarity."
      },
      {
        "author": "sumisha",
        "timestamp": "1 month ago",
        "content": "HI ​\n@abir\nI am trying to implement similar solution. Below are my configurations:  When I create referenced object in catalog item and reference computedContent, it gives error that it cannot be referenced. Do I create arbitrary referend object?"
      },
      {
        "author": "Albert de Ruiter",
        "timestamp": "1 month ago",
        "content": "Hi ​\n@sumisha\n,\nIndeed you cannot reference to the computed content directly. It should work by following these two steps:\nIn the create an ‘embedded object’ property, where the object is your computed content entity.\nCreate a ‘delegated scalar property’, with ‘Via property’ being the name of the computed content entity (in your case ‘computedContent’) and the ‘Delegate property’ being the name of the property in ‘computedContent’ that you want to apply (so in your case ‘layer').\nIf you are interested you can refer to an article that I wrote about this topic:\nBusiness glossary – computed content (part 4)\n.\nKind regards,\nAlbert"
      },
      {
        "author": "sumisha",
        "timestamp": "1 month ago",
        "content": "Thanks Albert for your reply."
      },
      {
        "author": "Albert de Ruiter",
        "timestamp": "1 month ago",
        "content": "Hi Community,\nI forgot to update you regarding the follow-up from Ataccama Support, that investigated why the computed content didn't work for my example.\nIn short, the issue is that one of the entities that I need in the computed content is an extension of term. When trying to add this entity in the computed content screen it cannot be selected (it is not visible in the dropdownbox).\nThe feedback I received was that it is considered a bug, being fixed in version 15.4.\nKind regards,\nAlbert"
      },
      {
        "author": "Cansu",
        "timestamp": "1 month ago",
        "content": "Thank you for the update ​\n@Albert de Ruiter\n😊"
      },
      {
        "author": "sumisha",
        "timestamp": "1 month ago",
        "content": "Hello, I need further support. I need to join attribute and termInstance but getting error when I bring in attribute of display name from termInstance.\nselect\na.id_i       ,\na.parent_id_i,\na.from_h     ,\na.path_i     ,\na.type_i     ,\ncase\nwhen a.lop like 'DateOfBirth%'   then 'PII'\nelse ''\nend as sensitivity\nfrom\n(\nselect\nci.$ci_id$           as id_i       ,\nci.$ci_id$           as parent_id_i,\nci.$ci_from$         as from_h     ,\n$path(ti.$ci_path$)$ as path_i     ,\n$type()$             as type_i     ,\nti.$ti_dn$           as lop\nfrom\n$ci$ ci\ninner join $ti$ ti\non ci.$ci_id$ = ti.$ti_pid$\n) as a"
      },
      {
        "author": "sumisha",
        "timestamp": "1 month ago",
        "content": "Also trying different approach but giving different error , support here would be appreciated:"
      }
    ]
  },
  {
    "title": "Data Observability -Explore options to automate Catalog items/Attribute addition/Deletion using out of box workflow (Metadata change Sync) under Recommendations tab in issues section",
    "url": "https://community.ataccama.com/data-quality-catalog-94/data-observability-explore-options-to-automate-catalog-items-attribute-addition-deletion-using-out-of-box-workflow-metadata-change-sync-under-recommendations-tab-in-issues-section-1730",
    "question": {
      "author": "Rama",
      "timestamp": "[No timestamp]",
      "content": "In data observability, after configuring a source and setting up a schema for monitoring, the Schema Changes tab displays all detected issues. For each issue, recommendations such as \"Update in Catalog Items\" or \"Import into Catalog\" are provided.\nIs there a way to automate the process of updating newly added or missing catalog items in the Knowledge catalog section and need to view the Profiling results for the newly added attributes (When Change = ‘New Attribute’ in data observability)."
    },
    "answers": []
  },
  {
    "title": "How to get the DQ rule dimension name for each rule via API.",
    "url": "https://community.ataccama.com/data-quality-catalog-94/how-to-get-the-dq-rule-dimension-name-for-each-rule-via-api-1695",
    "question": {
      "author": "Hanish N",
      "timestamp": "[No timestamp]",
      "content": "I am able to fetch all the rule details like name, label, id, stewardship etc. But I am unable to get the dimension of the rule. I don’t find any entity relation between rule and DQ dimension.\nCould anyone please help me on this?"
    },
    "answers": [
      {
        "author": "Adrian Anderson",
        "timestamp": "1 month ago",
        "content": "Hi ​\n@Hanish N\n.\nDoes the following help?\nquery getRulesWithDqDimension {\n  rules(\n    versionSelector: {publishedVersion:\ntrue\n}\n  ) {\n    edges {\n      node {\n        gid\n        publishedVersion {\n          name\n          description\n          implementation {\n            storedVersion {\n              dqDimension {\n                gid\n                storedVersion {\n                  _displayName\n                }\ntype\n}\n            }\n          }\n        }\n      }\n    }\n  }\n}\nIf you’re looking for something else, please provide some more details.\nKind regards,\nAdrian"
      },
      {
        "author": "Hanish N",
        "timestamp": "1 month ago",
        "content": "Thanks ​\n@Adrian Anderson\nfor your response. It definitely helps me."
      }
    ]
  },
  {
    "title": "Is it possible to display embedded object array in a grid within the entity view?",
    "url": "https://community.ataccama.com/data-quality-catalog-94/is-it-possible-to-display-embedded-object-array-in-a-grid-within-the-entity-view-1713",
    "question": {
      "author": "may_kwok",
      "timestamp": "[No timestamp]",
      "content": "Hi folks,\nWe’re on 14.5.3 right now, and we have a use case to display an embedded object in a grid view on a glossary term.\nTake my example here:\nglossaryTechnicalInformation is an embedded object array under my term entity. Inside glossaryTechnicalInformation are 4 simple string scalar properties. But yes I forgot to give it “name” property, my mistake.\nWhen I add some info in there, I get a guid. I know I can get it to display the “name” if I give it a property “name”, but then, I would only get the “name” to display in the page.\nIs there some way to make it display like this? (excuse my poor skills in Paint!)"
    },
    "answers": [
      {
        "author": "may_kwok",
        "timestamp": "1 month ago",
        "content": "I think ​\n@Albert de Ruiter\nachieved something like this with\ncomputed content\n, is there some way we can avoid the computed content part now in v15/v16?"
      },
      {
        "author": "Albert de Ruiter",
        "timestamp": "1 month ago",
        "content": "Hi ​\n@may_kwok\n,\nIndeed I used computed content to combine properties from a different entity, but then it shows nothing more than a concatenated string, not as a grid or table. A use case is for instance to display a translation together with its language code. Do you want more details of this approach?\nJust to be sure, you are aware of the entity trait core:displayName? With this trait you can make you property Source act as name, by applying\n{\n“propertyName”: “Source”\n}\nRegarding your remark about computed content in V15/16, I think I will create an ‘Idea’ that with AI computed content code can be created, so in a similar fashion that AI now (in V16 I believe) supports the creation of DQ rules coding.\nKind regards,\nAlbert"
      },
      {
        "author": "may_kwok",
        "timestamp": "1 month ago",
        "content": "I was hoping in v15/v16 there would be ability to display embedded array in a grid in the entity, and there wouldn’t be a need to do computed content at all.\nIn my use case concatenating 4 different fields would just be a bit too much I think..."
      },
      {
        "author": "may_kwok",
        "timestamp": "1 month ago",
        "content": "Thanks ​\n@Albert de Ruiter\n! I’ve made a feature request, feel free to upvote!\nhttps://community.ataccama.com/ideas/ability-to-display-embedded-object-array-or-reference-object-array-in-a-grid-within-entity-view-1716"
      }
    ]
  },
  {
    "title": "\"Add Attribute\" button in catalog item",
    "url": "https://community.ataccama.com/data-quality-catalog-94/add-attribute-button-in-catalog-item-1676",
    "question": {
      "author": "akshayl09",
      "timestamp": "[No timestamp]",
      "content": "Is there any way to create a calculated attribute on existing data using “Add Attribute”. As there is no option to add existing attribute in the table while creating attribute using “Add Attribute”. Is there any example how to use it? I have created one it contains only blank values."
    },
    "answers": [
      {
        "author": "ekaterina.ponomareva",
        "timestamp": "1 month ago",
        "content": "Hello!\nThank you for your question.\nCatalog items generally store metadata and static attributes related to the data source. They may not support dynamic calculations directly within the attribute fields.\nHave you considered using\nSQL Catalog Item\nfor your Use Case?\nKind regards,\nEkaterina"
      },
      {
        "author": "akshayl09",
        "timestamp": "1 month ago",
        "content": "Thanks ​\n@ekaterina.ponomareva\n. Yes, but just thought to have user friendly UI interface easily calculated column can be added or change data type easily."
      },
      {
        "author": "ekaterina.ponomareva",
        "timestamp": "1 month ago",
        "content": "Hi ​\n@akshayl09\n,\nPlease note that if data type is changed in the source system, then after reprofiling it will also be changed in Catalog Item.\nKind regards,\nEkaterina"
      },
      {
        "author": "akshayl09",
        "timestamp": "1 month ago",
        "content": "Yes, I understand that.\nConsider There are different stages of data development raw Layer & Reporting Lyer and there is “ID” column in long Format and same column is presented in Reporting layer in String format as expected data type to avoid any trimming of zero.\nNow I have to create two rules for the same column as it has different data types in different layer of analysis. So, “add attribute” would have help here to create new column by changing data type and tested with same rule instead of creating new rule.\nI hope it explained."
      }
    ]
  },
  {
    "title": "Alternative Solutions for Group Access Restrictions in Tableau via Ataccama",
    "url": "https://community.ataccama.com/data-quality-catalog-94/alternative-solutions-for-group-access-restrictions-in-tableau-via-ataccama-1701",
    "question": {
      "author": "karine.davtyan",
      "timestamp": "[No timestamp]",
      "content": "Hello everyone,\nI'm currently facing an issue with accessing Tableau content through Ataccama. Although I've successfully connected to various Tableau Server sites, most of the content remains inaccessible due to group access restrictions. The only workaround I've seen so far is granting the connecting account a site administrator explorer role on Tableau, but I'd prefer to avoid this elevated access if possible.\nHas anyone encountered this problem and found a solution that doesn't involve using a site administrator explorer role?\nAny insights, workarounds, or experiences would be greatly appreciated.\nThank you,\nKarine"
    },
    "answers": [
      {
        "author": "Cansu",
        "timestamp": "1 month ago",
        "content": "Hi ​\n@karine.davtyan\n, could you please raise a support ticket on this at support.ataccama.com, and my colleagues can help you troubleshoot as soon as possible 🙋🏻‍♀️"
      }
    ]
  },
  {
    "title": "Enabling Email Notifications for Tasks",
    "url": "https://community.ataccama.com/data-quality-catalog-94/enabling-email-notifications-for-tasks-1677",
    "question": {
      "author": "apejko",
      "timestamp": "[No timestamp]",
      "content": "Hello everyone,\nI was looking through the email templates and was wondering if it would be possible to create one for the tasks dashboard? I know its possible to enable these for monitoring projects, but would like to know if there's any way to also enable them for updates to tasks, ie task assigned, task status changed, task deleted etc. Searching through the documentation I haven't seen anything relating to this,\nAny help would be very much appreciated!"
    },
    "answers": [
      {
        "author": "stephanie.miller",
        "timestamp": "1 month ago",
        "content": "Hi ​\n@apejko\n,\nI recently solved this for a customer so I can share with you one approach you could use to get task information via email.\nFirst thing to say is that notifications for tasks can’t be configured in the web application in the same way that it can for Monitoring projects.  The idea with tasks is that users work collaboratively on the taks in the kanban board.\nHowever, there are graphQL endpoints for tasks so it is possible to read all the tasks information.   In my recent solution the tasks data was then transformed in a desktop plan and an email constructed that was sent out to a group email address summarizing open tasks, changed tasks and closed tasks.  With further development it might also be possible to parameterise the email and send custom messages to individuals.  This extra development seems closer to your original request and may be something you want to develop.\nAs this is a desktop plan it would need to be deployed to the server and executed via a workflow and it can then also be schedule to run at set intervals.   If you are interested in this solution I have attached the plan for you.\nHope this helps.\nStephanie"
      },
      {
        "author": "Cansu",
        "timestamp": "1 month ago",
        "content": "Hi ​\n@apejko\n, I’m closing this thread for now, if you have any follow up questions please feel free to share them in the comments or create a new post 🙋🏻‍♀️"
      }
    ]
  },
  {
    "title": "Implementing Automatic Data Masking on Export Using Data Protection Classification in Ataccama",
    "url": "https://community.ataccama.com/data-quality-catalog-94/implementing-automatic-data-masking-on-export-using-data-protection-classification-in-ataccama-1678",
    "question": {
      "author": "karine.davtyan",
      "timestamp": "[No timestamp]",
      "content": "Hello,\nI am looking for guidance on ensuring automatic data masking/hiding during data exports to csv/excel files based on Data Protection Classification in Ataccama ONE.\nFrom the documentation, it appears that access to sensitive attributes (ones tagged with terms that have data classification tags applied to them) can be restricted for unauthorized users, but I would like to confirm:\n1.  How can we configure permissions to ensure that unauthorized users can still export data but only receive masked/hidden values for classified attributes? What permissions do we give them?\n2. Is it possible to automatically apply masking transformations based on classification tags, so that even authorized users exporting data only receive masked values instead of raw sensitive data?\n3. Are there any recommended workflows for enforcing automatic masking at the export level while leveraging Data Protection Classification?\nThank you,\nKarine"
    },
    "answers": [
      {
        "author": "anna.spakova",
        "timestamp": "1 month ago",
        "content": "Hello ​\n@karine.davtyan\n,\nthank you for your questions.\nThe export should be possible for anyone with the View Data permissions or higher on the catalog item you want to export:\nhttps://docs.ataccama.com/one/latest/sources/data-export.html\n. The application takes care of the data masking so if the user cannot see some hidden attributes in the web application, the masking will be automatically applied to the export as well. Please note that while the access of the users to the catalog item can be View Data, the access to the tag must be View Metadata Access as a maximum (anything higher would basically disable the tag for the group/user), see.\nhttps://docs.ataccama.com/one/latest/user-access-management/data-protection-classification.html\nAFAIK this is not possible to do, users can only uncheck the attributes during the export and will not receive them at all. I can only advise to create a feature request. A workaround is (for those users that can export the data) to use the ONE Desktop tool to hide the data\nafter\nthe export.\nThe data hiding during export is automatic so no workflow is needed at the moment.\nPlease let me know if this answers your questions or if further clarification is needed.\nKind regards,\nAnna"
      }
    ]
  },
  {
    "title": "Ataccama and workday integration",
    "url": "https://community.ataccama.com/data-quality-catalog-94/ataccama-and-workday-integration-816",
    "question": {
      "author": "mahesh Ar",
      "timestamp": "[No timestamp]",
      "content": "By using one\nurl\nwe imported multiple  tables structure from workday into Ataccama.\nif we need to query any specific imported table in ataccama  is ataccam will dynamically updates the API endpoints ?"
    },
    "answers": [
      {
        "author": "Cansu",
        "timestamp": "1 year ago",
        "content": "Hi\n@mahesh Ar\n, apologies for getting back to you just now. I’ve reached out to the team and understand that you have an ongoing discussion on CDATA driver atm.\nIf I understand you correctly, you’d like to use REST API endpoints to access Workday via VCI. If this is the case, then the API endpoints won’t get updated automatically for every table that you’d like to profile. VCI has to be manually created.\nI hope this helps!"
      },
      {
        "author": "Varun Singhal",
        "timestamp": "4 months ago",
        "content": "Hi ​\n@Cansu\n, I am looking to understand about ‘Ataccama-Workday’ integration. Can you share any documentation on the same?\nThanks,\nVarun"
      },
      {
        "author": "Cansu",
        "timestamp": "4 months ago",
        "content": "Hi ​\n@Varun Singhal\n, welcome to the community and thank you for posting. I’m checking this with the team, I’ll be back with some resources as soon as possible 🙂"
      },
      {
        "author": "michal misiejuk",
        "timestamp": "1 month ago",
        "content": "Hi, ​\n@Cansu\nany new information about Ataccama - Workday integration?"
      },
      {
        "author": "Cansu",
        "timestamp": "1 month ago",
        "content": "Hi ​\n@michal misiejuk\nI’ve checked this with our team and shared it with your Ataccama representative. They will be in touch to discuss it further 🙋🏻‍♀️"
      }
    ]
  },
  {
    "title": "🖇 Ataccama Default Connections - Part 1️⃣",
    "url": "https://community.ataccama.com/data-quality-catalog-94/ataccama-default-connections-part-1-1708",
    "question": {
      "author": "RuslanK",
      "timestamp": "[No timestamp]",
      "content": "Hi everyone,\nWe often get questions on integrations and in this post you can find several technologies that Ataccama can connect to, the prerequisites for integration, key rules to follow, and fundamental troubleshooting steps.\n👉 Understanding these aspects ensures a smoother deployment and operation, minimizing potential integration issues. By following the guidelines provided, you can efficiently prepare your environment, meet technical requirements, and troubleshoot common challenges, ensuring seamless connectivity with Ataccama solutions.\nAtaccama comes with a set of default connections that work out of the box without requiring additional configuration. These connections include the most commonly used databases, cloud storage solutions, and other data sources, all pre-configured with the necessary drivers.\nFor the majority of default connections, no manual setup is required — Ataccama automatically includes and manages the appropriate JDBC or API drivers, ensuring seamless integration.\nFor most use cases, default connections provide a plug-and-play experience, allowing users to quickly establish connectivity without additional effort.\nAtaccama out-of-the-box sources 📦\nONE Data\n🔗\nDatasources: Relational Databases - JDBC Connections\nCreate in\nCatalogue Items\n→\nSources\nMS SQL Server\n👇\nHide content\nShow content\nAtaccama supports connecting to\nMicrosoft SQL Server\n, whether it is a standalone on-premises instance or a cloud-based service like\nAzure SQL Database\nor\nAzure Synapse Analytics.\nFor datasource connection, network access must be configured to allow connections via\nTCP/IP\n, typically on port\n1433\n. Cloud-hosted instances may require additional firewall rules, private endpoints, or VPN access depending on security policies.\nPort to open:\n1433\nDriver:\nprovided\nmssql-jdbc-*.jar\nwith\nmssqlLibs\nout-of-the-box within\ndrivers.zip\npackage\nJDBC String pattern:\njdbc:sqlserver://<hostname>:<port>;databaseName=<database>;encrypt=true;trustServerCertificate=true\nAuthentication options:\nUsername and password\n- A\nusername\nand\npassword\nare required to establish a connection to the JDBC datasource. Credentials are provided in the Ataccama UI and\nstored securely\n. A\nread-only\nuser is sufficient, as Ataccama never writes back to the client’s datasource.\nIntegrated credentials\n-\nAD Authentication via Kerberos\n- Confirm with your Ataccama representative for more details on the connection setup.\n📚\nRead more on our documentation\nAzure Synapse Analytics\n👇\nHide content\nShow content\nAzure Synapse Analytics (formerly SQL Data Warehouse) uses\nTCP port 1433\nfor JDBC connections. It requires\naccess to the Synapse workspace endpoint\n,\nauthentication via Azure AD, SQL authentication, or Managed Identity\n, and\nproper firewall/VNet rules\nto allow access from approved IPs or Azure services.\nPort to open:\n1433\nDriver:\nprovided\nmssql-jdbc-*.jar\nwith\nmssqlLibs\nout-of-the-box within\ndrivers.zip\npackage\nJDBC String pattern:\njdbc:sqlserver://<hostname>:<port>;databaseName=<database>;encrypt=true;trustServerCertificate=true\nAuthentication options:\nUsername and password\n- A\nusername\nand\npassword\nare required to establish a connection to the JDBC datasource. Credentials are provided in the Ataccama UI and\nstored securely\n. A\nread-only\nuser is sufficient, as Ataccama never writes back to the client’s datasource.\n📚\nRead more on our documentation\nOracle\n👇\nHide content\nShow content\nFor on-premises deployments, connections typically use\nTCP/IP\nwith the\nOracle Listener\nservice, often running on\nport 1521\n. Cloud-based instances may require configuring\nsecurity lists, private endpoints, or VPN access\nto allow external connections.\nPort to open:\n1521\nDriver:\nprovided\nojdbc8-*.jar\nwith\nORACLElIBS\nout-of-the-box within\ndrivers.zip\npackage\nJDBC String pattern:\njdbc:oracle:thin:@<hostname>:<port>:<sid>\nAuthentication options:\nUsername and password\n- A\nusername\nand\npassword\nare required to establish a connection to the JDBC datasource. Credentials are provided in the Ataccama UI and\nstored securely\n. A\nread-only\nuser is sufficient, as Ataccama never writes back to the client’s datasource.\n📚\nRead more on our documentation\nPostgreSQL (\nAmazon Aurora PostgreSQL, Amazon RDS PostgreSQL, Azure Service for PostgreSQL, Google Service for PostgreSQL)\n👇\nHide content\nShow content\nAtaccama supports connections to\nPostgreSQL\n, including\non-premises instances, managed cloud services (such as AWS RDS, Azure Database for PostgreSQL, and Google Cloud SQL), and self-hosted deployments\n.\nFor standalone deployments, a connection may require adjustments to the\npg_hba.conf\nand\npostgresql.conf\nfiles to allow remote access.\nCloud-based services may require\nwhitelisting IP addresses, configuring VPC settings, or enabling SSL connections\ndepending on security policies.\nPostgreSQL typically listens on\nport 5432\n.\nPort to open:\n5432\nDriver:\nprovided\npostgresql-*.jar\nout-of-the-box within\ndrivers.zip\npackage\nJDBC String pattern:\njdbc:postgresql://<hostname>:<port>/<database>\nAuthentication options:\nUsername and password\n- A\nusername\nand\npassword\nare required to establish a connection to the JDBC datasource. Credentials are provided in the Ataccama UI and\nstored securely\n. A\nread-only\nuser is sufficient, as Ataccama never writes back to the client’s datasource.\n📚\nRead more on our documentation\nMariaDB\n👇\nHide content\nShow content\nMariaDB uses the\nMySQL protocol\nand typically listens on\nport 3306\n.\nFor\nself-hosted or on-premises deployments\n, remote access may require modifying the\nbind-address\nsetting in the MariaDB configuration and updating\nuser privileges\nto allow external connections.\nFor\ncloud-based services\n, additional configurations such as\nIP whitelisting, VPC settings, or SSL enforcement\nmay be required, depending on the security policies of the hosting provider.\nPort to open:\n3306\nDriver:\nprovided\nmariadb-java-client-*.jar\nout-of-the-box within\ndrivers.zip\npackage\nJDBC String pattern:\njdbc:mariadb://<hostname>:<port>/<database>\nAuthentication options:\nUsername and password\n- A\nusername\nand\npassword\nare required to establish a connection to the JDBC datasource. Credentials are provided in the Ataccama UI and\nstored securely\n. A\nread-only\nuser is sufficient, as Ataccama never writes back to the client’s datasource.\n📚\nRead more on our documentation\nTeradata\n👇\nHide content\nShow content\nAtaccama supports connections to\nTeradata\n, which is primarily used as an\nenterprise data warehouse (EDW)\nand can be deployed\non-premises\nor as a\ncloud-based service\nvia\nTeradata Vantage\n(available on AWS, Azure, and Google Cloud).\nTeradata uses the\nTeradata SQL Engine\nand typically communicates via\nJDBC\n. The default port for connections is\n1025\nfor\nJDBC\naccess.\nTeradata uses the\nTeradata SQL Engine\nand supports\nJDBC connections\n, which typically communicate over\nport 1025\n.\nFor\non-premises Teradata\n, network access must be configured to allow external JDBC connections. For\nTeradata Vantage (Cloud)\n, additional configurations such as\nSSL encryption, IAM roles, and IP whitelisting\nmay be required.\nPort to open:\n1025\nDriver:\nprovided\nterajdbc-*.jar\nout-of-the-box within\ndrivers.zip\npackage\nJDBC String pattern:\njdbc:teradata://<hostname>/database=<database>,charset=UTF8\nAuthentication options:\nUsername and password\n- A\nusername\nand\npassword\nare required to establish a connection to the JDBC datasource. Credentials are provided in the Ataccama UI and\nstored securely\n. A\nread-only\nuser is sufficient, as Ataccama never writes back to the client’s datasource.\n📚\nRead more on our documentation\nAmazon Redshift\n👇\nHide content\nShow content\nAmazon Redshift uses TCP port\n5439\nby default and supports connections over JDBC. It requires access to the cluster's\nVPC security group\nand\nsubnet\nsettings, ensuring inbound rules allow connections from permitted IPs or VPCs. Redshift can be accessed via\npublic or private endpoints\n, and for cross-region or external access,\nAWS PrivateLink\nor\nVPC peering\nmay be required.\nPort:\n5439\nDriver:\nprovided\nredshift-jdbc*.jar\nout-of-the-box within\ndrivers.zip\npackage\nJDBC String pattern:\njdbc:redshift://<hostname>:<port>/<database>\nAuthentication options:\nUsername and password\n- A\nusername\nand\npassword\nare required to establish a connection to the JDBC datasource. Credentials are provided in the Ataccama UI and\nstored securely\n. A\nread-only\nuser is sufficient, as Ataccama never writes back to the client’s datasource.\n📚\nRead more on our documentation\nSnowflake\n👇\nHide content\nShow content\nSnowflake uses\nTCP port 443\nfor secure connections over HTTPS and supports JDBC/ODBC drivers. It requires\nnetwork policies\nto allow access from permitted IPs or\nPrivateLink\nfor VPC integration. Depending on deployment (AWS, Azure, GCP), additional networking settings like\negress rules\nor\nfirewall configurations\nmay be needed.\nPort:\n443\nDriver:\nprovided\nsnowflake-jdbc-*.jar\nout-of-the-box within\ndrivers.zip\npackage\nJDBC String pattern:\njdbc:snowflake://<account_identifier>.snowflakecomputing.com/?db=<database>&schema=<schema>&warehouse=<warehouse>&role=<role>\nPushdown processing:\nTo process\nData Quality\nevaluation entirely using a\nwarehouse cluster\n. When query pushdown processing is completed, only\nData Quality\nresults and invalid data samples are returned to ONE platform.\nAuthentication options:\nUsername and password\n- Is basically a\nSnowflake JWT (Private Key)\nmethod,\na\nusername\nand\npassword\nin combination with\nSnowflake JWT authentication\nis supported\nConfiguration details\nOfficial Ataccama documentation:\nhttps://docs.ataccama.com/one/latest/sources/how-to-connect-to-snowflake-using-jwt-authentication.html\nTechnical details:\n# Generate private key - correct way:\nopenssl genrsa\n2048\n| openssl pkcs8 -topk8 -inform PEM -v1 PBE-SHA1-\n3\nDES -\nout\nrsa_private_key.p8\n# Generate public key from the private key:\nopenssl rsa -\nin\nrsa_private_key.p8 -pubout -\nout\npublickey.crt\n# Set public key to Snowflake account:\nalter user\n\"<username>\"\nset\nrsa_public_key=\n'<public key content>'\nOAuth credentials\n- For\nSnowflake OAuth\nauthentication, you need to provide\nClient ID, Client Secret, Token Endpoint,\nand a\nRefresh Token\n(generated in the Ataccama UI during setup). All credentials are stored securely upon saving.\nOAuth user SSO credentials\n- For\nSSO authentication\n, the authentication method needs to be configured to enable seamless access.\n📚\nRead more on our documentation\nBigQuery\n👇\nHide content\nShow content\nBigQuery uses\nHTTPS (port 443)\nfor JDBC connections. It requires\nIAM authentication\nand access to the\nGoogle Cloud project\nwhere the dataset is stored. For authentication, OAuth2 or service account keys are typically required.\nPort:\n443\nDriver:\nprovided\nGoogleBigQueryJDBC42.jar\nwith\nbigQuery:ibs\nout-of-the-box within\ndrivers.zip\npackage\nJDBC String pattern:\njdbc:bigquery://https://www.googleapis.com/bigquery/v2:443;ProjectId=<project_id>;Dataset=<dataset>;\nAuthentication options:\nOAuth Google user credentials\n-\nClient ID\nand\nClient Secret\nneed to be provided and are stored securely upon saving.\nGoogle service account key credentials\n- the\nservice account key\nfile needs to be provided and is stored securely upon saving.\n📚\nRead more on our documentation\nSybase\n👇\nHide content\nShow content\nSybase Adaptive Server Enterprise (ASE) typically uses\nTCP port 5000\n(default) for connections and supports JDBC. It requires access to the\nSybase server hostname/IP\n,\ndatabase name\n, and\nauthentication details\n. Secure access can be enforced using\nSSL/TLS\nand\nfirewall rules\n.\nPort:\n5000\nDriver:\nprovided\njtds-*.jar\nout-of-the-box within\ndrivers.zip\npackage\nJDBC String pattern:\njdbc:jtds:sybase://<hostname>:<port>;DatabaseName =<database>\nAuthentication options:\nUsername and password\n- A\nusername\nand\npassword\nare required to establish a connection to the JDBC datasource. Credentials are provided in the Ataccama UI and\nstored securely\n. A\nread-only\nuser is sufficient, as Ataccama never writes back to the client’s datasource.\n📚\nRead more on our documentation\nDerby\n👇\nHide content\nShow content\nApache Derby is an\nembedded or client-server\ndatabase that uses\nTCP port 1527\nby default for network connections. It supports\nJDBC connections\nin both\nembedded\nand\nserver modes\n.\nPort:\n1527\nDriver:\nprovided\nderbyclient-*.jar\nwith\nderbyLibs\nout-of-the-box within\ndrivers.zip\npackage\nJDBC String pattern:\njdbc:derby:<database_path>;create=true\nAuthentication options:\nUsername and password\n- A\nusername\nand\npassword\nare required to establish a connection to the JDBC datasource. Credentials are provided in the Ataccama UI and\nstored securely\n. A\nread-only\nuser is sufficient, as Ataccama never writes back to the client’s datasource.\nHope you find these useful! Let us know if you have any questions in the comments below."
    },
    "answers": []
  },
  {
    "title": "Ranking numeric values in code query",
    "url": "https://community.ataccama.com/data-quality-catalog-94/ranking-numeric-values-in-code-query-1627",
    "question": {
      "author": "Lazov",
      "timestamp": "[No timestamp]",
      "content": "Dear members,\nI am struggling with a step that rather reflects query/array logic than the logic of steps available in component editor.\nTask is to find the maximum from a range of values. I tried to retrieve it through a dynamic query written in a Representative Creator step, see details below:\nL1:= round((1-(levenshtein(W_CUA1,W_CUA1_MATCH)/(max(length(W_CUA1),length(W_CUA1_MATCH)))))*100,0);\nL2:= round((1-(levenshtein(W_CUA1,W_CUA2_MATCH)/(max(length(W_CUA1),length(W_CUA2_MATCH)))))*100,0);\nL3:= round((1-(levenshtein(W_CUA1,W_CUA3_MATCH)/(max(length(W_CUA1),length(W_CUA3_MATCH)))))*100,0);\nL4:= round((1-(levenshtein(W_CUA1,W_CUA4_MATCH)/(max(length(W_CUA1),length(W_CUA4_MATCH)))))*100,0);\nL5:= round((1-(levenshtein(W_CUA2,W_CUA1_MATCH)/(max(length(W_CUA2),length(W_CUA1_MATCH)))))*100,0);\nL6:= round((1-(levenshtein(W_CUA2,W_CUA2_MATCH)/(max(length(W_CUA2),length(W_CUA2_MATCH)))))*100,0);\nL7:= round((1-(levenshtein(W_CUA2,W_CUA3_MATCH)/(max(length(W_CUA2),length(W_CUA3_MATCH)))))*100,0);\nL8:= round((1-(levenshtein(W_CUA2,W_CUA4_MATCH)/(max(length(W_CUA2),length(W_CUA4_MATCH)))))*100,0);\nL9:= round((1-(levenshtein(W_CUA3,W_CUA1_MATCH)/(max(length(W_CUA3),length(W_CUA1_MATCH)))))*100,0);\nL10:= round((1-(levenshtein(W_CUA3,W_CUA2_MATCH)/(max(length(W_CUA3),length(W_CUA2_MATCH)))))*100,0);\nL11:= round((1-(levenshtein(W_CUA3,W_CUA3_MATCH)/(max(length(W_CUA3),length(W_CUA3_MATCH)))))*100,0);\nL12:= round((1-(levenshtein(W_CUA3,W_CUA4_MATCH)/(max(length(W_CUA3),length(W_CUA4_MATCH)))))*100,0);\nL13:= round((1-(levenshtein(W_CUA4,W_CUA1_MATCH)/(max(length(W_CUA4),length(W_CUA1_MATCH)))))*100,0);\nL14:= round((1-(levenshtein(W_CUA4,W_CUA2_MATCH)/(max(length(W_CUA4),length(W_CUA2_MATCH)))))*100,0);\nL15:= round((1-(levenshtein(W_CUA4,W_CUA3_MATCH)/(max(length(W_CUA4),length(W_CUA3_MATCH)))))*100,0);\nL16:= round((1-(levenshtein(W_CUA4,W_CUA4_MATCH)/(max(length(W_CUA4),length(W_CUA4_MATCH)))))*100,0);\nmax(L1,L2,L3,L4,L5,L6,L7,L8,L9,L10,L11,L12) // and here comes the trouble: Ataccama has no formula to pick maximum from an N number range but from 2 sources at a time.\nIndependently from above logic, the case can be simplified as below:\nL1:= levenshtein(W_CUA1,W_CUA1_MATCH)   //everything else is just custom calculation, we can restrict the case to basics\nL2:= levenshtein(W_CUA1,W_CUA2_MATCH)\nL3:= levenshtein(W_CUA1,W_CUA3_MATCH)\netc.. to L16.\nIt is already annoying that there is no loop function in code writer function, but having no chance to set a simple rank or choose a maximum NOT from a certain column but across different variables is something I cannot believe.\nCould anyone help me out with a solution?\nWhat I want finally is:\nto get the maximum match\nto get the name of two fields that provide the highest match, such as ‘W_CUA3,W_CUA1_MATCH’.\nThank you in advance!"
    },
    "answers": [
      {
        "author": "Adrian Anderson",
        "timestamp": "3 months ago",
        "content": "Hi ​\n@Lazov\n.\nThanks for the question!\nWith Ataccama plans/components it’s important to understand that the majority of steps are\nmulti-threaded\n. In this case there is no guarantee of the order that records will traverse through the plan/component.\nThere are some steps that force the flow to wait until records are available. This includes the Representative Creator step that you mentioned.\nI’m not 100% clear on the requirement here but am hoping that the attached example plan might help (see community_dq_1627.zip).\nFeel free to explain the requirement further or ask any follow up questions if this isn’t what you were trying to achieve.\nKind regards,\nAdrian"
      },
      {
        "author": "Lazov",
        "timestamp": "3 months ago",
        "content": "Hi Adrian,\nthank you indeed for your kind answer!\nYou are on the right track, despite my explanation bleeding from many wounds. :-)\nIt is a little more complex, as I am playing hide-and-seek with a value that can be stored in four fields: W_CUA1, W_CUA2, W_CUA3, W_CUA4.\nSo each W_CUA* must be matched to the other W_CUA* in each pair of records.\nThe whole picture is as follows:\n1. I join a dataset back  to itself, based on GROUPING_ID.\nGROUPING_ID covers an n number of records, belonging together by identical value in a certain field - it is indifferent what field it is, my point is that I have already done the grouping successfully.\nSo now, in the same row, I have the in_left.W_CUA1... to in_left.W_CUA4 and in_right.W_CUA1... to in_right.W_CUA16. For simplification's sake I apply name in_right.W_CUA1 -> W_CUA1_MATCH etc. to W_CUA4_MATCH.\n2. I remove the pairs where in_left.ID = in_right.ID where ID is the record ID within the group.\n3. In a Representative Creator, I remove the combinations where the pairs are the same i.e. left ID appears on right join and vice versa, eg. 4 vs 5, and 5 vs 4.\n4. In the Representative Creator, I set a dynamic query to check the similarity between the all 16 combinations - that is the code in my first post.\nThen the desired result would be to find the highest match, indicating which fields are attached - data comes from W_CUA1 vs. W_CUA3_MATCH etc.\nFirst screenshot demonstrates the component flow I built, while second one is the case of the field matching through a life-like example.\nAnd I still have no clue how to present in what form I want to see the results i.e. how to mark which fields bear the highest rank combination. Another field should store this information eg. ‘W_CUA1|W_CUA3_MATCH’.\nAnd yes, your question is relevant how to rank the results when we have eg. 3 pairs with the same level match - I don’t know yet, but learning the database, there is zero chance of such case, due to the nature of the data inspected here.\nIs this clear I wrote or I just made it a bigger mess? I hope I did not.\nThank you, Laszlo"
      },
      {
        "author": "Lazov",
        "timestamp": "3 months ago",
        "content": "Hi Adrian, there’s a mistake I made here, but the page does not save the text again when I edit and send it.\nInstead of this:\n‘So now, in the same row, I have the in_left.W_CUA1... to in_left.W_CUA4 and in_right.W_CUA1... to in_right.\nW_CUA16\n’\nI meant this:\n‘So now, in the same row, I have the in_left.W_CUA1... to in_left.W_CUA4 and in_right.W_CUA1... to in_right.\nW_CUA4\n’\nSorry for confusion,\nLaszlo"
      },
      {
        "author": "Adrian Anderson",
        "timestamp": "3 months ago",
        "content": "Hi Laszlo.\nApologies for not responding sooner. There are a couple of issues with the Community site at the moment and I’m not able to see the images shared previously.\nCould you perhaps try sharing them again and/or attaching a copy of any shareable plans/components?\nThanks,\nAdrian"
      },
      {
        "author": "Lazov",
        "timestamp": "3 months ago",
        "content": "Hi ​\n@Adrian Anderson\n,\nno need to apologize, I am grateful that you are dealing with this challenge I have.\nI rather embed the same here in the text box - sharing the whole thing would not meet my agreement with my customer on data handling. Building it in a demonstrative test component would take more time than I have at the moment.\nBut certainly I will do so if your feedback was that you could not see a thing again. :-)\nSo now, let’s try again.\nFirst screenshot demonstrates the component flow I built, while second one is the case of the field matching through a life-like example.\nPlease let me know whether you can see the images this time.\nThank you, Laszlo"
      },
      {
        "author": "Lazov",
        "timestamp": "1 month ago",
        "content": "Hi ​\n@Adrian Anderson\n,\nI finally managed to build a hard coded line, similar to XML structure, in a Representative Creator that does all the calculations and gets the valid data for each case.\nToo bad that it is still 16 hard coded lines because I have 16  combinations in total and Ataccama is not able to handle a single loop with iterator.\nBR, Laszlo"
      },
      {
        "author": "Cansu",
        "timestamp": "1 month ago",
        "content": "Hi ​\n@Lazov\n, thank you for sharing your implementation. If you’d like please share your use case and feedback in our\nideas section\nwhere our product team has direct access to review 👈"
      }
    ]
  },
  {
    "title": "Write multi-line Definition property with Metadata Writer",
    "url": "https://community.ataccama.com/data-quality-catalog-94/write-multi-line-definition-property-with-metadata-writer-1126",
    "question": {
      "author": "Albert de Ruiter",
      "timestamp": "[No timestamp]",
      "content": "Hi Community,\nThe purpose of the follwing scenario is to update multiple term names and definitions. I have created a plan that writes the terms names, definitions and GID to a csv file. In that file I can make updates on the names and definitions. With another plan I read that file and write it back into Ataccama.\nThat works well, except in the following case. The text in the Definition can be entered on multiple lines by using the Enter button. So OneWeb will show:\nText on line 1.\nText on line 2.\nText on line 3.\nIn the export (the csv file) the text will look like this:\n\"[{\"\"type\"\":\"\"paragraph\"\",\"\"children\"\":[{\"\"text\"\":\"\"Text on line 1.\"\"}]},{\"\"type\"\":\"\"paragraph\"\",\"\"children\"\":[{\"\"text\"\":\"\"Text on line 2}]},{\"\"type\"\":\"\"paragraph\"\",\"\"children\"\":[{\"\"text\"\":\"\"Text on line 3}]}]\"\nIf you import the text like this, the content will not appear on 3 lines, but literally as above and on one line.\nHow can I import the text so it is on 3 lines again?\nThanks for any suggestion.\nKind regards,\nAlbert"
    },
    "answers": [
      {
        "author": "joyce",
        "timestamp": "1 year ago",
        "content": "Hi\n@Albert de Ruiter\n! You can alter term definitions using a Column Assigner step to include the newline character\n\\n\nto indicate a new line. For example,\n\"Text on line 1. \\nText on line 2. \\nText on line 3. \\n\"\nwill separate each sentence onto a newline. Hope this helps!"
      },
      {
        "author": "Albert de Ruiter",
        "timestamp": "1 year ago",
        "content": "Hi\n@joyce\n,\nThanks for thinking along. Actually I had already tried to use the \\n in the input file, as replacement of the \"[{\"\"type\"\":\"\"paragraph\"… etc. But then the \\n is not recognized as a newline character by the Metadata Writer. Do you have a clue?\nKind regards,\nAlbert"
      },
      {
        "author": "DaanDirven",
        "timestamp": "1 year ago",
        "content": "I use  replace(AttributeDescription, \"\\l\\n\",\" \")  in a similar case where I needed to delete the CR/LF from an attribute discription. After that the csv could properly be imported"
      },
      {
        "author": "Albert de Ruiter",
        "timestamp": "1 year ago",
        "content": "Hi Daan, actually the idea is to keep the linefeeds in the Definition field of the term!\nSo the question is, when I use a csv file as input in a desktop plan as follows\nand using a Metadata Writer to import the data back into Ataccama\nin OneWeb how do I get the definition to show on 3 lines separately, like\nBut with ‘Text on line 1. \\n Text on line 2. \\n Text on line 3.’ in the csv file in OneWeb the definition content remains the same, so again ‘Text on line 1. \\n Text on line 2. \\n Text on line 3.'."
      },
      {
        "author": "joyce",
        "timestamp": "1 year ago",
        "content": "Hi\n@Albert de Ruiter\n, I’ve created a plan which imports\nABC Term\nand a description with newlines. In the ONE Web App, each description line populates onto a newline.\nIf this doesn’t work for you, could you verify that the description is correct (with the proper newline characters) before the ONE Metadata Writer step? Along with a screenshot of the ONE Web App term description that’s populated after the plan is run? Thanks!"
      },
      {
        "author": "Albert de Ruiter",
        "timestamp": "1 year ago",
        "content": "Hi\n@joyce\n, thanks for your example. I can recreate this successfully on my side, but my scenario is a bit different with respect to the input of data. As input for the plan I have a csv file that contains many updates to be executed via the Metadata writer step. In your example the data input is manually in the Alter format, so that approach won't work for me. I have tried to include your setup in my scenario as follows, but unfortunately the definition content is still on 1 line."
      },
      {
        "author": "Albert de Ruiter",
        "timestamp": "1 year ago",
        "content": "I have now also raised this question at Ataccama Support. I will share the outcome in this post."
      },
      {
        "author": "Albert de Ruiter",
        "timestamp": "1 month ago",
        "content": "Hi all,\nAlso with the aid of Ataccama Support there is no real solution available. Meanwhile I have created a component that I use for instance when exporting glossary details (the business definition field mainly). The component does most of the job, but it isn't flawless. Sometimes still clutter appears, so based on that some further cleaning steps can be defined.\nFrankly not all cleansing requirements are straightforward: basically I want to get rid of the json-like formats that are added in the output, but also formatting options like italic/bold text, or bulleting will lead to ‘garbage’ in the export. But then, how can you keep that formatting in an export to either a text file or a database table?\nRegarding the component, the approach is quite basic (or ugly, actually), simply replacing unwanted strings by an empty string. I have choses a multi-step approach in order to keep the component understandable, because if you nest all steps the logic becomes quite unreadable.\nSo if anyone has a better idea, please share 😉\nWith respect to the attachment, a component file could not be uploaded so I renamed the file extension. Rename back to ‘comp’and you can use it in Desktop again.\nKind regards,\nAlbert"
      },
      {
        "author": "Cansu",
        "timestamp": "1 month ago",
        "content": "Thank you for sharing an update here ​\n@Albert de Ruiter\n🙌"
      }
    ]
  },
  {
    "title": "How can we see the Cumulative Data Quality Score in Ataccama One WebApp for a CDE ?",
    "url": "https://community.ataccama.com/data-quality-catalog-94/how-can-we-see-the-cumulative-data-quality-score-in-ataccama-one-webapp-for-a-cde-1699",
    "question": {
      "author": "Apurva Kapoor",
      "timestamp": "[No timestamp]",
      "content": "For example, if there are multiple rules applied on an Attribute that is CDE then would it show filtered Data Quality Score based on the nested rule that is implemented. Can it be checked or configured in Ataccama One WebApp or it would need to be done in Ataccama One Desktop ?"
    },
    "answers": [
      {
        "author": "anna.spakova",
        "timestamp": "1 month ago",
        "content": "Hello ​\n@Apurva Kapoor\n,\nat the moment, the score is available only as part of the post-processing output, either via post-processing plans or transformation plans:\nhttps://docs.ataccama.com/one/latest/monitoring-projects/post-processing-plans.html\nhttps://docs.ataccama.com/one/latest/monitoring-projects/data-transformation-plans.html\nThere is an extra column Score that is a cumulative score of all failed DQ checks for the record.\nI hope this answers the question.\nKind regards,\nAnna"
      }
    ]
  },
  {
    "title": "Monitoring Project and DQ Quality rule suggestion",
    "url": "https://community.ataccama.com/data-quality-catalog-94/monitoring-project-and-dq-quality-rule-suggestion-1687",
    "question": {
      "author": "akshayl09",
      "timestamp": "[No timestamp]",
      "content": "In Monitoring Project Data quality rule suggestion works sometimes and it give rules already used or as per term assigned to catalog item but most of the time it doesn't work. At point of time, rule suggestions appear, and they disappear when you want to check again.\nHow the rule suggestions work in Monitoring Project of Ataccama?  Is there any pattern or do we have to do something so that we can rule suggestions?"
    },
    "answers": [
      {
        "author": "ekaterina.ponomareva",
        "timestamp": "1 month ago",
        "content": "Hi,\nThank you for your question!\nIn Monitoring Projects Rules are suggested in the following situations:\nwhen the rule is applied to a term applied to an attribute.\nwhen the rule is applied to the same attribute in another monitoring project.\nThe rules suggested can be accepted or rejected. Also, the rules suggestion can be enabled/disabled.\nIf you are interested in more details, please see the dedicated page in documentation -\nhttps://docs.ataccama.com/one/latest/data-quality/rule-suggestions.html#disable-or-enable-rule-suggestions\nLet me know if you have more questions.\nKind regards,\nEkaterina"
      },
      {
        "author": "akshayl09",
        "timestamp": "1 month ago",
        "content": "Thanks for information!\nIt seems that rule suggestion settings are available in 15.4 and above version. It is not available in 14.5 version.\nIn Monitoring Projects Rules are suggested in the following situations:\nwhen the rule is applied to a term applied to an attribute.\nwhen the rule is applied to the same attribute in another monitoring project.\nThis what I am expecting but it works sometimes and most of the time doesn't work. Hence, is there any solution to it?"
      },
      {
        "author": "Srija",
        "timestamp": "1 month ago",
        "content": "Hi ​\n@ekaterina.ponomareva\n,\nIs there a way to fetch those Dq rule suggestions like how we export the term suggestions that are linked to the attributes . I’m trying to use the json call or check the metadata reader steps it is only giving the rules which are mapped to the attributes not the rule suggestions. Is there a API call which I can fetch the rule suggestions that are assigned to the attributes.?"
      },
      {
        "author": "ekaterina.ponomareva",
        "timestamp": "1 month ago",
        "content": "Hi ​\n@akshayl09\nThe Monitoring Project Rule suggestions should work in v.14, as well.\nIs that possible that you have loaded terms via desktop plan? In such case, they might work differently.\nKind regards,\nEkaterina"
      },
      {
        "author": "ekaterina.ponomareva",
        "timestamp": "1 month ago",
        "content": "Hi ​\n@Srija\n,\nThat is an interesting question, thank you for that! Let me check how the query would look like and come back to you.\nKind regards,\nEkaterina"
      },
      {
        "author": "Srija",
        "timestamp": "1 month ago",
        "content": "Hi ​\n@ekaterina.ponomareva\n,\nThanks for responding. I was able to pull those information from API. You can close this thread.\nRegards,\nSrija Piratla"
      }
    ]
  },
  {
    "title": "User Admin",
    "url": "https://community.ataccama.com/data-quality-catalog-94/user-admin-1693",
    "question": {
      "author": "SamMAG",
      "timestamp": "[No timestamp]",
      "content": "Is there a way to identify usage stats, last login etc easily?  or if someone has built a query to show this information?  I would think the admin would have an easy where to manage users access / licenses etc"
    },
    "answers": [
      {
        "author": "ekaterina.ponomareva",
        "timestamp": "1 month ago",
        "content": "Hi ​\n@SamMAG\n!\nThank you for your question.\nThere are several places where you can get these stats:\nKeycloak:\nThe \"Events\" section where you can view a list of User IDs, date/timestamps of events (such as logins), and the IP addresses from which they originated. This can be filtered for the desired time period.\nAudit Module:\nIt could be used to track activities performed by users during their login sessions. This module provides detailed logs of user actions.\nLet me know if you have more questions.\nKind regards,\nEkaterina"
      }
    ]
  },
  {
    "title": "Get all the workflow names from runtime server",
    "url": "https://community.ataccama.com/data-quality-catalog-94/get-all-the-workflow-names-from-runtime-server-1654",
    "question": {
      "author": "anik",
      "timestamp": "[No timestamp]",
      "content": "Is there a way to retrieve all the workflows deployed on the Ataccama runtime server? For instance, we can obtain all the monitoring projects and post-processing details from the Ataccama metadata model. Additionally, I would like to get the workflow names along with their associated plans and component files from the runtime server."
    },
    "answers": [
      {
        "author": "Adrian Anderson",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@anik\n.\nCould you tell us a bit more about your use case, please?\nAre you wanting to see the workflows in Ataccama Desktop or somewhere else? If you have access to the git repo for the respective environment, the workflows and associated plans will be part of the environment’s branch and can be simply pulled into Desktop from there.\nKind regards,\nAdrian"
      },
      {
        "author": "anik",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@Adrian Anderson\n, thank you for your response. I need a list of all workflow names, associated plans, and scheduler names from the runtime server or Git. To achieve this, I want to create a workflow that retrieves the list of all workflows currently deployed in the production or development branch."
      },
      {
        "author": "anik",
        "timestamp": "1 month ago",
        "content": "Hi ​\n@Adrian Anderson\n, Can we extract a list of workflow names, associated plan names, folder names and scheduler names as shown below by creating a plan or workflow from the runtime server?"
      }
    ]
  },
  {
    "title": "Import configuration in monitoring project",
    "url": "https://community.ataccama.com/data-quality-catalog-94/import-configuration-in-monitoring-project-1688",
    "question": {
      "author": "akshayl09",
      "timestamp": "[No timestamp]",
      "content": "While importing the configuration in Monitoring Project from different monitoring Project, we can map catalog item but unable to it on column level.\nHere column names are getting changed in databrick layers (foundation, Enriched, curated).\nIs there any way to import configuration from one project to another project with column mapping as names gets changed?"
    },
    "answers": [
      {
        "author": "ekaterina.ponomareva",
        "timestamp": "1 month ago",
        "content": "Hi!\nThank you for your question!\nIn version 15.4 of the product you have an option to manually map the attributes.\n1)Please select different catalog item\n2) Map attributes\nPlease let me know if you have any other questions.\nKind regards,\nEkaterina"
      },
      {
        "author": "akshayl09",
        "timestamp": "1 month ago",
        "content": "Thanks for information."
      }
    ]
  },
  {
    "title": "Dality Quality and Governance Flow Diagram/ Architectural Diagram",
    "url": "https://community.ataccama.com/data-quality-catalog-94/dality-quality-and-governance-flow-diagram-architectural-diagram-1689",
    "question": {
      "author": "Harshitha Gedela",
      "timestamp": "[No timestamp]",
      "content": "Architectural Diagram for Data Quality and Governance"
    },
    "answers": [
      {
        "author": "ekaterina.ponomareva",
        "timestamp": "1 month ago",
        "content": "Hello!\nDepending on the version of the product and type of deployment the diagrams can vary a little.\nPlease find an example of High Level Architecture Diagram for hybrid PaaS deployment.\nMore low-level details will depend a lot on the way your solution is implemented and customized.\nLet me know if you have any more questions.\nKind regards,\nEkaterina"
      },
      {
        "author": "Harshitha Gedela",
        "timestamp": "1 month ago",
        "content": "Thank you, Ekaterina,"
      }
    ]
  },
  {
    "title": "Is ataccama one and DQ analyzer are same ?",
    "url": "https://community.ataccama.com/data-quality-catalog-94/is-ataccama-one-and-dq-analyzer-are-same-1679",
    "question": {
      "author": "lokeshpotti",
      "timestamp": "[No timestamp]",
      "content": "I am new to Ataccama and have some basics questions on how to start .\nis the ataccama one desktop and DQ analyzer are same tool or different ?\ni have installed ataccama one desktop but how to get trial license ?"
    },
    "answers": [
      {
        "author": "Cansu",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@lokeshpotti\n, DQ Analyzer is a legacy feature that no longer is available in Ataccama. You can get started on\nONE Desktop here,\nand within the tutorials in product in ONE Desktop. I’ve already sent an email to you regarding licensing. Please let me know if you have any additional questions 🙋🏻‍♀️"
      }
    ]
  },
  {
    "title": "detection",
    "url": "https://community.ataccama.com/data-quality-catalog-94/detection-1680",
    "question": {
      "author": "sumisha",
      "timestamp": "[No timestamp]",
      "content": "How do we create a detection rule which either using both data and metadata option in same rule or ability to add multiple detection rules on business glossary with AND and OR conditions both working together and not just one option?"
    },
    "answers": [
      {
        "author": "Cansu",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@sumisha\n, it’s not possible on ONE Web to use both data and metadata option at the same time, this also applies for and/or. You can\nsee more information here in our documentation."
      }
    ]
  },
  {
    "title": "DQ Firewall API Integration",
    "url": "https://community.ataccama.com/data-quality-catalog-94/dq-firewall-api-integration-1662",
    "question": {
      "author": "Gloria",
      "timestamp": "[No timestamp]",
      "content": "Can the API response generated from the Data Quality Firewall feature be edited?"
    },
    "answers": [
      {
        "author": "ekaterina.ponomareva",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@Gloria\n!\nThank you for your question.\nIt depends on what you want to change in the response generated by DQ Firewall. The response would be different if you changed anything in the rule, e.g. explanations, DQ dimension, DQ dimension results, or you can change the label (ruleInstanceId in the response).\nDoes this answer your question? If there is anything else you need to understand, please let me know.\nKind regards,\nEkaterina"
      },
      {
        "author": "Cansu",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@Gloria\n, I’m closing this thread for now, if you have any follow-up questions please do share them in the comments or create a new post 🙋🏻‍♀️"
      }
    ]
  },
  {
    "title": "License issue with free desktop version of DQ analyzer",
    "url": "https://community.ataccama.com/data-quality-catalog-94/license-issue-with-free-desktop-version-of-dq-analyzer-723",
    "question": {
      "author": "Gregers",
      "timestamp": "[No timestamp]",
      "content": "Hi\nI have with great success been using your free desktop version of DQ Analyzer.\nBut the license has run out. Last time I had this issue a new download and a re-installation solved it.\nI have tried to do the same, but it did not work.\nSee issue “DQ analyzer windows version update” for further details.\nAny advice what to do?"
    },
    "answers": [
      {
        "author": "Cansu",
        "timestamp": "1 year ago",
        "content": "Hi\n@Gregers\n, thank you for taking the time to post here. I’ve contacted our team internally, they’ll be reaching out to you as soon as possible."
      },
      {
        "author": "TimMcG",
        "timestamp": "1 year ago",
        "content": "Hi, I also have the same problem. I have downloaded the latest version of DQ Analyser but the tool is prompting for a license."
      },
      {
        "author": "dickensr",
        "timestamp": "1 year ago",
        "content": "Same problem."
      },
      {
        "author": "Gregg",
        "timestamp": "1 year ago",
        "content": "Hi, also getting this error after downloading the latest version."
      },
      {
        "author": "BillS",
        "timestamp": "1 year ago",
        "content": "How do I get the free license?  I haven’t used DQA for awhile and the last time I used is you would request a license and you would get a licence file returned that was good for a year.  Is that still the process?"
      },
      {
        "author": "DannyRyan",
        "timestamp": "1 year ago",
        "content": "Hi everyone,\nQuick update: Our team is actively working on resolving the license issue with the free desktop version of DQ Analyzer. We're currently generating an updated license and incorporating it into the upcoming build.\nRest assured, our development and license teams are aware of the situation and are working diligently to resolve it as quickly as possible.\nWe'll keep you posted on any major updates. Thank you for your patience!"
      },
      {
        "author": "Silvia",
        "timestamp": "1 year ago",
        "content": "Hi,\nI also have the same problem, so I would like to know if you have any advance about it\n@DannyRyan\nThanks"
      },
      {
        "author": "Mario",
        "timestamp": "1 year ago",
        "content": "Same problem, hope it will be resolved soon"
      },
      {
        "author": "DannyRyan",
        "timestamp": "1 year ago",
        "content": "Hello everyone,\nI am excited to announce the availability of a new build of DQ Analyzer!\nWe invite you to download the latest version of DQA along with a brand-new license key, which will remain valid for one year until July 8th, 2024. Simply visit the following link to initiate the download:\nhttps://www.ataccama.com/download/dq-analyzer\nThe download package is a convenient .zip archive that includes the .exe binary, .plf license file, and a readme.txt file. The readme.txt file provides clear instructions on how to seamlessly replace the license file after installation.\nThank you for your continued support!"
      },
      {
        "author": "Silvia",
        "timestamp": "1 year ago",
        "content": "Hi,\n@DannyRyan\nthanks a lot for this solution. I followed the steps and problem solve! My DQA is working as always."
      },
      {
        "author": "mdaflucas",
        "timestamp": "1 year ago",
        "content": "Well, according to the link it was supposed to be resolved yesterday.\nI downloaded 11.1.1 yesterday and I still have the problem."
      },
      {
        "author": "Gregg",
        "timestamp": "1 year ago",
        "content": "@DannyRyan\nThanks, I have downloaded the latest .zip and applied the license included in it to my current installation (the one that did not work) and it is now working correctly."
      },
      {
        "author": "Cansu",
        "timestamp": "1 year ago",
        "content": "Hi\n@mdaflucas\n, I’ve reached out to you via PM."
      },
      {
        "author": "Gregers",
        "timestamp": "1 year ago",
        "content": "@DannyRyan\nI had same experience, a new download did not fix the problem, but installing the new license key worked 😀"
      },
      {
        "author": "lokeshpotti",
        "timestamp": "2 months ago",
        "content": "i have installed ataccama one desktop but it is asking for license to execute anything. Can someone please help in providing free trial license."
      },
      {
        "author": "Cansu",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@lokeshpotti\nI’ve sent you an email 🙋🏻‍♀️"
      }
    ]
  },
  {
    "title": "Confusing date data",
    "url": "https://community.ataccama.com/data-quality-catalog-94/confusing-date-data-1602",
    "question": {
      "author": "TimBrown74",
      "timestamp": "[No timestamp]",
      "content": "The Business wants this date attribute validated.  I don’t know why, it’s a key field and I assume managed by the DB.  Nonetheless, I need to build a rule.\nBut I think it’s strange that the Data Type is\nDATE\nwhile the date values are YYYY-MM-DD. I would expect to see DATE format data in the MM-DD-YYY HH:MM:SS AM format.\nI don’t know how to code it in the date functions since it doesn’t have the standard date format and it doesn’t work in any string functions.\nAny ideas?"
    },
    "answers": [
      {
        "author": "sam.dahl",
        "timestamp": "3 months ago",
        "content": "Hi Tim,\nIt sounds like this is already a proper DATE type, so there’s no need for time or string functions. The way it’s displayed (YYYY-MM-DD vs. MM-DD-YYYY HH:MM:SS AM) is just a formatting choice, not usually how it's actually stored. In most DBMS, DATE fields are strictly typed and inherently validate as proper Gregorian dates so there’s no “format” in storage, just a native date representation.\nIf the goal is validation using Evaluation rules, then the usual things to check would be:\na) Missing values\n- are there nulls where there shouldn’t be?\nb) Expected ranges -\ndo dates fall within a logical timeframe (e.g., no orders from 1800)?\nc) Common default or disallowed values -\nthings like 1900-01-01, 9999-12-31, or placeholders that might indicate bad data.\nd) Relativity & dependencies -\ndoes this date make sense in relation to others? (e.g., a ship date shouldn’t be before an order date.)\ne) Uniqueness -\nprobably not relevant unless this is actually a key field (which seems unlikely-maybe it’s just important rather than a true key).\nIf these dates were loaded from another source and incorrectly interpreted during ingestion, there could be a mix of issues-not just valid-looking but incorrect dates, but also records that were dropped entirely if parsing failed. That would likely have been framed as a bigger problem, though, so it’s probably not the case here.\nOne other possible issue is truncation or timezone shifts. If the original source included a time component and it was stripped to just a DATE, you could have unintended offsets-some systems default to midnight UTC when stripping time, which might cause subtle shifts depending on how time zones were handled. It’s an edge case, but worth considering if unexpected discrepancies show up.\nOtherwise, if the field is already a strict DATE type in the DB, this is more of a standard data integrity check rather than a formatting issue.\nDoes that help?"
      },
      {
        "author": "Albert de Ruiter",
        "timestamp": "3 months ago",
        "content": "Hi Tim,\nIn addition to what Sam already explained, there is also a distinction between a date and a datetime (or timestamp) datatype. In this case apparently the date is displayed without the time details, but it can also be displayed as date with 00:00:00 as time details.\nIf the business is only requesting to make sure that the values are only date values, so Tim's examples a-e don't apply,\nand\nindeed the date datatype applies for this field in the database, you can simply tell the business that their requirement is already met.\nKind regards,\nAlbert"
      },
      {
        "author": "TimBrown74",
        "timestamp": "3 months ago",
        "content": "Thank you, your replies are in line with my experience."
      },
      {
        "author": "Cansu",
        "timestamp": "3 months ago",
        "content": "Thank you ​\n@sam.dahl\nand ​\n@Albert de Ruiter\nfor sharing your solutions here!"
      },
      {
        "author": "TimBrown74",
        "timestamp": "3 months ago",
        "content": "Thank Sam and Albert.\nMy experience aligns with your responses.\nI have passed your comments onto my mgr, who has the task (if he chooses) to convince the business users."
      },
      {
        "author": "TimBrown74",
        "timestamp": "2 months ago",
        "content": "Another analyst developed this code to validate a datetime datatype.  It seems valid to me:\nIn the Variables section:\nIn the logic section:\nNot (VARIABLE_1 is not null)"
      },
      {
        "author": "Cansu",
        "timestamp": "2 months ago",
        "content": "Thank you for sharing ​\n@TimBrown74\n✔️🙌"
      }
    ]
  },
  {
    "title": "Bulk uploading relationships between attributes and business terms in Ataccama",
    "url": "https://community.ataccama.com/data-quality-catalog-94/bulk-uploading-relationships-between-attributes-and-business-terms-in-ataccama-1685",
    "question": {
      "author": "anastasiia.popova",
      "timestamp": "[No timestamp]",
      "content": "Hi everyone,\nIf you’re looking for a way to bulk upload relationships between attributes and business terms in Ataccama, there are two main approaches you can use: Metadata Writer step and GraphQL API. Both methods allow you to establish these relationships efficiently.\nOption 1: Metadata Writer\nThe\nMetadata Writer\nmethod enables you to create relationships between attributes and business terms by structuring your data appropriately. Here’s what you need to do:\nEntity: Use\ntermInstance\nas the entity type.\nParent ID: This should be the ID of the attribute you want to link.\nTarget: This should be the ID of the business term you want to associate with the attribute.\nState: Set this value to\npublished\nto ensure the relationship is active.\nOnce you’ve prepared a file containing these details, you can upload it using the Metadata Writer, automating the bulk linking process.\nOption 2: GraphQL API\nAnother approach is leveraging the\nGraphQL API\n. This involves:\nCapturing the API requests made when manually attaching terms through the Ataccama UI.\nReverse-engineering these API calls to understand the structure and parameters required.\nScripting a bulk upload process using these API calls to automate term-to-attribute relationships.\nThis method provides greater flexibility and can be integrated into existing workflows for automated management of metadata relationships.\nWhich method should you choose?\nIf you prefer a structured file-based approach, the Metadata Writer is straightforward and easy to use.\nIf you need a more dynamic, automated solution, the GraphQL API offers greater control and scalability.\nHave you tried bulk uploading business term relationships in Ataccama? Share your experience or any challenges you’ve faced in the comments below! 👇"
    },
    "answers": []
  },
  {
    "title": "How to insert Columns from a Plan to Send Mail email body?",
    "url": "https://community.ataccama.com/data-quality-catalog-94/how-to-insert-columns-from-a-plan-to-send-mail-email-body-1671",
    "question": {
      "author": "vishnu",
      "timestamp": "[No timestamp]",
      "content": "I have created a plan for data quality checks using Data Quality Indicator, and it will produce an excel file which flags each records with error_messages corresponding to each checks, and a summary txt file. Now, I have created  a workflow using Run DQC and Send Mail step. The requirement is to soft code the below values in the email template.\nSummary excel file\nBelow is the email body:\nDear Business Users,\nPlease find the attached Error Result (Error_Report.csv), which details the records that failed validation and their respective error types.\nSummary:\nTotal Processed Records(Total):\nTotal Error Records(Success_Count):\nError Types(Code):\nWe recommend reviewing the attached file and taking the necessary corrective actions. If you need further clarification or assistance, please feel free to reach out.\nBest regards,"
    },
    "answers": [
      {
        "author": "Adrian Anderson",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@vishnu\n.\nWith the Sendmail step, columns can be embedded into the email by surrounding them in curly brackets, e.g., {count_total}.\nIs this what you were looking for?\nKind regards,\nAdrian"
      },
      {
        "author": "vishnu",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@Adrian Anderson\n, thank you for your help. So, I’m looking to insert the columns in the Send Email task inside the wok flow.\nPlease see the attachments.\nWork-flow\nSend Email task"
      }
    ]
  },
  {
    "title": "Duplicate Record in the table based on all the columns",
    "url": "https://community.ataccama.com/data-quality-catalog-94/duplicate-record-in-the-table-based-on-all-the-columns-1667",
    "question": {
      "author": "akshayl09",
      "timestamp": "[No timestamp]",
      "content": "Is there a method to create a single rule for identifying duplicate records based on all columns in a table? This rule should be applicable to all tables, regardless of the number of columns they contain.\nCurrent situation we need to add the column to use aggregated rule. It is very difficult if the columns are more than 10 or 15 in table."
    },
    "answers": [
      {
        "author": "Lisa Kovalskaia",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@akshayl09\n, the only option I can think of is to create an SQL CI or a VCI which would present a single column with the concatenated values of the source columns. The uniqueness rule would be very simple then, but of course there would be some additional effort at the data preparation stage. Whether or not this approach is reasonable also depends on where you do data prep, e.g. in the source system or in Ataccama. If you already use SQL CIs/VCIs to preshape your source data for DQ, then you might as well add the composite key column, but if you generally apply rules to source tables then I'd rather keep the complexity in the aggregation rules.\nI\\m curious what is the use case beyond identifying these duplicates? It sounds like this is a very common issue in the data, why does it occur and how do you work with it after getting DQ results? Thanks for sharing!"
      },
      {
        "author": "akshayl09",
        "timestamp": "2 months ago",
        "content": "Yes. Use case is to find out duplicates on fact tables which mostly has composite keys made of many columns. There are some of the tables which 15-20 columns in the table. In Sql, it can be quickly done. select count (*), count (distinct (*)). Creating SQL CI is option, but it has to create for all the tables which duplication of tables.\nAlthough, we tried to create the common rule based on composite key in table, many tables have different combinations (datatype) of the composite key, so we need to create separate rule for each table. Just a thought about re-usability of the rule which are not dependent data type so that one rule can be used re-used for any combination of datatype. Each table has different number of columns to form composite key sometime is 2 or 3 or 10 as well so duplicate checks need to be created for each of the table. If there is something table level rule. It might be helpful to avoid the duplication of rules. Same rule can be re-purpose for all tables."
      },
      {
        "author": "Lisa Kovalskaia",
        "timestamp": "2 months ago",
        "content": "​\n@akshayl09\nsounds like creating SQL CIs for just this one purpose may not be the right approach for you. In that case you may need to continue with data-specific / non-reusable uniqueness rules - sometimes this is a necessary tradeoff to support certain validation requirements.\nFrom what you described, in some tables composite keys are a combination of all columns, while in other tables, only a subset of columns make up the composite keys, and on top of that there's variation in data types. All in all, I think adding a generic product feature to make this sort of uniqueness rule reusable would be a significant change from how rules work at the moment. Nevertheless, if you have a moment please share your idea on our dedicated Community space for product feedback:\nhttps://community.ataccama.com/ideas\n. This is where our product team draw insights on customer needs and your suggestion may gain inputs and upvotes from other users.\nThank you!"
      },
      {
        "author": "akshayl09",
        "timestamp": "2 months ago",
        "content": "Thanks Lisa! I have shared the idea\nDuplicate Record in the table based on all the columns | Community"
      }
    ]
  },
  {
    "title": "Arithmetic operations on Lookup using DQ evaluation rule in Ataccama One ui",
    "url": "https://community.ataccama.com/data-quality-catalog-94/arithmetic-operations-on-lookup-using-dq-evaluation-rule-in-ataccama-one-ui-1656",
    "question": {
      "author": "Ayush kumar",
      "timestamp": "[No timestamp]",
      "content": "I am making a DQ evaluation rule in ataccama one ui where I have to subtract each long value of catalogue item from its corresponding long value in lookup and if the difference is greater than 100, I want to raise that particular value as invalid sample, so far I am not able to achieve this, it would be great if anyone can help me with this."
    },
    "answers": [
      {
        "author": "Lisa Kovalskaia",
        "timestamp": "2 months ago",
        "content": "​\n@Ayush kumar\nI would turn to the ONE Desktop app here and build a component rule. Using the Lookup step, you can not only check that the given value matches one of the values in the lookup file, but also capture the specific lookup key value that was found in the file. That is not something you can do in the web app.\nIn the Lookup step configuration, use the lookup.key expression to store the value into some working column:\nFrom there you could calculate the difference between the src_value and the working column, and decide the VALID/INVALID result based on the outcome.\nDoes that help?"
      },
      {
        "author": "Ayush kumar",
        "timestamp": "2 months ago",
        "content": "Thanks for the solution ​\n@Lisa Kovalskaia\n. That's quite helpful, just wanted to know how can I make this solution possible, I mean I am having my lookup and catalog item in web version, how can I establish a connection in between web version and desktop version and take my lookup and CI there , it would be great if you can provide any link of ataccama support/help document or link of any related community post."
      },
      {
        "author": "Lisa Kovalskaia",
        "timestamp": "2 months ago",
        "content": "​\n@Ayush kumar\nsure, first off, please connect your ONE Desktop to your ONE Platform:\nConnecting ONE Desktop to your Ataccama ONE Platform 🟣\nWith that integration, you can access and work with many key assets of the Web app, here's more on that:\nHow to Work with Data in ONE Desktop 🖥️\nFor your specific use case, you’ll want to build a DQ rule that relies on a validation component:\nhttps://docs.ataccama.com/one/15.4.0/data-quality/validation-components.html#working-with-validation-components\nNote that you will need to download the lookup file you have in your ONE Web app and upload it to the component in ONE Desktop - once you publish the component the lookup file will be uploaded and stored in Ataccama's object storage. Please refer to ONE Desktop Tutorials available within the app for additional info and examples.\nWhen the component rule is ready, you can go back to your ONE Web app and apply the rule to a catalog item - just like you would apply any other rule built in the Web app.\nI would also encourage you to check out the\nAtaccama Academy\n, there's a wealth of resources for self-paced learning, could serve as a great reference for all things Ataccama.\nIf you have additional questions, let me know!"
      },
      {
        "author": "Ayush kumar",
        "timestamp": "2 months ago",
        "content": "​\n@Lisa Kovalskaia\nyour replies are helping me immensely would be following this method, just wanted to know if we have an automated process to - download the lookup file i have in my ONE Web app and upload it to the component in ONE Desktop because this lookup file is updated daily following a schedule and it will be quite challenging for me to update this lookup file everyday for various rules."
      },
      {
        "author": "Lisa Kovalskaia",
        "timestamp": "2 months ago",
        "content": "​\n@Ayush kumar\nglad I could help!\nRefreshing lookups used in component rules requires some additional setup, since there's currently no way to \"link” a lookup from the Web app to a component.\nTypically an orchestration workflow is configured to run a lookup building plan on schedule and to place fresh lookup file(s) into the expected location(s). The validation component, and more specifically the Lookup step in it, should be look for the resource in that location (e.g. MinIO object storage). The file will change but the resource path should remain the same so that the component continues working automatically.\nIf you haven't worked with orchestration workflows before, here's a couple of links to get you started:\nhttps://docs.ataccama.com/runtime-server/latest/workflow-and-scheduler/workflow-and-scheduler.html\nM\nAtaccama Admin Centre: ServerConfig and RuntimeConfig for beginners (Self-managed installations) 1/2\nTutorial\nOnce you have the workflow it can be easily used to establish a refresh cadence for many lookups. Please let me know if you have additional questions!"
      },
      {
        "author": "Ayush kumar",
        "timestamp": "2 months ago",
        "content": "Thanks ​\n@Lisa Kovalskaia\n, its quite helpful, I would go through this, in case I am having any doubts I would ask you."
      }
    ]
  },
  {
    "title": "Data Profiling schedule with failure notification",
    "url": "https://community.ataccama.com/data-quality-catalog-94/data-profiling-schedule-with-failure-notification-1663",
    "question": {
      "author": "akshayl09",
      "timestamp": "[No timestamp]",
      "content": "Is there any way to get notification for data Profiling schedule failure with log details or successful."
    },
    "answers": [
      {
        "author": "Lisa Kovalskaia",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@akshayl09\n- the easiest way to see successful and failed profiling jobs is the Processing Center built into the Ataccama Web app:\nYou can open the Processing Center and narrow down to the subset of jobs you're interested in. There is not a way to set up a persistent job-specific notification from here, but you may add the Notification widget to your Home page in the app, to serve as a reminder to jump to the Processing Center when you log in to Ataccama.\nIf you do need these notifications to be job-specific, user/group-specific and persistent, I would recommend setting up a notification workflow to send e.g. email notifications or to create tasks in an issue tracking system like Jira. Ataccama's\nnotification handler\ncan listen to metadata events published by the platform, specifically create/update/delete events and their details. I would try to subscribe to events on the catalogItemProfile entity, which I expect should include information about profile type (manual or automatic), as well as relevant catalogItem, timestamp, etc. You would also configure a\nworkflow\nto receive event details from the notification handler and then generate notifications based on your requirements. Here's an\nexample\nthat follows the same pattern but for DQ monitoring notifications - I hope you will find it useful.\nIf you have questions or comments please do let me know!"
      },
      {
        "author": "akshayl09",
        "timestamp": "2 months ago",
        "content": "Thank you, Lisa! It appears that setting up notifications requires both the Runtime server and Ataccama Desktop. Is it possible to transition this functionality to a web-only version?\nManually tracking alerts for over 100 jobs, especially when they are scheduled outside of working hours, is somewhat challenging with processing center."
      },
      {
        "author": "Lisa Kovalskaia",
        "timestamp": "2 months ago",
        "content": "​\n@akshayl09\nI agree with you, the Processing Center isn't perfect for keeping an eye on a large number of jobs. I would encourage you to post your ideas on this topic to this dedicated Community area\nhttps://community.ataccama.com/ideas\n- our product team keeps a close eye on suggestions and upvotes there and whenever possible integrates customer feedback into our development roadmap.\nRight now I don't see any specific plans for the kind of notifications you described to be available in the web app, so for the time being the best option is to configure them with the help of ONE Desktop and Ataccama Server. Please don't hesitate to reach out to your Ataccama rep to discuss this, too!"
      },
      {
        "author": "akshayl09",
        "timestamp": "2 months ago",
        "content": "Thanks Lisa!\nI have posted idea on this topic.\nNotification for schedule in (Data profiling, LOOKUP update) | Community"
      }
    ]
  },
  {
    "title": "Best Practices for Searching JSON/XML Values in Database Columns Using Regular Expressions",
    "url": "https://community.ataccama.com/data-quality-catalog-94/best-practices-for-searching-json-xml-values-in-database-columns-using-regular-expressions-1673",
    "question": {
      "author": "onufry",
      "timestamp": "[No timestamp]",
      "content": "I'm working with a database table that contains a column storing complicated JSON/XML data structures. I need to implement data quality rules that will search for and validate specific values within these structures and compare them with values in other columns.\nMy specific requirements:\nNeed to search for particular patterns and values nested within JSON/XML structures\nConsidering using regular expressions for this search capability\nConcerned about the performance impact on database resources\nWant to ensure this approach is sustainable as data volume grows\nHas anyone implemented data quality rules for searching/validating values within JSON or XML columns? Are regular expressions an appropriate approach, or are there better alternatives within Ataccama (Desktop?) for handling hierarchical data structures?\nI'm particularly concerned about performance impacts and would appreciate insights from those who have solved similar challenges."
    },
    "answers": []
  },
  {
    "title": "Understanding general vs. specific notifications and how to enable email notifications",
    "url": "https://community.ataccama.com/data-quality-catalog-94/understanding-general-vs-specific-notifications-and-how-to-enable-email-notifications-1669",
    "question": {
      "author": "anastasiia.popova",
      "timestamp": "[No timestamp]",
      "content": "Hi everyone,\nManaging email notifications effectively is essential for monitoring Data Quality and ensuring timely alerts. In this post, we will explore the differences between general and specific notifications and how to enable specific email notifications, as they are turned off by default in self-managed installations.\nWhat is the difference between general and specific notifications?\nGeneral notifications\nare triggered based on predefined system-wide scenarios, such as monitoring failures or anomalies. These notifications alert users to general events occurring in the system, ensuring broad visibility into key operational issues. However, general notifications do not include detailed Data Quality results.\nOn the other hand,\nspecific notifications\nprovide more granular alerts, tailored to particular data quality conditions, thresholds, or projects. These notifications are linked to specific configurations, allowing users to receive alerts based on structured errors, Data Quality issues, or anomalies. Specific notifications ensure that recipients receive detailed information related to the exact Data Quality checks they are monitoring.\nIf only general notifications are active and no specific notifications are configured, the system will still send notifications for general events. However, without specific notifications, users will not receive detailed Data Quality metrics or targeted alerts.\nHow to enable email notifications in self-managed installations?\nBy default, email notifications are disabled in self-managed installations. To activate them, you must enable the necessary plugin and configure the SMTP server settings in the MMM Configuration section. This setup allows you to manage your email notification preferences efficiently. Please follow\nthis link\nto see more information about setting up SMTP server settings.\nOnce these configurations are in place, specific notifications will be active, providing detailed insights and customized alerts based on Data Quality conditions.\nHave you encountered challenges in managing email notifications for Data Quality monitoring? Let’s discuss in the comments below! 👇"
    },
    "answers": []
  },
  {
    "title": "Replicate LISTAGG(DISTINCT columnA, ' | ') OVER (PARTITION BY columnB) in ONE Desktop",
    "url": "https://community.ataccama.com/data-quality-catalog-94/replicate-listagg-distinct-columna-over-partition-by-columnb-in-one-desktop-1664",
    "question": {
      "author": "Radziah",
      "timestamp": "[No timestamp]",
      "content": "Hi\nI have a sub-query in Snowflake that is using the function of LISTAGG and PARTITION BY in multiple SELECT DISTINCT statements for example LISTAGG(DISTINCT TRIM(SUBJECT_REF), ' | ') OVER (PARTITION BY IDENTIFIER) AS EN_NUM_AGG, kindly see attachment to see the full SQL statement.\nExample of one of the subqueries:\n…\nen AS\n(\nSELECT DISTINCT\nIDENTIFIER,\nID,\nLISTAGG(DISTINCT ID, ' | ') OVER (PARTITION BY IDENTIFIER) AS ID_AGG,\nTRIM(SUBJECT_REF) AS EN_NUM,\nLISTAGG(DISTINCT TRIM(SUBJECT_REF), ' | ') OVER (PARTITION BY IDENTIFIER) AS EN_NUM_AGG,\nDQ_EN_CARD,\nLISTAGG(DISTINCT DQ_EN_CARD, ' | ') OVER (PARTITION BY IDENTIFIER) AS DQ_EN_CARD_AGG,\nDATA_SOURCE,\nORIGINAL_SOURCE\nFROM\ndb.schema.view1\nWHERE\nsubj_type = 'English'\n) ...\nI wanted to replicate the same in Ataccama ONE Desktop using the Group Aggregator step, where\nI allocate each sub-query in one aggregation set with a condition of subj_type = '<value>' in the When condition.\nFor each column in the SELECT DISTINCT subquery if it is a\nnon-aggregated value\n, I used first() function to get a distinct value,  for example first(ID). I tried using distinct() but it was not working with an error of 'unknown column' when the column is clearly available there. Why is that so?\nFor each column in the SELECT DISTINCT subquery if it is\naggregated value\n, I used this expression to replicate the same as LISTAGG.. concatenate(distinct(DQ_EN_CARD),'|'), however some columns are not producing any results but some have results like 1|1|0 for column like DQ_EN_CARD_AGG when the actual result should only show distinct value of 1 and 0 so it should be 1|0. I believe because in Ataccama it behaves slightly different compared to the Snowflake as Snowflake read and group line by line, but Ataccama seems to read all aggregations sets first then only group them?\nBased on this query for example,  LISTAGG(DISTINCT TRIM(SUBJECT_REF), ' | ') OVER (PARTITION BY IDENTIFIER) AS EN_NUM_AGG I define the Group By in the step to IDENTIFIER to replicate the PARTITION BY. I bet the behaviour is different here since I did not get the expected results.\nFinally, I joined the right output with the left output of the Group Aggregator because to my understanding the right join only will produce the columns that we add in the aggregation set, other columns that are not used in the aggregation set shall be retrieved from the left output.\nSo I am not sure whether this step is best to fit in this use case or not. Appreciate your assistance. Attached is the full SQL statement."
    },
    "answers": [
      {
        "author": "Phil Holbrook",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@Radziah\nThere are two steps in ONE Desktop that you could use to tackle this problem: the group aggregator and the representative creator.  Group aggregator is easier to understand coming from SQL - so we'll stick with that.  The way you are using the group aggregator generally looks OK.\nYour main problem is that concatenate and distinct are very different types of expression in ONE Desktop.\nConcatenate is an aggregating function - it is only meaningful in the context of a grouping operation (like the group aggregator) and it operates across each set of rows that are being grouped.  If you use the statement,\nconcatenate(DQ_EN_CARD, '|')\nyou are saying,\n\"Take the value of DQ_EN_CARD from every row and concatenate them separated by the '|' character\"\n... exactly as you would expect from the SQL equivalent.\nThe \"distinct\" function is\nnot\nan aggregating function and doesn't work like SQL.  It is not valid in an aggregating context, which is why you got the \"Unknown column\" error. Although the column exists, it is not single-valued.\nDistinct is actually a word-set operation - fully specified as\nset.distinct(<string>,<separator>)\nbut you can omit the \"set.\" prefix.  There are many word set operators which can be very useful. They operate on a single string value, from a single row, and treat the string as a set of values with the specified separator. The separator defaults to a space, hence \"word set\" operator - it was designed to handle sentences.\nSo, if you use the statement:\ndistinct('1|3|2|3|1', '|')\nyou will get the result '1|3|2'.\nSo - to achieve the concatenated set of distinct values you want in the group aggregator step, you need to specify\ndistinct(concatenate(DQ_EN_CARD, '|'), '|')\nIt has to be this way round, because you need to convert your multiple rows into a single string with the concatenate function, then deduplicate it with distinct.  You have to specify the separator in the context of both functions.\nSimilarly, there is a \"sort\" word set operation, so\nsort(distinct('1|3|2|3|1', '|'), '|')\nis '1|2|3' (Watch out, though - it’s an alpha-sort!)\nIt is a common use case to group values using an aggregating function and a separator, and then process the resulting string with word set operations.\nYou can read up on both aggregating functions and word-set expressions in the ONE Desktop built-in help under\nHelp -> Help Contents -> Ataccama ONE Desktop -> ONE Basics -> Expressions"
      }
    ]
  },
  {
    "title": "Joining SQL Views and Data Slices to DQ Rules",
    "url": "https://community.ataccama.com/data-quality-catalog-94/joining-sql-views-and-data-slices-to-dq-rules-1659",
    "question": {
      "author": "TimBrown74",
      "timestamp": "[No timestamp]",
      "content": "We’re using version 15.4, mostly in the web tool.\nWe’re trying to a subset of our input tables for testing before implementation the full table(s) in Production.\nWe looked at using Data Slices, but we found that the ROWNUM feature of SQL met our purpose better:\nselect * from (select * from dbname.tblname) where ROWNUM <=10000\nOur primary question is how do we link/connect the SQL View and/or the Data Slice to the DQ Rule.  We believe that it occurs in the Monitoring Project.\nI looked thorough the Ataccama online documentation, and the Community, but I can’t find the options to make these either of these connections.\nAny ideas!\nThanks!\nTim"
    },
    "answers": [
      {
        "author": "anna.spakova",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@TimBrown74\n, if I understand your question correctly, you are creating an SQL Catalog Item using the SQL query above instead of data slices. The SQL CI behaves like a standard new catalog item so to link it to a DQ rule, you have two options:\nvia a Monitoring project as you mention (under Configuration tab, you will just add this new SQL catalog item and add a DQ rule to it - see the documentation below)\napply a DQ rule to the Catalog Item directly in the Catalog Item DQ rule tab (see details below)\nCouple links to the documentation:\nMonitoring projects:\nhttps://docs.ataccama.com/one/15.4.0/monitoring-projects/monitoring-projects.html#add-catalog-items\nhttps://docs.ataccama.com/one/15.4.0/monitoring-projects/monitoring-projects.html#dq-rules\nDQ Rule via attributes in the Catalog Items:\nhttps://docs.ataccama.com/one/15.4.0/data-quality/add-dq-rules-to-attributes.html\nIn case you would like to use the Data slice, AFAIK you have only the option via a monitoring project:\nhttps://docs.ataccama.com/one/15.4.0/monitoring-projects/monitoring-projects.html#use-data-slices-in-monitoring-projects\nLet me know if this answers the question.\nKind regards,\nAnna"
      },
      {
        "author": "TimBrown74",
        "timestamp": "2 months ago",
        "content": "Thank you Anna, this was very useful."
      }
    ]
  },
  {
    "title": "AQL to filter catalog by Term name",
    "url": "https://community.ataccama.com/data-quality-catalog-94/aql-to-filter-catalog-by-term-name-1666",
    "question": {
      "author": "sumisha",
      "timestamp": "[No timestamp]",
      "content": "What is the AQL to filter all catalog items’ attributes tagged with a particular term?"
    },
    "answers": [
      {
        "author": "sumisha",
        "timestamp": "2 months ago",
        "content": "attributes.any(termInstances.any(displayName is 'xx'))"
      }
    ]
  },
  {
    "title": "Relationship type, or not?",
    "url": "https://community.ataccama.com/data-quality-catalog-94/relationship-type-or-not-1657",
    "question": {
      "author": "Albert de Ruiter",
      "timestamp": "[No timestamp]",
      "content": "Hi all,\nI would like to share the guidance of Ataccama Support regarding a question that I had with respect to using relationship types. I suppose the question will become relevant for us all at some point. So hereby the question and Ataccama's response.\nQuestion\n“In the metadata model we can define relations between entities by means of references, or we can embed properties from other entities etc.\nAnother way to define relations between entities is by means of relationship types. The latter option allows us to visualize the relation in for instance a relationship tab, like there is for terms.\nSuppose you are to model new entities. There is no requirement to have this visualized in a relationship tab. So you define a reference from one entity to the other. Later a requirement pops up from a new project, that the relation should be depicted in the relationship tab. So, unfortunately, not possible. Changing the reference into a relationship type is a major system change.\nThen always choose the safe way: only use relationship types? Feels a bit awkward right? Having the relationship functionality next to the regular references in the model feels a bit strange anyway, despite the nice functionality of visualizing relations.\nSo my question is, what are your\nprinciple\nconsiderations to implement a relation: like a reference or like a relationship type?”\nResponse\n“I clarified the behavior with the engineers and our general approach here is to keep the model as simple as possible. By default, we recommend using references unless there is a clear need for a more complex relationship structure from the start. Relationship types introduce additional complexity and can have performance implications, which is particularly important given our existing performance considerations.\nConverting a reference into a relationship type later is not trivial and requires significant system changes. Potential solutions include allowing visualization of references or simplifying the conversion process, but both require substantial development effort.\nFor now, we recommend using references unless visualization is a known requirement from the start.”\nHope this is of help.\nKind regards,\nAlbert"
    },
    "answers": [
      {
        "author": "Cansu",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@Albert de Ruiter\n, thank you for sharing, it’s 💯 useful for any member looking into the model architecture."
      }
    ]
  },
  {
    "title": "Add location feature",
    "url": "https://community.ataccama.com/data-quality-catalog-94/add-location-feature-1649",
    "question": {
      "author": "Varsha Kumari",
      "timestamp": "[No timestamp]",
      "content": "Hi all,\nCan anyone help me with what this\nAdd location\nfeature do which is available in\noverview tab\nof\nSource connection\n?"
    },
    "answers": [
      {
        "author": "Cansu",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@Varsha Kumari\n, thank you for posting! We have an in-depth documentation about it\nhere\n. It’s to identify where in the source your item is located.\nPlease let me know if this helps 🙋🏻‍♀️"
      }
    ]
  },
  {
    "title": "A guide to Data Quality validation: Understanding % threshold in regex-based detection checks",
    "url": "https://community.ataccama.com/data-quality-catalog-94/a-guide-to-data-quality-validation-understanding-threshold-in-regex-based-detection-checks-1661",
    "question": {
      "author": "anastasiia.popova",
      "timestamp": "[No timestamp]",
      "content": "Hi everyone,\nIn this post, I’ll walk you through regex-based detection checks. Understanding regex-based detection checks is crucial for ensuring data quality, validation, and anomaly detection. One common question that arises is about the % threshold when specifying such a check.\nWhat does % threshold mean in regex-based detection checks?\nWhen applying a regular expression (regex) check, the % threshold represents the proportion of data entries that must match the regex pattern in order for the check to pass.\nFor example, if you set the threshold to 70%, it means that at least 70% of the data elements must match the regex pattern for the detection check to be considered successful.\nCommon misconception: Matching data vs. matching regex\nA frequent point of confusion is whether the threshold means:\n70% of the data should match the regex\n(Correct)\n70% of the regex should match each data element\n(Incorrect)\nA regex pattern either matches or doesn’t for each data element - it does not partially match individual elements by percentage. Instead, the check considers the proportion of elements that fully match.\nExample: How % threshold works in regex validation\nLet’s assume you are applying a regex pattern check to 100 data entries:\nIf the threshold is 70%, at least 70 out of 100 entries must match the regex for the check to pass.\nIf only 65 entries match, the check fails.\nIf 85 entries match, the check passes.\nWhy is % threshold important for Data Quality?\nEnsuring a sufficient proportion of data matches a given pattern is vital for data validation, anomaly detection, compliance and data quality assurance. Setting an appropriate threshold helps balance false positives and false negatives, making your data validation process more accurate and reliable.\nBy implementing regex-based detection checks, organizations can maintain clean, structured, and trustworthy datasets for analytics, reporting, and decision-making.\nWhat are some challenges you’ve faced in regex-based detection checks? Let’s discuss in the comments below! 👇"
    },
    "answers": []
  },
  {
    "title": "Ataccama One Desktop - Select only distinct rows",
    "url": "https://community.ataccama.com/data-quality-catalog-94/ataccama-one-desktop-select-only-distinct-rows-324",
    "question": {
      "author": "Marnix Wisselaar",
      "timestamp": "[No timestamp]",
      "content": "Is there a way to quickly select only distinct rows in Ataccama One Desktop?\n(I now used a record descriptor and filter with Regex find(“.:.:1”, rd_column)"
    },
    "answers": [
      {
        "author": "DannyRyan",
        "timestamp": "2 years ago",
        "content": "Hi\n@Marnix Wisselaar\nUsing the Record Descriptor Builder step is certainly the best practice approach to solve this use case.\nAn alternative to using the Filter step with Regular Expression is to use the built-in expression function word().\nI have attached a quick example using the expression function word()."
      },
      {
        "author": "Marnix Wisselaar",
        "timestamp": "2 years ago",
        "content": "Thanks so much. Great how you documented it! Will certainly be added to our cookbook ;)"
      },
      {
        "author": "Radziah",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@DannyRyan\nI downloaded your plan and ran it but unfortunately I could not see the results as it turned to be blank for both columns. What would be the the input and expected results look like on  your end? As I have similar use case to replicate the select distinct values from Snowflake in Ataccama."
      },
      {
        "author": "DannyRyan",
        "timestamp": "2 months ago",
        "content": "Dear @Radziah,\nThank you for reaching out and for trying out the plan. I understand you're experiencing an issue with blank results in both columns, and I'm happy to help you troubleshoot this.\nTo better understand the situation, would you mind sharing a screenshot of your plan's configuration and the execution results? This will give me valuable context.\nThe original plan is designed to generate 100 random records, and it should function correctly. Given the randomized nature of the input data, it's worth noting that running the plan multiple times can produce varied datasets, which may affect the distinct values reflected in the output.\nIt's also possible that there might have been modifications to the plan, potentially impacting the flow of the 100 randomly generated records into the RecordDescriptorBuilder step. Sharing your plan file would allow us to review it together and pinpoint any potential discrepancies.\nPlease feel free to attach the screenshot and your plan file to your reply. I look forward to assisting you in resolving this issue."
      },
      {
        "author": "Radziah",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@DannyRyan\nThanks for prompt reply, I did not change anything in the plan and ran it as it is. Please find my attached plan and the screenshot of the output.\ni tried to change the 100 random records generation to 50 also gave me blank output."
      },
      {
        "author": "DannyRyan",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@Radziah\nI've made some tweaks to the plan and attached a new version for you. This one should now show both all the records and the filtered ones, which I hope will be much clearer!\nHere's a breakdown of what I've changed:\nReplacing the Filter step:\nI switched the regular \"Filter\" step to an \"Extract Filter\" step. This allows us to output both the full dataset and the filtered results within the same pipeline, making it easier to see what's happening.\nAdding a Text File Writer:\nI've included an extra \"Text File Writer\" step to capture all the records. This way, you'll have a complete view of the data.\nFixing the filter condition:\nThis was the key change! The original filter condition was preventing any results from showing up.\nLet's talk about the expression change in the Filter/Extract Filter step:\nOriginal:\nword(rd_value,2) = '1'\nUpdated:\nword(rd_value,1,':') = '1'\nHere's why:\nThe \"Record Descriptor\" has three values separated by colons (\":\").\nThe first value is the \"Group ID,\" the second is the \"Group Size,\" and the third is the \"Position\" within the \"Group ID.\"\nTo find distinct records, we need to look for records where the \"Group Size\" is '1' (meaning it's the only record of its kind).\nThe original expression was looking at the wrong index. We needed to target the \"Group Size\" (index 1), not the \"Position\" (index 2).\nAlso we needed to tell the word() function that the delimiter was a colon.\nSo, by changing the index to 1 and explicitly setting the delimiter to \":\", we're now correctly checking for records with a \"Group Size\" of '1'.\nI really hope this updated plan works well for you and helps you get a better grasp of:\nRecord Descriptors\nThe word() expression\nDebugging plans and expressions\nPlease don't hesitate to reach out if you have any more questions or need further clarification. I'm happy to help!\nBest regards,\nDanny"
      },
      {
        "author": "Radziah",
        "timestamp": "2 months ago",
        "content": "Thanks ​\n@DannyRyan\nI ran the latest plan couple of times only got the expected output which I would say only Community will be out in the output_distinct since the group size is 1? Based on the rd_value of 3:1:1.\nMy understanding of select distinct is let’s say we have generated 49 rows of ‘Data’, 50 rows of ‘People’ and 1 row of ‘Community’, isn’t it supposed to have three rows of output in total; 1 row ‘Data’, 1 row ‘People’ and 1 row ‘Community’? In your plan after I ran multiple times, the only output_distinct that I managed to get is like in the image below:"
      },
      {
        "author": "DannyRyan",
        "timestamp": "2 months ago",
        "content": "Dear ​\n@Radziah\n,\nThank you for your inquiry. It appears your specific requirement for extracting unique values from a dataset differs from the original post's context.\nTo achieve the desired outcome of displaying only unique values for each group within your data, you can utilize the following implementation.\nRecord descriptors, as previously mentioned, consist of three components separated by colons (:). These components are:\nRecord Group Identifier (Index 0):\nThis identifies the group to which a record belongs. In your case, it would represent the distinct categories (e.g., 'People', 'Data', 'Community').\nGroup Size (Index 1):\nThis indicates the total number of records within a specific group.\nPosition within Group (Index 2):\nThis denotes the ordinal position of a record within its group. For instance, if a group has three records, their positions would be 1, 2, and 3.\nTo select only one record per Group ID, we can apply a filter based on the position within the group. Specifically, we can filter for records where the position is equal to 1. This can be expressed as:\nword(rd_value, 2, ':') = '1'\nThis condition can be interpreted as follows:\nSplit the Record Descriptor string into three distinct words, using the colon (:) character as the delimiter.\nEvaluate the third word (index 2), which represents the Position within Group.\nIf the Position within Group is equal to '1', then retain the corresponding record.\nConsequently, the output will comprise all records with a Position within Group value of 1, effectively representing the first record from each distinct group.\nI hope this explanation is clear and helpful. Please do not hesitate to ask if you have any further questions.\nSincerely,\nDanny"
      },
      {
        "author": "Radziah",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@DannyRyan\nThank you! This time it works as expected as shown in the image below. Let’s say if I have more columns to read the distinct values, additional columns named column2 and column3. If i want to see the distinct values of all columns, I should add the column2 and column3 under src_value under Expressions in Record Descriptor Builder?"
      },
      {
        "author": "DannyRyan",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@Radziah\nYou're welcome! I'm glad to hear the previous solution addressed your needs.\nYes, you are absolutely correct. To obtain distinct values across multiple columns you should include those columns in the \"Partition By\" section of the Record Descriptor Builder. This effectively groups the records based on the combined values of all specified columns.\nThink of it as concatenating the values of those columns (column1, column2, column3, etc.) and then identifying the unique combinations.\nTo illustrate this, I've created an example using six \"food groups,\" with each group represented by a separate column in the \"Partition By\" section.\nIn the attached screenshot, you'll notice the following:\nRecord 1 has a \"group size\" of 1, indicating that the specific combination of values across all six food group columns is unique within the dataset.\nRecords 3 and 4, on the other hand, have a \"group size\" of 2. This signifies that these two records share identical values across all six food group columns.\nBy adding more columns to the \"Partition By\" section, you're essentially expanding the criteria for determining distinctness, allowing you to identify unique combinations across a broader set of attributes.\nI hope this clarifies the process.\nSincerely,\nDanny"
      }
    ]
  },
  {
    "title": "Post deployment plan filter",
    "url": "https://community.ataccama.com/data-quality-catalog-94/post-deployment-plan-filter-1660",
    "question": {
      "author": "Susan24us",
      "timestamp": "[No timestamp]",
      "content": "I applied two rules to a monitoring project, but I only want to export the results for one of the rules. In the post-deployment plan, I initially used a filter where the column is not null, and it worked fine. However, it exported all results where the column was not null, meaning the results of both rules were included. I would like to update the filter to only export specific invalid samples, but it's not working, as no data is being exported. Is this filter correct? Previously, it was \"Column Name IS NOT NULL,\" and I updated it to \"Column Name IS NOT NULL AND Column Name = 'XYZ specified rule’\nHow can i resolve this ?"
    },
    "answers": [
      {
        "author": "Albert de Ruiter",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@Susan24us\n,\nThe syntax of the filter looks ok, but if ‘Column Name’ applies to the same column then simply Column Name = 'XYZ specified rule’ would already suffice (it implies the column value being not null).\nWhat can help to check why the filter doesn't work, it to export the data to a txt ot csv file, just prior to the filter step. Then you can examine the data that will be processed by the filter and hopefully conclude why the filter doesn't work as expected.\nKind regards,\nAlbert"
      }
    ]
  },
  {
    "title": "Business Domain Analysis",
    "url": "https://community.ataccama.com/data-quality-catalog-94/business-domain-analysis-1644",
    "question": {
      "author": "sumisha",
      "timestamp": "[No timestamp]",
      "content": "How do we activate Business Domain Analysis option in Profiling section."
    },
    "answers": [
      {
        "author": "Lisa Kovalskaia",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@sumisha\n, here's how to enable Business Domain Analysis:\nOpen up the Profiling step, and click the button in the top right corner to switch to Advanced configuration.\nIn the tree on the left-hand side, expand the Inputs node, navigate to your input and then to the column of interest.\nIn the middle panel, mark the checkbox for Analyse Business Domain.\nThe Business Domains that the tool can identify are prebuilt into the code so there's no additional configuration, just switch it on and run the profiling plan.\nI hope this helps!"
      },
      {
        "author": "sumisha",
        "timestamp": "2 months ago",
        "content": "Thanks ​\n@Lisa Kovalskaia"
      }
    ]
  },
  {
    "title": "Export of DQ monitoring project results along with filters",
    "url": "https://community.ataccama.com/data-quality-catalog-94/export-of-dq-monitoring-project-results-along-with-filters-1653",
    "question": {
      "author": "manidhar",
      "timestamp": "[No timestamp]",
      "content": "Hi Ataccama community\nI am trying to do the export of DQ monitoring project the results and use the filters in the export. I a post on this one\nONE Desktop: Data Quality Results 🧑‍\non using the filters in the export data. I am able to get the filters extracted using the DQ monitoring project filter values but couldn't get the corresponding monitoring project results using the DQ Monitoring project check results. I am getting an empty file with just the column names in the file in the output file which I have checked the output into a CSV file just after the DQ monitoring project check results step. And I have enabled only one filter in the DQ monitoring project as I read in the community forum that two filter wouldn’t work with this setup.\nAre there any changes I need to make the way I am using the filter in DQ monitoring project check results step ? I am attaching the screenshots below for reference."
    },
    "answers": [
      {
        "author": "ChrisK",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@manidhar\n,\nI remember struggling with these steps for way too long as well. I don't know all you settings and mappings, but in one of my plans I've applied a Filter Step, right after getting the MP filter values, filtering out the #ALL# and null values. I can't remember if this was actually necessary, or I just didn't need these, but you could give it a try."
      }
    ]
  },
  {
    "title": "Unable to find the SFTP upload file step in OneDesktop",
    "url": "https://community.ataccama.com/data-quality-catalog-94/unable-to-find-the-sftp-upload-file-step-in-onedesktop-1652",
    "question": {
      "author": "manidhar",
      "timestamp": "[No timestamp]",
      "content": "Hi Ataccama Community, I want to upload a CSV file which is an export of DQ monitoring project results to SharePoint folder. I couldn’t see the\nTask SFTP upload file\nstep in my OneDesktop (version 15.4). Is this step not available in the current version of Ataccama OneDesktop ? Or is there any work around to upload the CSV files to the Share Point server.\nCheers\nManidhar"
    },
    "answers": [
      {
        "author": "manidhar",
        "timestamp": "2 months ago",
        "content": "Found it in the workflow of Ataccama One desktop."
      },
      {
        "author": "Cansu",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@manidhar\ngreat to hear that and thank you for sharing it for the other members who might look for it in the future!"
      }
    ]
  },
  {
    "title": "Connect with Collibra",
    "url": "https://community.ataccama.com/data-quality-catalog-94/connect-with-collibra-1643",
    "question": {
      "author": "Ajeesh_G",
      "timestamp": "[No timestamp]",
      "content": "Hi,\nI am attempting to connect to Collibra using the Collibra reader (refer to the page:\nCollibra Integration in Ataccama ONE Desktop 🏹\n).\nHowever, I am encountering the following error message when I execute the plan. Could someone please assist me?\nI have set up a server without a username/password because I don't need to enter a username/password when accessing Collibra."
    },
    "answers": [
      {
        "author": "Lisa Kovalskaia",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@Ajeesh_G\n- while this error typically points to a missing certificate issue, let us sort out authentication first. The reason you don't have to authenticate when accessing Collibra in your browser is likely because you're logged in automatically via company SSO. Could you please try to open the same endpoint that you're trying to reach from ONE desktop, in an incognito tab of the browser? If you get an error or a login page then your Collibra instance does expect requests to this URL to be authenticated.\nSSO login is not supported in ONE Desktop, and it's a best practice to have a dedicated system user for integration purposes, so I would recommend reaching out to your Collibra admin to adjust the security configuration. They would need to check that basic auth is allowed for your Collibra server, and create a user/password for the integration.\nPlease let me know how it goes!"
      },
      {
        "author": "Cansu",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@Ajeesh_G\n, I’m closing this thread for now. If you have any follow-up questions please don’t hesitate to share them below or create a new post 🙋🏻‍♀️"
      }
    ]
  },
  {
    "title": "Nested SQL Catalog Items",
    "url": "https://community.ataccama.com/data-quality-catalog-94/nested-sql-catalog-items-1650",
    "question": {
      "author": "kellymremzilyb",
      "timestamp": "[No timestamp]",
      "content": "Hello,\nCan we create nested sql catalog items.  Meaning, can I use a SQL catalog item in the query as a source to another sql catalog item?"
    },
    "answers": [
      {
        "author": "Albert de Ruiter",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@kellymremzilyb\n,\nWhen creating a SQL catalog item, to my knowledge you always refer to a physical data source. So to achieve what you describe, the following suggestions may be of help.\nIn the data source create a view that matches the logic of the intended nested catalog item (so a view that contains common table expressions or subqueries)\nDefine the nested logic in the SQL catalog item itself.\nKind regards,\nAlbert"
      }
    ]
  },
  {
    "title": "Creating array of values from multiple rows",
    "url": "https://community.ataccama.com/data-quality-catalog-94/creating-array-of-values-from-multiple-rows-1647",
    "question": {
      "author": "Albert de Ruiter",
      "timestamp": "[No timestamp]",
      "content": "Hi all,\nSuppose you have 2 tables that relate as 1:n. When joining the tables in a query without filtering, the result will be n rows.\nMy requirement is to have just one row returned, with the values of the n-table shown as an array. In SQL Server you could use a query like\nSELECT\np.PK_ONETEST,\np.Name,\np.Description,\n(SELECT\nc.Name\nFROM ODS.NTEST c\nWHERE c.FK_ONETEST = p.PK_ONETEST\nFOR JSON PATH) AS Children\nFROM ODS.ONETEST p\n;\nresulting in\nMy question is if we can also achieve this in one or more steps in One Desktop.\nKind regards,\nAlbert"
    },
    "answers": [
      {
        "author": "Adrian Anderson",
        "timestamp": "2 months ago",
        "content": "Hi Albert.\nGreat question! There are a few ways to do this. The Representative Creator and Group Aggregator steps can help here.\nAttached is a simple plan (see community_dq_1647.zip) that takes two text ‘tables’ as input, joins them and then creates the array of children.\nLet me know if this helps.\nKind regards,\nAdrian"
      },
      {
        "author": "Albert de Ruiter",
        "timestamp": "2 months ago",
        "content": "Hi Adrian,\nThis is indeed what I am looking for!\nSo both the Representative Creator and Group Aggregator steps prepare the data, grouping the join result, then the concatenate(If) function does in fact the actual creation of the array. Nice!\nThanks!!!\nKind regards,\nAlbert"
      }
    ]
  },
  {
    "title": "ONE Desktop: Data Quality Results 🧑‍🔬",
    "url": "https://community.ataccama.com/data-quality-catalog-94/one-desktop-data-quality-results-868",
    "question": {
      "author": "Cansu",
      "timestamp": "[No timestamp]",
      "content": "Hi Community!\nAre you looking to retrieve Data Quality (DQ) results from monitoring projects or catalogs using Ataccama's ONE Desktop? We have created a set of easy-to-follow steps to help you achieve this efficiently. These steps will mimic the API calls created for the same purpose and return DQ results. Let’s get into it 🔮\nPrerequisites\nBefore diving into the process, make sure you have the following prerequisites in place:\nActive Server Connection:\nEnsure you are connected to the ONE Web Application.\nInput Data:\nYou need the data for which you want to retrieve results. You can use either the appropriate\nInput\nstep or the\nMMM Reader step\n. You can add input data by dragging and dropping files onto the canvas or by configuring the steps manually. If multiple input steps are required, use the\nJoin\nstep and create connections accordingly.\nNote: Depending on the step you wish to use to retrieve results, different parameters will be required in the input. Refer to the individual step information for mandatory inputs.\nDQ Attribute Aggregation Results\nThis step returns results of certain DQ aggregations for attributes.\nIn your\nInput\nstep, ensure you have mappings to ONE Web Application for all required parameters and optional parameters as desired:\nExample: To map the new step to catalog ID information, you can configure the\nMMM Reader\nwith\nEntity Type:\nCatalog Item\nand\nId Column Name:\ncatalogItemId\n.\nAttribute ID (mandatory):\nList of attribute IDs, each with an optional list of aggregation IDs (aggregation IDs can be, for example, the ID of a dimension).\nCatalog Item ID (mandatory):\nCatalog item ID.\nLimit and time ranges: Optionally define the number of runs to be returned, as well as a start and end date for the time range.\nAdd the\nDQ Attribute Aggregation Results\nstep to the canvas.\nCreate a connection to\nDQ Attribute Aggregation Results\nfrom the input step (or from the join step if multiple inputs are used).\nDouble-click on the\nDQ Attribute Aggregation Results\nstep and navigate to the\nProperties\ntab:\nAdd the IDs of the corresponding columns from the input in the available fields:\nAttribute Id (mandatory):\nColumn from previous steps in the flow mapped to the attribute ID information.\nCatalog Item Id (mandatory):\nColumn from previous steps in the flow mapped to the catalog item ID information.\nAggregation Id:\nColumn from previous steps in the flow mapped to the aggregation ID information.\nLimit:\nColumn from previous steps in the flow mapped to limit definition.\nTime Range From:\nColumn from previous steps in the flow mapped to the start date of the specified time range.\nTime Range To:\nColumn from previous steps in the flow mapped to the end date of the specified time range.\nAlternatively, select\nCreate...\nto create a shadow column in the previous step in the flow and map this field to that column.\n(Optional) Define shadow columns to create a new column of a specific type in the output format. This column can contain initial data as a result of a\nDefault Expression\n.\nThis step has two outputs by default: partition information and results. You can remove connections as needed if you don't require a particular output.\nDQ Monitoring Project Aggregation Results\nRetrieve aggregations for monitoring projects using this step:\nConfigure the Input step, mapping Monitoring Project ID, Catalog Item ID, Aggregation ID, and time ranges.\nAdd the DQ Monitoring Project Aggregation Results step.\nEstablish a connection between the Input step and DQ Monitoring Project Aggregation Results.\nSet properties for the DQ Monitoring Project Aggregation Results step, mapping relevant columns.\nOptionally, define shadow columns for customized output formats.\nOptionally, define attribute filters to further refine your results.\nThis step offers various outputs by default: alerts, explanations of invalidity, and results. Customize your outputs as needed.\nDQ Monitoring Project Check Results\nThis step returns results of certain aggregations (custom, catalog item level, attribute level).\nIn your\nInput\nstep make sure you have mappings to ONE Web Application for all required parameters, and optional parameters as desired:\nMonitoring Project Id (mandatory):\nmonitoring project IDs.\nCatalog Item ID (mandatory)\n: list of catalog item IDs (not the id catalog item instance).\nDQ Check ID (mandatory)\n: list of DQ check IDs.\nLimit and time ranges: optionally define the number of runs which should be returned and a start and end date of time range.\nAdd\nDQ Monitoring Project Check Results\nstep to the canvas.\nAdd an connection to\nD\nQ Monitoring Project Check Results\nfrom the input step (or from the join step if multiple inputs are used).\nDouble-click on the\nDQ Monitoring Project Check Results\nstep and  in the\nProperties\ntab:\nAdd the IDs of the corresponding columns from the input in the fields available:\nMonitoring Project Id (mandatory):\ncolumn from previous steps in flow mapped to the monitoring project ID information.\nCatalog Item Id (mandatory)\n: column from previous steps in flow mapped to the catalog item ID information.\nDQ Check Id\n: column from previous steps in flow which is mapped to DQ check ID information.\nLimit:\ncolumn from previous steps in flow which is mapped to limit definition.\nTime Range From\n: column from previous steps in flow which is mapped to start date of specified time range.\nTime Range To\n: column from previous steps in flow which is mapped to end date of specified time range.\nAlternatively, select\nCreate...\nto create a shadow column in the previous step in the flow and map this field to that column.\n(Optional) Define shadow columns to define a new column of a specific type in the output format. The created column can contain initial data as a result of\nDefault Expression\n.\n(Optional) Define attribute filters to filter project results and project aggregation results. To do this:\nSelect the\nFilters\ntab and create a new configuration\nIn\nAttribute Id\n, provide the ID of the attribute you would like to be able to filter by.\nIn\nFilter Value\n, add the attribute values you would like to be able to select in the filter. You can only add values which are present in the selected attribute.\nWhen adding multiple values, the separator is comma by default. This can be changes by changing the entry in\nValue Separator\n.\nSelect\nAdd\nto add another attribute filter and repeat steps 6a-6,c or select\nOK\nif you are done.\nThis step has three outputs by default: alerts, explanations of invalidity, and results. Remove connections as required if you do not want a particular output to be created.\nDQ Monitoring Project Filter Values\nRetrieve data values of filter attributes using this step:\nConfigure the Input step, mapping Monitoring Project Processing ID and Catalog Item ID.\nAdd the DQ Monitoring Project Filter Values step.\nEstablish a connection between the Input step and DQ Monitoring Project Filter Values.\nSet properties for the DQ Monitoring Project Filter Values step, mapping relevant columns.\nOptionally, define shadow columns for customized output formats.\nDQ Monitoring Project Results\nRetrieve project validity results over time with this step:\nConfigure the Input step, mapping Monitoring Project ID and time ranges.\nAdd the DQ Monitoring Project Results step.\nEstablish a connection between the Input step and DQ Monitoring Project Results.\nSet properties for the DQ Monitoring Project Results step, mapping relevant columns.\nOptionally, define shadow columns for customized output formats.\nThis step provides three outputs by default: alerts, explanations of invalidity, and results. Customize your outputs based on your requirements.\nOptionally, define shadow columns for customized output formats.\nOptionally, define attribute filters to refine your results.\nBy following these best practices, you can efficiently retrieve DQ results for attribute aggregations in Ataccama's ONE Desktop.\nStay tuned for more best practice posts for other DQ results retrieval steps!"
    },
    "answers": [
      {
        "author": "Siva_Madhavan",
        "timestamp": "1 year ago",
        "content": "Hello Cansu, Thanks for this write-up. Very useful & informative. Today we are achieving the same functionality by joining multiple Metadata Reader objects.\nOne Feedback: If you could provide Screenshot of any one Sample Monitoring Project from One-Web and the output of this Metadata Readers would help us visualize and see the results."
      },
      {
        "author": "aysel_jafarzade",
        "timestamp": "1 year ago",
        "content": "What is considered as Aggregation ID in DQ Monitoring Project Aggregation Results?"
      },
      {
        "author": "Cansu",
        "timestamp": "1 year ago",
        "content": "Hi\n@aysel_jafarzade\n, thanks for posting\nAggregation Id\ncan be considered as an ID of a\nDQ Dimension\n. Some examples are:\nName\n: The name of the DQ dimension.\nOverall contribution:\nIndicates whether results from this dimension are contributing to\nOverall Quality\nor not.\nActive:\nIndicates whether this dimension can be selected or not during rule creation.\nOrder:\nThe order in which the dimensions are checked during DQ Evaluation.\nPlease let me know if this helps 🙋🏻‍♀️"
      },
      {
        "author": "ViktorSzucs",
        "timestamp": "1 year ago",
        "content": "Hello\n@Cansu\nI am playing around with some plans regarding the above steps and just got stuck at one point. My Plan looks like so and deliberately just feed the\nDQ MP Filter Values Reader\nwith one\nMonitoring Project\none\nProcessing\nand the plan works perfectly as long as there is only one filter attribute.\nThe output of debug1 looks like so with the filter values: one CatalogItem, three filter attributes and multiple values\nThe\nDQ Monitoring Project check Results\nStep look like so:\nAll this is working properly as long as there is only one filter attribute per catalog item, but once multiple filters come in play, the results in debug2 no longer seem to be in line with Ataccama ONE Web even applying the same filtering there.\nAny advice for filtering with multiple filter values is welcome.\nThanks, Viktor"
      },
      {
        "author": "ViktorSzucs",
        "timestamp": "1 year ago",
        "content": "The short answer to the above is that while multiple filters can be set up on the Web UI and they work, the step in the One Desktop is only able to read one filter attribute. This also means that once multiple filters are set up, the step doesn’t return anything, so we can only use one filter in web and desktop."
      },
      {
        "author": "Cansu",
        "timestamp": "1 year ago",
        "content": "Hi\n@ViktorSzucs\nthank you for coming back to the thread and sharing the answer 🙌"
      },
      {
        "author": "manidhar",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@Cansu\nAND ​\n@ViktorSzucs\n, I am trying to use the DQ Monitoring Project Filter Values as shown in the above comment by Viktor. I am able to get the filters extracted using the DQ monitoring project filter values but couldn't get the corresponding monitoring project results using the DQ Monitoring project check results. I am getting an empty file with just the column names in the file in the output file which I have checked just after the DQ monitoring project check results step. And I have enabled only one filter in the DQ monitoring project as you said two filter wouldn’t work with this setup.\nAre there any changes I need to make the way I am using the filter in DQ monitoring project check results step ? I am attaching the screenshots below for reference.\nThanks\nManidhar"
      }
    ]
  },
  {
    "title": "How to Implement Logic in Ataccama to Process One Record and Flag Another as Error for Duplicate Correlation ID and Version Number",
    "url": "https://community.ataccama.com/data-quality-catalog-94/how-to-implement-logic-in-ataccama-to-process-one-record-and-flag-another-as-error-for-duplicate-correlation-id-and-version-number-1379",
    "question": {
      "author": "manikanta v",
      "timestamp": "[No timestamp]",
      "content": "Two records as same correation id and same version number one should process another one should error out how to implement this logic in ataccama"
    },
    "answers": [
      {
        "author": "Samuel Muvdi",
        "timestamp": "8 months ago",
        "content": "Hi! On ONE app you can create uniqueness rules, these uniqueness rules will point out records that failed due to them having some duplicated ID. Since you mention it will have an ID and a VERSION, I would use an aggregate rule, that way its groups by ID and VERSION so if there's a duplicate of ID + VERSION it will show the failed records. Note, it will show for all duplicates, including the first instance of this tuple combination"
      },
      {
        "author": "manikanta v",
        "timestamp": "8 months ago",
        "content": "Can you provide some steps on how to configure in Ataccama."
      },
      {
        "author": "Samuel Muvdi",
        "timestamp": "8 months ago",
        "content": "Hi\n@manikanta v\n!\nWhat you would need to do is:\ncreate a DQ evaluation rule on one app\ngo inside the dq evaluation rule and open implementation\nonce inside implementation, for DQ evaluation rule click “uniqueness”\nthen you will add in 2 inputs “ID” and “versionNum” in your use case\nNext for rule logic you will put “aggregation rule”\nthen, for group by, make sure to put ID and versionNum inputs you created above\nthen for the NOT_UNIQUE box, you will put ID, versionNum is not unique\nfor result you will put not unique and for explanation as well\nOnce this is completed, you can test it using the test rule button towards the top, you can use the sample image I put in the previous post as a guideline for how it should look\nHope this helps :)"
      },
      {
        "author": "manikanta v",
        "timestamp": "8 months ago",
        "content": "Hi\n@Samuel Muvdi\n, Could you please explain how to do this in one desktop."
      },
      {
        "author": "Samuel Muvdi",
        "timestamp": "8 months ago",
        "content": "Hi\n@manikanta v\n!\nIf you would like to do something similar to this on one desktop, I reccomend you use a record descriptor builder.\nYou will have your data file being read in, lets say it will have ID column and versionID column.\nWhen you add in the record descriptor builder, you will make the expression be ID+versionID (this makes it so if both ID and version ID are the same, it is a duplicate), what you will then do is add that descriptor created into a new column called descriptor or whatever you would like to call it. That descriptor column will hold 3 numbers per cell, the first number is just like an ID number for that group (ID+versionID), the second number is the group size, so if theres duplicates with the same ID and versionID the group size will be higher than 1. The last number we can consider it to be like the specific instance in that group. So it will look like this:\nID\nversionID\nDescriptor\nDuplicate\n1\n1\n1:1:1\nFALSE\n2\n1\n2:2:1\nFALSE\n2\n1\n2:2:2\nTRUE\nWith some additional logic additions we will make, we will get it to say that the 3rd record there is a duplicate\nOnce your record descriptor configuration looks something like this:\nWe can now do the final part which is configure the “Duplicate” boolean column. The duplicate column should only display if a record is a duplicate(TRUE) if the rightmost number is larger than 1, and display as unique(FALSE) if the rightmost number is 1. To do this, we will use the lastIndexOf() function, which will look for the last Index of  “:” which is what separates the 3 record descriptor numbers. This is the expression:\niif(toInteger(substr(descriptor, lastIndexOf(descriptor, ':')+1)) = 1, FALSE, TRUE)\nIts basically checking if that last digit is equal to 1 (the first instance of this ID + versionID) or not, meaning if not then its a duplicate. You will set the “duplicate” boolean column to be equal to that equation. This is a sample of how the final result would look like:\nIf you have any more questions feel free to ask! :)"
      },
      {
        "author": "manikanta v",
        "timestamp": "8 months ago",
        "content": "Hi\n@Samuel Muvdi\n,  The solution was very helpful and resolved the issue efficiently. Thank you."
      },
      {
        "author": "Samuel Muvdi",
        "timestamp": "8 months ago",
        "content": "Thats great! Im glad to be of help :)"
      },
      {
        "author": "manikanta v",
        "timestamp": "8 months ago",
        "content": "Hi\n@Samuel Muvdi\n, \"In Ataccama One Desktop, when using batch mode with AWS SQS Standard Queue-based ingestion, the version number of the incoming second record should be greater than that of the first record. If the version number is less than the existing one, an error should be triggered. Could you please explain how to build this logic in Ataccama One Desktop?\""
      },
      {
        "author": "Samuel Muvdi",
        "timestamp": "8 months ago",
        "content": "Hi, in the case of streaming it will be a bit different. Since streaming is usually 1 by 1, and in this case lets say its batches of 10,000 streamed records per batch, there could be chances that the duplicate record can come in a couple of batches down the line and it wont be picked up as the record descriptors will be made unique per batch. The best workaround I would say is to send the batches to a database table that has a compound key constraint on ID+VersionID so that the jdbc writer will throw the error regarding “duplicate key constraint”"
      },
      {
        "author": "manikanta v",
        "timestamp": "6 months ago",
        "content": "Hi\n@Samuel Muvdi\n,\nCould you help me with how to implement logic in Atacama ONE Desktop to include '+' signs, numbers, space, 'X', ‘x’ and '-' in phone numbers while excluding all other special characters and letters? The ready-made clean component does not support this."
      },
      {
        "author": "Samuel Muvdi",
        "timestamp": "6 months ago",
        "content": "Hi\n@manikanta v\n! Sorry for the late reply, to accomplish the task you are describing, I would use the matching values step. Inside the matching value step. Inside this step, there is a generator which allows you do select various operations to be performed on the columns of your choice, including:\nremove accents\nremove repeated chars\nsqueeze whitespaces\nuppercase\nsubstitutions\nsupported characters\nFor your specific task, I recommend using the supported characters box to fill out which characters will be supported in your use case. For your specific use case i recommend doing [:digit:][:white:]-xX+\nThis will ensure only digits, white space (space) - x X or + are characters showing in your phone columns.\nI will attach below an example of how this works:\nOriginal Sample Data\nPhone numbers cleansed, only containing accepted chars\nHope this helps!"
      },
      {
        "author": "manikanta v",
        "timestamp": "6 months ago",
        "content": "Hi\n@Samuel Muvdi\n, Sorry for the late reply, thank you very much for your prompt response and for providing the solution."
      },
      {
        "author": "Susan24us",
        "timestamp": "2 months ago",
        "content": "In a situation where you want to check multiple columns within the same field, such as verifying if the status for employees remains the same across different qtr of the year , how do i approach that ​\n@Samuel Muvdi"
      }
    ]
  },
  {
    "title": "DATA STORIES",
    "url": "https://community.ataccama.com/data-quality-catalog-94/data-stories-1630",
    "question": {
      "author": "Susan24us",
      "timestamp": "[No timestamp]",
      "content": "Hello, i am using a free version of the Data Stories , I cant seems to find where to upload or import dataset as seen from the demo ---\nData Stories Demo\ndoes this free version not support creating a story , Dashboard or visualization ? if its does, is there something i’m not doing right"
    },
    "answers": [
      {
        "author": "Lisa Kovalskaia",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@Susan24us\n- I see that demo dates back to an early version of Data Stories, I'll flag it to our website team.\nFor the moment, our\ndocumentation\nis a better resource.\nCurrently there's no way to import a local file directly to Data Stories. What you can do is create a data visualization out of a catalog item, so please have a look at the page above for the list of supported data sources and how to configure one to integrate with Data Stories. Then just make your dataset available in that data source and you can\ncreate a visualization\n.\nHope this helps! Please let me know if you have additional questions."
      },
      {
        "author": "Cansu",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@Susan24us\nI’m closing this thread for now, if you have any questions please feel free to share them in the comments or create a new post 🙋🏻‍♀️"
      }
    ]
  },
  {
    "title": "Nested JSON with DAT file extension processing in Ataccama ONE",
    "url": "https://community.ataccama.com/data-quality-catalog-94/nested-json-with-dat-file-extension-processing-in-ataccama-one-1622",
    "question": {
      "author": "Data_Vault",
      "timestamp": "[No timestamp]",
      "content": "Hello,\nWe recently started getting nested JSON event files with DAT file extension. We are getting an ‘Unknow Error’, when we try to import it.\nCan you help me in understanding, how nested JSON with .DAT extension can be processed in Ataccama.\nThank you."
    },
    "answers": [
      {
        "author": "Lisa Kovalskaia",
        "timestamp": "3 months ago",
        "content": "Hi ​\n@Data_Vault\n,  I would set up a\nvirtual catalog item\nwith the JSON Reader step as an input configured to parse your specific file structure. The error could be related either to the parser configuration or another part of the ONE Desktop plan behind the VCI - I see you've already opened a support ticket on this so I'll let the support team take it forward with troubleshooting."
      },
      {
        "author": "Cansu",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@Data_Vault\nI’m closing this thread for now, if you have any follow up questions please don’t hesitate to share them in the comments or create a new post 🙋🏻‍♀️"
      }
    ]
  },
  {
    "title": "Multiple workflows in one scheduler",
    "url": "https://community.ataccama.com/data-quality-catalog-94/multiple-workflows-in-one-scheduler-1648",
    "question": {
      "author": "Ajeesh_G",
      "timestamp": "[No timestamp]",
      "content": "Hi\nCan we have multiple workflows in single scheduler? I tried to add multiple workflows but I am getting error. is there any way to do this?\nThanks in advance"
    },
    "answers": [
      {
        "author": "AKislyakov",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@Ajeesh_G\n,\nThis is not supported. To use the same schedule for multiple workflows, create an orchestrating workflow that triggers individual workflows and schedule the orchestrating workflow instead."
      }
    ]
  },
  {
    "title": "Connections Update",
    "url": "https://community.ataccama.com/data-quality-catalog-94/connections-update-1625",
    "question": {
      "author": "Susan24us",
      "timestamp": "[No timestamp]",
      "content": "We recently updated the Data Sources connection, and all Catalog items that were using the old connection are no longer working in the monitoring projects. When I expand the catalog items, an error message appears:\n\"You are looking at a deleted item.\"\nHowever, if I create a new catalog item and add it as an attribute in the monitoring project, it works perfectly fine.\nMy question is:\nHow can I update the existing catalog items to use the new connection?\nDo I need to manually recreate all these catalog items with the new connection, or is there a way to automatically switch all catalog items from the deleted connection to the new one"
    },
    "answers": [
      {
        "author": "anna.spakova",
        "timestamp": "3 months ago",
        "content": "Hi ​\n@Susan24us\n,\nif it’s just about updating the connection, you need to use the API to update the reference to the new connection for all relevant catalog items. You can use the ONE Desktop and ONE Metadata Writer step:\nThe new_connection_id has to store the GID of the new connection (when you open the connection, the ID is in the URL).\nBut please note, the origin path for the tables have to still correspond, which means the names of the database and schemas must still be the same, otherwise the items will still fail.\nAre you sure you deleted just the connection and not any location? The error message suggests that the catalog items were actually deleted.. This might require deeper analysis and if that’s the case, I would kindly ask you to report it to our support team and they can assist you further.\nKind regards,\nAnna"
      },
      {
        "author": "Cansu",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@Susan24us\ndoes the solution from ​\n@anna.spakova\nabove help? If yes, could you please mark it as a best answer? 🙌"
      }
    ]
  },
  {
    "title": "How to export a Summary of DQ Monitoring Project results?",
    "url": "https://community.ataccama.com/data-quality-catalog-94/how-to-export-a-summary-of-dq-monitoring-project-results-1628",
    "question": {
      "author": "manidhar",
      "timestamp": "[No timestamp]",
      "content": "Hello Ataccama Community,\nI am trying to export the summary statistics of the DQ monitoring project?   I wanted to have a csv file with the columns like Date of the test results execution, Data_Catalog_Name, Attribute Name, DQ rule name, Data Source, DQ dimension, Total Number of Records, Number of records passed, and Number of records failed.\nTo begin with I have tried using DQ monitoring projects step in One Desktop and the DQ Monitoring project is failing if I enable this post processing plan. Not sure why it was failing after enabling the below post processing plan. Any help is much appreciated.\nThanks in Advance"
    },
    "answers": [
      {
        "author": "C_R",
        "timestamp": "3 months ago",
        "content": "Hi ​\n@manidhar\nCheck out this best practice post, maybe it can help with your use case. Good luck!\nExport Monitoring Project Metedata + Results For Multiple Runs And Versions"
      },
      {
        "author": "Cansu",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@manidhar\n, I’m closing this thread for now, if you have any follow up questions please share them in the comments or create a new post 🙋🏻‍♀️"
      }
    ]
  },
  {
    "title": "Boolean DQ",
    "url": "https://community.ataccama.com/data-quality-catalog-94/boolean-dq-1623",
    "question": {
      "author": "Nataliia",
      "timestamp": "[No timestamp]",
      "content": "Hi guys, how did you check boolean date type in DQ rules? When I apply this rules  it  also make numbers, string, others value as correct."
    },
    "answers": [
      {
        "author": "rafeeq.durowoju",
        "timestamp": "3 months ago",
        "content": "Hey ​\n@Nataliia\n,\nRegarding your use case, since your input variable has to be type boolean, it would mean your rule can only be applied to a field that is recognized as boolean in Ataccama. Towards that end the field wouldn’t have string values or numbers other than potentially 0 and 1.\nHope that makes sense!\nRegards,\nRafeeq"
      },
      {
        "author": "Cansu",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@Nataliia\n, I’m closing this thread for now. If you have any questions please don’t hesitate to share in the comments or create a new post here🙋🏻‍♀️"
      }
    ]
  },
  {
    "title": "Creating Source in Ataccama One  for Snowflake",
    "url": "https://community.ataccama.com/data-quality-catalog-94/creating-source-in-ataccama-one-for-snowflake-1605",
    "question": {
      "author": "jaygsk",
      "timestamp": "[No timestamp]",
      "content": "I am creating new source connection from Ataccama ONE and I need help on how to connect Snowflake.\nFrom below screenshot I am trying to understand JDBC connection looks like.\nFor e.g. I am using\njdbc:snowflake://myorganization-myaccount.snowflakecomputing.com/\n, but it is not working.\nAny help is greatly appreciated."
    },
    "answers": [
      {
        "author": "Adrian Anderson",
        "timestamp": "3 months ago",
        "content": "Hi ​\n@jaygsk\n.\nThe Snowflake JDBC documentation might be useful here:\nhttps://docs.snowflake.com/en/developer-guide/jdbc/jdbc-configure#label-jdbc-connection-string\nPlease note the section on Connection Parameters and the following detailed descriptions:\nhttps://docs.snowflake.com/en/developer-guide/jdbc/jdbc-parameters\nTypically, we would want to specify the\nwarehouse\n,\ndb\n,\nschema and/or\nrole\nparameters, e.g.:\njdbc:snowflake:\n//myorganization-myaccount.snowflakecomputing.com/?warehouse=mywh&db=mydb&schema=public&role=myrole\nPlease let us know if this helps.\nKind regards,\nAdrian"
      },
      {
        "author": "Cansu",
        "timestamp": "3 months ago",
        "content": "Hi ​\n@jaygsk\n! Did ​\n@Adrian Anderson\n’s solution help with your question? If yes, could you please kindly mark it best answer and don’t hesitate to let us know if you have any follow up questions in the comments!"
      },
      {
        "author": "jaygsk",
        "timestamp": "2 months ago",
        "content": "Hello Adrian:\nIssue is not resolved; I am working with Ataccama support team on the same. we can close this ticket."
      }
    ]
  },
  {
    "title": "inserting data to Bigquery table using Ataccama ONE Desktop",
    "url": "https://community.ataccama.com/data-quality-catalog-94/inserting-data-to-bigquery-table-using-ataccama-one-desktop-1642",
    "question": {
      "author": "Ankur Sinha",
      "timestamp": "[No timestamp]",
      "content": "Hi,\nI am able tot connect Bigquery from Ataccama ONE Desktop application but getting below error while inserting data in table -\ncom.ataccama.dqc.commons.sql.atc.WrappedSqlException: [Simba][BigQueryJDBCDriver](100032) Error executing query job. Message: Transaction control statements are supported only in scripts or sessions\nwhile executing INSERT INTO …….."
    },
    "answers": []
  },
  {
    "title": "How to get terms tagged to attributes in Catalog in One desktop",
    "url": "https://community.ataccama.com/data-quality-catalog-94/how-to-get-terms-tagged-to-attributes-in-catalog-in-one-desktop-1636",
    "question": {
      "author": "sumisha",
      "timestamp": "[No timestamp]",
      "content": "How do I get Terms tagged to attributes to various catalog items in One Desktop?\nI need to get all catalog items which are tagged with a specific Glossary Term."
    },
    "answers": [
      {
        "author": "Albert de Ruiter",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@sumisha\n,\nIn Desktop when you define a metadata reader for ‘attribute’ (so the catalog item attribute) you can select the desires properties when clicking ‘Map to entity’. Here you can select ‘termInstances’ which can further be defined as embedded entity stream. displayName will give you the related term.\nWhen defing\nattributeID\nas Id Column Name for attribute, and as Parent Id Column Name for the embedded stream, you can join on these properties in a following step. It might also be good to filter on a specific catalog item, incase you have many catalog items. In the filter tab you can define something like $parent.$id = '74543024-0000-7000-0000-0000abcdefgh'.\nKind regards,\nAlbert"
      },
      {
        "author": "sumisha",
        "timestamp": "2 months ago",
        "content": "Thanks Albert!"
      }
    ]
  },
  {
    "title": "One desktop trans comoponent issue",
    "url": "https://community.ataccama.com/data-quality-catalog-94/one-desktop-trans-comoponent-issue-1541",
    "question": {
      "author": "Thanuja",
      "timestamp": "[No timestamp]",
      "content": "When I try to open a Trans component on Ataccama One Desktop, I receive the error 'Index -1078132736 out of bounds for length 10.' Do you have any ideas or suggestions to resolve this issue?"
    },
    "answers": [
      {
        "author": "Cansu",
        "timestamp": "3 months ago",
        "content": "Hi ​\n@Thanuja\n, apologies for the late reply here! Could you please create a support ticket on this? Our team will assist you as soon as possible."
      },
      {
        "author": "Thanuja",
        "timestamp": "2 months ago",
        "content": "sure, Thank you!"
      }
    ]
  },
  {
    "title": "Audit log configuration in ataccama one desktop for both backend/front end",
    "url": "https://community.ataccama.com/data-quality-catalog-94/audit-log-configuration-in-ataccama-one-desktop-for-both-backend-front-end-1639",
    "question": {
      "author": "Thanuja",
      "timestamp": "[No timestamp]",
      "content": "Hi everyone, I was wondering if anyone could share how the\ni_event_log\ntable is configured, as well as its details for the front-end (UI)? Also, could anyone provide guidance on how to configure the back-end audit log and retrieve it using pgAdmin? I’d really appreciate any insights. Thank you!"
    },
    "answers": []
  },
  {
    "title": "Flow of columns after using group Aggregator",
    "url": "https://community.ataccama.com/data-quality-catalog-94/flow-of-columns-after-using-group-aggregator-434",
    "question": {
      "author": "Abby",
      "timestamp": "[No timestamp]",
      "content": "Hello All,\nI am using group aggregator in our plan but I am not sure how to make all the columns flow from input till the end/output through group aggregator."
    },
    "answers": [
      {
        "author": "alexAguilarMx",
        "timestamp": "2 years ago",
        "content": "hi\n@Abby\nThe\ngroup aggregator\nhas two output endpoints.\nout_results\nendpoint contains the result of your\ngroup aggregator\nstep\nout\nsends the input records to subsequent steps.\nYou can use a\njoin\nstep after the group aggregator, it joins two separate data flows into a single data flow, this is the way to keep all columns"
      },
      {
        "author": "Radziah",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@alexAguilarMx\nbased on your suggestion, I applied the same to my use case but I found out that the left and the right join produces the same column names. For example, SUPPLY_CREATE_DATE that is created in one of the aggregate sets appear twice both on left and right after the join, so I have left.SUPPLY_CREATE_DATE and right.SUPPLY_CREATE_DATE. Ideally, after the join, I should take the right side for the aggregation since I would expect the column to come from the out_results only?\nOn a side not, I have several aggregator sets which each set has condition on the ‘when’ configuration."
      }
    ]
  },
  {
    "title": "Creating rule for productline",
    "url": "https://community.ataccama.com/data-quality-catalog-94/creating-rule-for-productline-1629",
    "question": {
      "author": "shikoh.z",
      "timestamp": "[No timestamp]",
      "content": "Hi,\nCould someone guide me on creating rules for the\nProductline\nfield? I am following the lab exercise but am unable to find the option for selecting\n\"is not from reference data.\"\nAny guidance would be appreciated.\nThanks,\nShikoh Zaidi"
    },
    "answers": [
      {
        "author": "Lisa Kovalskaia",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@shikoh.z\n, looks like there's a mismatch between the current Ataccama version UI and our training materials - thank you for the catch! We will update the labs.\nReference data could be stored either in a lookup file or a catalog item. If you want the rule to validate data against a lookup file that you've built earlier, then you're looking for the option \"is not from lookup”. If you want to run validation against a column in a catalog item, then select \"is not from catalog item”. Hope this helps!"
      },
      {
        "author": "DannyRyan",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@shikoh.z\n,\nThanks for bringing your question to the community! We've looked into your query about the 'Productline' field rules.\nIt seems you're currently working through the 14.5 training materials while accessing a 15.4 LTS environment. This difference in versions explains why you're seeing \"is not from lookup\" instead of \"is not from reference data.\"\nTo ensure a smoother learning experience, we'd recommend aligning your training materials with the product version of your environment. Using 15.4 training resources on a 15.4 environment will eliminate any confusion caused by version discrepancies.\nWe hope this helps! If you have any further questions, please don't hesitate to ask. We're here to assist you.\nBest regards\nDanny"
      }
    ]
  },
  {
    "title": "Combining Excel file",
    "url": "https://community.ataccama.com/data-quality-catalog-94/combining-excel-file-1631",
    "question": {
      "author": "SoniyaN",
      "timestamp": "[No timestamp]",
      "content": "I have 10 excel sheets, which need to be combined in one single excel sheet. can this be done in Ataccama?"
    },
    "answers": [
      {
        "author": "Lisa Kovalskaia",
        "timestamp": "2 months ago",
        "content": "Hi ​\n@SoniyaN\n- sure, if your files are stored in a data source connected to your Ataccama ONE Web App, then you can import them to the Catalog and then use a\ntransformation plan\nto join or union the data. If your excel files are only available locally, then you can do that in a ONE Desktop plan."
      }
    ]
  },
  {
    "title": "Data Protection classification",
    "url": "https://community.ataccama.com/data-quality-catalog-94/data-protection-classification-1609",
    "question": {
      "author": "sumisha",
      "timestamp": "[No timestamp]",
      "content": "I have a column in my catalog item which contains sensitive data. I have tagged this column with a Business Term. This business term is tagged as PII in Data Protection Classification section in Glossary terms. Is there a way to show at catalog item level that this table contains PII data?"
    },
    "answers": [
      {
        "author": "ekaterina.ponomareva",
        "timestamp": "3 months ago",
        "content": "Hi Sumisha,\nThank you for your question!\nCurrently there is only an option to see it on the attribute level.\nI will put a request for our product team to consider adding this.\nCould you please give the examples of situations where you would need such visibility? Like this our product team will understand better how to improve this feature.\nHowever, you have the option to have a PII term detected on a CI level. It can be done in Term settings via ‘Detection on tables’ tab.\nHope this will be helpful to you.\nPlease let me know if you need further information.\nKind regards,\nEkaterina"
      },
      {
        "author": "sumisha",
        "timestamp": "3 months ago",
        "content": "Thanks for your reply Ekaterina. I started with creating Data Protection Classification here instead of as a rule\nAnd then added to Data Protection Classification section under Term -\nI know this is used to hide data from users who don’t have right privileges. But is there a way to show this classification in Catalog Item either at column or table level?"
      },
      {
        "author": "ekaterina.ponomareva",
        "timestamp": "3 months ago",
        "content": "Hi Sumisha,\nYes, it is possible to see it on a column level.\nData associated with a classified term\nis hidden from unauthorized users and groups in all relevant contexts. Classified data is marked with a lock icon.\nPlease see the difference how it is showed for authorized and unauthorized users.\nKind regards,\nEkaterina"
      },
      {
        "author": "sumisha",
        "timestamp": "3 months ago",
        "content": "Thanks ​\n@ekaterina.ponomareva\n. Is it possible to see the Classification type tagged here along with lock symbol? Whether it is PII/PCI etc based on configuration done?"
      },
      {
        "author": "ekaterina.ponomareva",
        "timestamp": "3 months ago",
        "content": "Hi Sumisha,\nYou will see on the attribute level the Term which is applied to this attribute. If you click on it, it will be seen on the term, which specific data protection classification was applied. Please, see an example of how it looks like.\nKind regards,\nEkaterina"
      },
      {
        "author": "sumisha",
        "timestamp": "3 months ago",
        "content": "Thanks Ekaterina"
      }
    ]
  },
  {
    "title": "SCD Type 2 using DQ rules",
    "url": "https://community.ataccama.com/data-quality-catalog-94/scd-type-2-using-dq-rules-1624",
    "question": {
      "author": "Nataliia",
      "timestamp": "[No timestamp]",
      "content": "Hi everyone,\nData Observability tab and AI functionality  are  not available. How can I check SCD Type 2 using DQ rules?\nData Freshness. How can I check  using DQ rules? Datetime datatype is for eff_strt_dttm, it’s one column where data is, but I don’t know how make:   today() >>>as datetime  minus 1 day"
    },
    "answers": [
      {
        "author": "Albert de Ruiter",
        "timestamp": "3 months ago",
        "content": "Hi ​\n@Nataliia\n,\nThe today() function will return a date, and the now() function a datetime.\nWith the dateAdd function you can substract a day, so like dateAdd(now(),-1,’DAY’).\nWhen developing with expressions this page will be of support:\nONE Expressions - Ataccama ONE Gen2 Platform Latest\nKind regards,\nAlbert"
      },
      {
        "author": "Lisa Kovalskaia",
        "timestamp": "3 months ago",
        "content": "Hi ​\n@Nataliia\n-- there may be a number of ways to approach this, depending on your criteria for data freshness. Do you consider data fresh if all current records in the dimension table are updated no earlier that yesterday? If so the logic for your DQ rule could be something like:\nWHEN IsCurrent is TRUE and eff_strt_dttm >= dateAdd(now(),-1,’DAY’) THEN Result is VALID, ELSE Result is INVALID\nIf you have a different validation in mind, please let me know, happy to discuss your case."
      },
      {
        "author": "Nataliia",
        "timestamp": "3 months ago",
        "content": "Hey guys, thanks for your answers. But what in case values are different  in the column and just the newest ones have today's date, how I find the maximum value in the column and then compare? This is how my tries looks like, but looks it doesn't aggregate it"
      },
      {
        "author": "Lisa Kovalskaia",
        "timestamp": "3 months ago",
        "content": "Hi ​\n@Nataliia\n, here's how I'd check for groups of records where even the maximum/newest date in the timestamp column is older than yesterday:\nThere's a couple of things to look out for:\nthe Group By field is blank here - which means the entire dataset would be evaluated as a single group. That's probably not what you want, so apart from the etl_insrt_dttm input you should introduce the primary key as another input attribute from the data (or several attributes which you would then concatenate into a key using the Variable field).\nif the maximum date record in a given group violates the rule then all records in this group will be flagged as invalid - that's the nature of aggregation rules as they look at a group not at an individual row in itself.\nif you want to flag individual records that are out of date, consider using a regular/non-aggregation rule and adding another input attribute which would filter for current records (as I'm assuming your table contains historical records too and you don't need to evaluate them since they are by definition out of date, so they must be already flagged as such).  You might end up with an expression like WHEN IsCurrent is TRUE and eff_strt_dttm < dateAdd(today(),-1,’DAY’) THEN Result is INVALID, ELSE Result is VALID\nHope this gives you a starting point - if you have other questions or comments, please share!"
      }
    ]
  },
  {
    "title": "ONE Web Redshift freshness possibility",
    "url": "https://community.ataccama.com/data-quality-catalog-94/one-web-redshift-freshness-possibility-1617",
    "question": {
      "author": "Nataliia",
      "timestamp": "[No timestamp]",
      "content": "Hi guys! Is there any opportunity to check data freshness in case we have Redshift connection?\nI see there is no such driver in freshness configuration drivers list -\nhttps://docs.ataccama.com/one/latest/data-observability/data-freshness-custom-configuration.html\n."
    },
    "answers": [
      {
        "author": "Cansu",
        "timestamp": "3 months ago",
        "content": "Hi ​\n@Nataliia\n, I just checked with the team and currently we do not support Redshift connection. If you’d like I can move this post to our ideas section and this can be considered as product feedback to the team then 🙋🏻‍♀️"
      }
    ]
  },
  {
    "title": "How do we filter the terms (AOL) by relationship type",
    "url": "https://community.ataccama.com/data-quality-catalog-94/how-do-we-filter-the-terms-aol-by-relationship-type-1616",
    "question": {
      "author": "sumisha",
      "timestamp": "[No timestamp]",
      "content": "Need support with AQL expression to filter terms based on a particular relationship type. Referring to Relations section under Glossary Terms."
    },
    "answers": [
      {
        "author": "Lisa Kovalskaia",
        "timestamp": "3 months ago",
        "content": "Hi ​\n@sumisha\n, try this:\n@termRelationship(target).any(type.$id='#relationship_type_id#') to find child terms in a given type of relationship\nOR\n@termRelationship(source).any(type.$id='#relationship_type_id#') to find parent terms in a given type of relationship.\nYou'll notice the use of\nback references\nand\naggregation operators\nin these queries, see the linked documentation pages for more details and examples.\nTo provide the relationship type id go to your environment's Global Settings » Relationship types » Click on the type and copy the GUID from the URL bar of your browser.\nIs that what you were looking for? Let me know if you have a more specific use case that you're working on."
      },
      {
        "author": "sumisha",
        "timestamp": "3 months ago",
        "content": "Thanks ​\n@Lisa Kovalskaia\nThat work! Thanks for your reply.\nI have similar issue with AQL query, slightly different context:\nIn Metamodel, I have created a new ‘Term’ Property as referenced object array and this points to ‘Source’ metamodel entity, name property. How can I use AQL to filter particular Source?"
      },
      {
        "author": "Lisa Kovalskaia",
        "timestamp": "3 months ago",
        "content": "​\n@sumisha\nthe following should work:\nSource.any(name='#your_source#')\nPlease make sure to check your capitalization when you type it up - AQL search is case sensitive so entity/property names should match the metadata model exactly. On that note, as a best practice we recommend to stick with camel case across the metadata model to make spelling more predictable and to streamline the process of working with the model for everyone on the team. In this case, I'd name this referenced object array property\nsources\n- using camel case and making it plural, to reflect this being an array rather than a single reference.\nJust curious, what is the purpose this new property? Out of the box you can apply terms onto sources, but you want to do it the other way around - how will you use that? Thanks for sharing!"
      },
      {
        "author": "sumisha",
        "timestamp": "3 months ago",
        "content": "Thanks for your support and guidance Lisa. I know what you are referring as right approach. Just trying a different approach here to get more exhaustive list than actual sources by creating list of sources as terms and linking them."
      }
    ]
  },
  {
    "title": "How to access “Monitoring projects” through API calls",
    "url": "https://community.ataccama.com/data-quality-catalog-94/how-to-access-monitoring-projects-through-api-calls-1581",
    "question": {
      "author": "Bhuvaneshwari Balasubramaniam",
      "timestamp": "[No timestamp]",
      "content": "Hi ,\nI am trying to access “Monitoring projects” through API calls. For this purpose , I have created service account in KeyCloak with ‘default” and “DQIT_admin” IAM role access. Able to generate tokens with the service account. But unable to access “Monitoring projects” API , receiving 401 “Unauthorized” response.\n2 questions:\nHow to check whether this service account is able to access the Ataccama application or not ?\nAny other or what level of access rights need to be provided to access API calls for a service account?\nPlease note I am using Ataccama web application version 14.4."
    },
    "answers": [
      {
        "author": "may_kwok",
        "timestamp": "3 months ago",
        "content": "I’d venture a guess that trying to access ONE Graphql,  you’ll need some sort of MMM role, instead of DQIT?"
      },
      {
        "author": "Bhuvaneshwari Balasubramaniam",
        "timestamp": "3 months ago",
        "content": "Hi ​\n@may_kwok\n, Thanks for the reply. Sure will look into the “MMM” roles."
      },
      {
        "author": "Bhuvaneshwari Balasubramaniam",
        "timestamp": "3 months ago",
        "content": "Hi ,\nWould like to give an update on this query.\nTo access ‘Monitoring projects’ through API, the service account should be provided with role “\nDMM_PUBLIC_API\n“."
      },
      {
        "author": "Cansu",
        "timestamp": "3 months ago",
        "content": "Thank you for the solution here ​\n@may_kwok\nand ​\n@Bhuvaneshwari Balasubramaniam\nfor chipping in with the update!"
      }
    ]
  },
  {
    "title": "Collibra Integration in Ataccama ONE Desktop 🏹",
    "url": "https://community.ataccama.com/data-quality-catalog-94/collibra-integration-in-ataccama-one-desktop-883",
    "question": {
      "author": "Cansu",
      "timestamp": "[No timestamp]",
      "content": "Hi everyone,\nToday’s post is specifically on Collibra! So if you work with Collibra and are curious about how to integrate it with Ataccama ONE Desktop read on. This post will cover how you can harness the power of the\nCollibra Reader\nand\nCollibra Writer\nsteps to effortlessly manage Data Quality (DQ) Rules.\nPrerequisites\nBefore we dive in, ensure you have the following essentials:\nAtaccama ONE Desktop\n: Make sure it's installed on your machine with a valid license.\nCollibra\n: You need an active Collibra instance, complete with valid credentials for accessing and managing assets.\nSetting Up a Collibra Server Connection\nLet's establish a connection to Collibra from ONE Desktop:\nIn ONE Desktop, right-click on\nServers\n, and choose\nNew > Server\n.\nFill in the details:\nImplementation\n: Choose\nGeneric\n.\nName\n: Give your connection a meaningful name.\nURL\n: Provide the URL of your Collibra instance.\nAuthentication\n: Select\nBasic\n.\nUsername\n: Enter your Collibra username.\nPassword\n: Input your Collibra password.\nClick\nFinish\n.\nReading Data Quality Rules from Collibra\nNow, let's extract DQ rule definitions from Collibra into a text file:\nStart by creating a project in Ataccama ONE Desktop to keep your work organized. Right-click on\nDQ Projects\nand select\nNew > Project\n.\nName your project and click\nFinish\n.\nWithin the project directory, right-click and select\nNew > Plan\n.\nGive your plan a name and click\nFinish\n. This opens the plan canvas for implementation.\nRight-click on the canvas and choose\nInsert Step\n(or use the shortcut Ctrl + I).\nSelect\nCollibra Reader\n, then click\nOK\nto place it on the canvas.\nDouble-click the step to configure it. On the\nGeneral\ntab, provide the following information:\nURL Resource\n: Select the Collibra server you created.\nCommunity Name\n: Specify the Collibra community you want to read from (found in Collibra).\nDomain Name\n: Define the domain within the community (found in Collibra).\nAsset Type\n: Choose the type of asset you want to read (found in Collibra).\nConfigure which columns from Collibra's data quality rules you wish to read on the\nColumns\ntab. Start by mapping the default columns:\nAsset Id Column\n: Rule identifier.\nAsset Name Column\n: Name of the DQ rule.\nAsset Type Column\n: Type of asset (e.g., Data Quality Rules).\nDomain Id Column\n: Domain identifier.\nImport the value of the\nDescription\nfield in Collibra by creating a new column.\nAfter configuring, click\nApply\nand then\nOK\nto save your settings.\nWith the Collibra Reader step set up, you can now output the results to a file using the\nText File Writer\nstep. Add this step to the canvas, and double-click to configure.\nSpecify the output file's name with a\n.txt\nextension, and select\nOK\n.\nCreate a connection between the\nCollibra Reader\nand\nText File Writer\nsteps.\nRight-click anywhere on the canvas and select\nRun\n.\nAfter execution, hold the Ctrl key and double-click the\nText File Writer\nstep to view the result.\nWriting Data Quality Rules to Collibra\nTo create a new rule definition from scratch and write it back to Collibra using the\nCollibra Writer\nStep, follow these steps:\nRight-click the desired directory (e.g., the one used in the previous section) and choose\nNew > Plan\n.\nProvide a name for your plan and click\nFinish\n. This opens the plan canvas.\nInsert the\nRandom Record Generator\nstep.\nDouble-click the\nRandom Generator\nstep to configure it. On the\nGeneral\ntab, set\nRecord Count\nto 1.\nOn the\nColumns\ntab, click\nAdd\nto select\nExpression\nfrom the\nSelect Implementation\ndialog.\nFill in the fields:\nName\n: RuleName.\nType\n: String.\nExpression\n: \"Ataccama Test Rule\".\nAdd another column following step 5:\nName\n: RuleDescription.\nType\n: String.\nExpression\n: \"This is a test to prove that Ataccama can write data back to Collibra\".\nInsert a\nCollibra Writer\nstep.\nConfigure the\nCollibra Writer\nstep by setting the\nUrl Resource\nas the name of the server created earlier in this guide.\nOn the\nAssets\ntab, create a new configuration with the following settings:\nAsset Name Column\n: The name of the column created in step 6.\nAsset Type Name\n: The asset type corresponding to Collibra.\nCommunity Name\n: The target community in Collibra.\nDomain Name\n: The community domain in Collibra.\nStatus Column\n: The desired status column name.\nWrite the value of the\nDescription\nfield to Collibra in\nAsset Writer Columns\n, using the column name from step 7.\nEstablish a connection between the\nCollibra Writer\nand\nRandom Record Generator\nsteps.\nRight-click anywhere on the canvas and select\nRun\n.\nAfter successfully running the plan, you can verify that the rule is created in Collibra.\nFollow these steps to successfully build the Collibra integration in Ataccama ONE Desktop for efficient data governance and quality control. Let us know if you have any questions in the comments 👇"
    },
    "answers": [
      {
        "author": "devi_potturi",
        "timestamp": "3 months ago",
        "content": "Hello Cansu,\nAre there any plans available to migrate the dq metrics from ataccama to collibra ? If yes, will you be able to provide or guide?"
      },
      {
        "author": "Cansu",
        "timestamp": "3 months ago",
        "content": "Hi ​\n@devi_potturi\nI’ll reach out to you via PM!"
      }
    ]
  },
  {
    "title": "Fuzzy Checks with No Unique Identifier",
    "url": "https://community.ataccama.com/data-quality-catalog-94/fuzzy-checks-with-no-unique-identifier-1606",
    "question": {
      "author": "Radziah",
      "timestamp": "[No timestamp]",
      "content": "Hi\nI would like to ask whether we could do fuzzy check in Ataccama that involves two different datasets let’s say Set 1 and Set 2 from different data sources. Please note that both datasets have different number of columns but fuzzy checks involved only customer name and address. However, these datasets don’t have any unique identifier. Is it achievable in Ataccama and what would be the possible steps to achieve the results after the fuzzy checks? Thanks."
    },
    "answers": [
      {
        "author": "anna.spakova",
        "timestamp": "3 months ago",
        "content": "Hello ​\n@Radziah\n,\nto build a DQ check on top of two data sets, you have to first join those together to create 1 dataset because at the moment the DQ rules do not support cross-table DQ checks. To achieve this, you can use the Virtual Catalog Item (since you mentioned the data are from different sources).\nhttps://docs.ataccama.com/one-desktop/15.4.0/work-with-ataccama-one/virtual-catalog-items.html\nWas this what you were asking or are you also interested in the fuzzy DQ checks? If that’s the case, can you provide more details on what this fuzzy DQ check should be?\nThank you.\nKind regards,\nAnna"
      },
      {
        "author": "Radziah",
        "timestamp": "3 months ago",
        "content": "Hi Anna\nWhen you said DQ rules do not support cross-table DQ checks, does it refer to ONE Desktop or ONE Web or both? If i am not mistaken, to have fuzzy checks done, it can only be done on ONE Desktop, correct? The data comes one from database, another one from flat file.\nThe details is we want to have the fuzzy checks of name and address between two datasets."
      },
      {
        "author": "anna.spakova",
        "timestamp": "3 months ago",
        "content": "Hi ​\n@Radziah\n,\nneither actually, because even when you want to do the DQ rules in the ONE Desktop (through the validation component), it still expects only one input = table. You would have to do the validation completely independently through ONE Desktop only - so in a plan that wouldn’t be connected to the ONE Web, and deploy it on the orchestration server if needed. In that case, you are not able to apply the rule in the monitoring projects and see the results in the web application.\nAs for the fuzzy check, I am unsure what exactly it means, if it’s about some fuzzy matching functions like e.g. Levenshtein, this is possible also in the UI, if it’s about some more complicated logic, then yes, the validation component might be a better choice, but it will highly depend on the logic of the rule.\nKind regards,\nAnna"
      },
      {
        "author": "Cansu",
        "timestamp": "3 months ago",
        "content": "Hi ​\n@Radziah\n, I’m closing this thread for now. If you have any follow up questions please don’t hesitate to share them in the comments or create a new post🙋🏻‍♀️"
      }
    ]
  },
  {
    "title": "Assigning Data Usage Licensing Agreements (Policy) Through One Desktop",
    "url": "https://community.ataccama.com/data-quality-catalog-94/assigning-data-usage-licensing-agreements-policy-through-one-desktop-1615",
    "question": {
      "author": "apejko",
      "timestamp": "[No timestamp]",
      "content": "Hello everyone! I currently have a large group of catalog items that I am trying to assign a Data Usage Licensing Agreement too. I know that I can manually add these through Atacama online, however because of the large number I was wondering if it would be possible to create a plan to do this for me? I have all the catalogItem ids already compiled I simply do not know how to assign the policy.\nWhile I do see the option to write to policy, I cant seem to set the parent as a catalog item. Any help would be appreciated.\nThank you!!!"
    },
    "answers": [
      {
        "author": "anna.spakova",
        "timestamp": "3 months ago",
        "content": "Hello ​\n@apejko\n,\ncan you please explain what is the Data Usage Licensing Agreement? I don’t see it in the default metadata model, so it is most likely something custom. Depending on what type of this property is, the way how to populate it from the ONE Desktop will differ.\nFirst of all, you will have to update the entity catalogItem (Entity Type = catalogItem) if this property is linked to it. Then if it’s some standard simple string, you can populate the values under Columns:\nIf the property is some object (either reference or embedded), the situation is more complicated. Could you provide a screenshot of the metadata model for the catalogItem to see what is the type of the property?\nThank you.\nKind regards,\nAnna"
      },
      {
        "author": "apejko",
        "timestamp": "3 months ago",
        "content": "Hi ​\n@anna.spakova\nthank you for the response. These are what im referring too when I say Data usage license agreement:\nThey are referred to as a policy and are located in the knowledge catalog under Data usage license management on our enviroment:\nHere is the property information of the Data Usage License Agreement in relation to the catalogItem:"
      },
      {
        "author": "Catherine",
        "timestamp": "3 months ago",
        "content": "Hello ​\n@anna.spakova\nto give you bit of more context, this processing is within the Policy Management functionality."
      },
      {
        "author": "anna.spakova",
        "timestamp": "3 months ago",
        "content": "Hello,\nthank you both for clarification and the screenshots, if I understood the screenshots right, you have the policies as Array of recerences inside the catalog item. If that’s the case, you need to use a special step called ONE Metadata Reference Array Writer and configure it like this (please adjust names of the fileds where you store the IDs):\nThe entity you are modifying is the catalog Item. Id Column Name is the column with the ID of the catalog item. The property is per your screenshot the dataUsageLicenseAgreement (=the array of references) and finaly, the Target Id Column Name is the ID of the policy you want to add into the property.\nDoes it make sense? Please give it a try and let me know if you run into any issues :)\nKind regards,\nAnna"
      }
    ]
  },
  {
    "title": "Creating rules for Name field",
    "url": "https://community.ataccama.com/data-quality-catalog-94/creating-rules-for-name-field-1573",
    "question": {
      "author": "Bhuvaneshwari Balasubramaniam",
      "timestamp": "[No timestamp]",
      "content": "Hi ,\nCould someone guide me on creating rules for Name field.\nRule : Name should not have any special characters but can have numbers and spaces.\nEx: Jhon Leo - Valid\nJho1 Leo - Valid\nJho( Leo* - Not Valid\nRegards,\nBhuvi"
    },
    "answers": [
      {
        "author": "C_R",
        "timestamp": "3 months ago",
        "content": "Hey Bhuvi, I had a similar requirement before. I solved it by using regex rule that allows only letters, numbers, and spaces while blocking special characters. You can create a DQ rule with this regex: (\n^[A-Za-z0-9 ]+$)\n. Hope this helps!"
      },
      {
        "author": "Bhuvaneshwari Balasubramaniam",
        "timestamp": "3 months ago",
        "content": "Thank you for the solution ​\n@C_R\n!"
      },
      {
        "author": "Cansu",
        "timestamp": "3 months ago",
        "content": "Thank you for the solution ​\n@C_R\n!"
      }
    ]
  },
  {
    "title": "Semi Structured Data",
    "url": "https://community.ataccama.com/data-quality-catalog-94/semi-structured-data-1566",
    "question": {
      "author": "Susan24us",
      "timestamp": "[No timestamp]",
      "content": "Hello, i have a report that we usually saved to hard drive that i would like to bring into Ataccama, the report is in semi structured format . does ataccama accept or work with semi structure data ?or do i need to convert the report first before connecting to source ?"
    },
    "answers": [
      {
        "author": "anna.spakova",
        "timestamp": "3 months ago",
        "content": "Hello ​\n@Susan24us\n, thank you for your question. Can you provide more details about your semi-structured data (what is the specific type) and version of Ataccama? Ataccama supports some types out of the box (like parquet files), some require an extra preprocessing (like JSON or XML), some we don’t support at all.\nThank you.\nKind regards\nAnna"
      },
      {
        "author": "OGordon100",
        "timestamp": "3 months ago",
        "content": "Hi Susan,\nJust to add to Anna’s answer, semi-structured data formats (e.g. Delta Lake, Iceberg, etc) are not formally supported in Ataccama right now. However, it is on our roadmap - if you speak to your account manager they will be able to share timelines with you.\nBest,\nOli"
      },
      {
        "author": "Cansu",
        "timestamp": "3 months ago",
        "content": "Hi ​\n@Susan24us\nI’m closing this thread for now. If you have any follow up questions please share them in the comments or don’t hesitate to create a new post🙋🏻‍♀️"
      }
    ]
  },
  {
    "title": "Improving Data Quality: Turning insights into action",
    "url": "https://community.ataccama.com/data-quality-catalog-94/improving-data-quality-turning-insights-into-action-1585",
    "question": {
      "author": "Cansu",
      "timestamp": "[No timestamp]",
      "content": "We are half-way into the end-to-end DQ guide! Just jumping on here? Read the first post of the series 👇\nIntroducing the end-to-end Data Quality Guide: Laying the foundation – understanding your data landscape\nIdentifying data issues is just the beginning—what really matters is fixing them 🛠️\nData quality improvement involves\nstandardizing formats, removing duplicates, and setting up automated remediation workflows\n​. Whether it’s fixing customer records, cleansing addresses, or resolving inconsistencies, clean data means more reliable insights.\nData transformation\ntools play a crucial role in making data fit for purpose, while\nmatching and merging\nensure that duplicate records don’t pollute your systems. Additionally,\ndata remediation workflows\nenable businesses to track and resolve data quality issues efficiently​.\nUltimately, improving data quality isn’t just about technology—it’s about creating a culture where teams take ownership of data health. Organizations that prioritize continuous data improvement see better business outcomes and reduced operational risks.\nHow did you manage to build a data culture in your organization?\nHow does your team handle data cleansing and remediation?\nHave you automated any of these processes, or are they still manual?\nShare your experiences in the comments and learn from others👇"
    },
    "answers": []
  },
  {
    "title": "Primary key/nullable/column size  field disappears after refreshing  catalog item",
    "url": "https://community.ataccama.com/data-quality-catalog-94/primary-key-nullable-column-size-field-disappears-after-refreshing-catalog-item-1618",
    "question": {
      "author": "Nataliia",
      "timestamp": "[No timestamp]",
      "content": "Hello, does anyone faced with issue when primary key/ nullable/ column size fields are gone after refreshing catalog item and rerun data select?\nI use ONE Web app v14, after setting attribute as primary key, nullable or defining column size then it disappears after  rerunning query.\nHow did you solve this issue?"
    },
    "answers": [
      {
        "author": "Lisa Kovalskaia",
        "timestamp": "3 months ago",
        "content": "Hi ​\n@Nataliia\n, those fields are populated during profiling based on the metadata available at the source, so you'll want to configure that column on the source table to see its properties correctly reflected in Ataccama. Hope this helps!"
      },
      {
        "author": "Nataliia",
        "timestamp": "3 months ago",
        "content": "thanks, the issue was resolved"
      }
    ]
  },
  {
    "title": "List of functions for advanced builder DQ rules",
    "url": "https://community.ataccama.com/data-quality-catalog-94/list-of-functions-for-advanced-builder-dq-rules-1619",
    "question": {
      "author": "Nataliia",
      "timestamp": "[No timestamp]",
      "content": "Hello guys, does someone know where I can find the information about all functions list that are available from Advanced builder while creating DQ rules? Will appreciate any help or advice, thanks."
    },
    "answers": [
      {
        "author": "Adrian Anderson",
        "timestamp": "3 months ago",
        "content": "Hi ​\n@Nataliia\n.\nThe Advanced Expression syntax is based on Ataccama’s Expression language.\nDoes this page help?\nhttps://docs.ataccama.com/one/latest/common-actions/one-expressions.html\nPlease let us know if this helps.\nNote: If ONE Desktop is available, we can also go to Help > Help Contents > Ataccama ONE Desktop (version x.x.x) > ONE Basics > Expressions\nKind regards,\nAdrian"
      }
    ]
  },
  {
    "title": "DQ - Monitoring project \"Export\"",
    "url": "https://community.ataccama.com/data-quality-catalog-94/dq-monitoring-project-export-1608",
    "question": {
      "author": "Bhuvaneshwari Balasubramaniam",
      "timestamp": "[No timestamp]",
      "content": "Hi ,\nKindly share your inputs on below scenario.\nI am trying to place the “Monitoring Projects” export  “.csv” file in a shared path/folder with in the organization. Purpose of this “Report need to accessed by different set of stakeholders who do not have Ataccama access”\nIs there a way to access this export file by any API calls or we can directly configure the shared folder path for example “S:/SharedPath/Ataccama/Exports” in output location field ?\nNote : This is on Ataccama ONE 14.4 version.\nAny input is appreciated.\nThanks,\nBhuvi"
    },
    "answers": [
      {
        "author": "anna.spakova",
        "timestamp": "3 months ago",
        "content": "Hello ​\n@Bhuvaneshwari Balasubramaniam\n,\nthe file is physically stored on Minio, so it should be possible to retrieve the file from there via API, I would check the official API documentation, e.g.\nhttps://min.io/docs/minio/linux/developers/python/API.html\nThe trouble can be if you are in Ataccama cloud, you might need to reach out to your Engagement manager or to our support to provide the necessary connection details.\nIt is also possible directly in the post-processing plan to send the file to another output, e.g. database or S3 or ADLS. You simply configure the connection in the Global configuration and use that inside the post-processing plan. Please note that you need connectivity to this data source from the DPE that is running the monitoring project.\nhttps://docs.ataccama.com/one/14.5.x/dpm-admin-console/dpm-admin-console.html#configuration\nDoes it make sense? Please let me know if you need more information.\nKind regards,\nAnna"
      }
    ]
  },
  {
    "title": "Apply rules only to newly created records through any filter",
    "url": "https://community.ataccama.com/data-quality-catalog-94/apply-rules-only-to-newly-created-records-through-any-filter-1601",
    "question": {
      "author": "Bhuvaneshwari Balasubramaniam",
      "timestamp": "[No timestamp]",
      "content": "Hi,\nCould anyone help me on below scenario. I am using Ataccama ONE web 14.4.\nMillions of records are populated in testing environment from Production real time data. However , for testing purpose manipulated few 1K records. Is there any way in Ataccama where we can apply rules and perform “Monitoring Projects” only to the new records 1K created by giving any date created filter?\nRegards,\nBhuvi"
    },
    "answers": [
      {
        "author": "anna.spakova",
        "timestamp": "3 months ago",
        "content": "Hi ​\n@Bhuvaneshwari Balasubramaniam\n, in your current version, you might need to create either SQL catalog item or Virtual catalog item to filter out the values (you can se some dynamic filter by using the today() or similar functions to always get the new records).\nhttps://docs.ataccama.com/one/14.5.x/catalog-items/sql-catalog-items.html\nhttps://docs.ataccama.com/one-desktop/14.5.x/work-with-ataccama-one/virtual-catalog-items.html\nIn the recent version 15.4 it is possible to use Data Slices, where you can specify the filter directly on the Catalog Item. If possible, I recommend you consider upgrading.\nhttps://docs.ataccama.com/one/latest/catalog-items/create-data-slice.html\nLet me know if this answers your question.\nKind regards,\nAnna"
      },
      {
        "author": "Bhuvaneshwari Balasubramaniam",
        "timestamp": "3 months ago",
        "content": "Hi Anna,\nThank you for the response."
      }
    ]
  },
  {
    "title": "Clearing all special characters expect defined special characters",
    "url": "https://community.ataccama.com/data-quality-catalog-94/clearing-all-special-characters-expect-defined-special-characters-1583",
    "question": {
      "author": "Anurag choube",
      "timestamp": "[No timestamp]",
      "content": "In the above image I want to clear all special characters expect .(dot) As their can be any thing in place of  ! number may have *, &, %, $, etc. I tried it using trashnondigit() function but it will remove all the characters from the number, Is their a way to do it?"
    },
    "answers": [
      {
        "author": "Adrian Anderson",
        "timestamp": "3 months ago",
        "content": "Hi ​\n@Anurag choube\n.\nThe substituteAll() function might be helpful here. It takes a regular expression as the substitution pattern and can be used in lots of these cases.\nFor example, this will remove all non-digits, except for dots and hyphens (allowing for negative numbers as input):\nsubstituteAll(\n'([^0-9\\\\.\\\\-])'\n,\n''\n, value )\nLet us know if this helps.\nKind regards,\nAdrian"
      },
      {
        "author": "Anurag choube",
        "timestamp": "3 months ago",
        "content": "Thank you so much ​\n@Adrian Anderson\nit worked."
      }
    ]
  },
  {
    "title": "Monitoring Data Quality – The key to trustworthy data",
    "url": "https://community.ataccama.com/data-quality-catalog-94/monitoring-data-quality-the-key-to-trustworthy-data-1584",
    "question": {
      "author": "Cansu",
      "timestamp": "[No timestamp]",
      "content": "Hi everyone,\nReady for the second part of our end-to-end DQ guide? Monitoring DQ is essential for building trust in your data assets.\nThis post explores how data profiling, observability, and automated monitoring help organizations detect and address data quality issues in real-time, ensuring data remains accurate, complete, and reliable.\nBefore starting, missed the first post? Check it out here 👇\nIntroducing the end-to-end Data Quality Guide: Laying the foundation – understanding your data landscape\nOnce your data is documented, the next step is understanding its quality. Without proper monitoring, data issues can go unnoticed, leading to poor decision-making.\nData profiling\nhelps by analyzing patterns, completeness, and accuracy, while\ndata observability\nprovides real-time monitoring to detect anomalies before they cause harm​.\nA\ndata quality monitoring\nsystem tracks key dimensions such as validity, completeness, and consistency, helping organizations demonstrate progress and ROI. Automate this process, detecting unexpected changes and flagging them for review​.\nBy continuously assessing data quality, organizations can build confidence in their data assets and reduce the risk of bad data entering critical workflows. But monitoring alone isn’t enough—data quality efforts must lead to real action.\nWhat are some of the biggest challenges you face in maintaining high-quality trusted data?\n➡️ Let’s discuss in the comments what are some helpful tactics for automation, monitoring, and profiling!"
    },
    "answers": []
  },
  {
    "title": "Configure the MMM Module for generating links in emails",
    "url": "https://community.ataccama.com/data-quality-catalog-94/configure-the-mmm-module-for-generating-links-in-emails-1589",
    "question": {
      "author": "NBalagura",
      "timestamp": "[No timestamp]",
      "content": "Hi everyone\nCould you please advice if there is any instruction how smpt server can be configured for e-mail notifications?\nWhere we can perform this configuration? Is there any option in Ataccama ONE web? I have checked this part of documentation\nMMM Configuration :: Ataccama ONE\nbut it is still not clear where mmm configuration can be done"
    },
    "answers": [
      {
        "author": "rafeeq.durowoju",
        "timestamp": "3 months ago",
        "content": "Hey ​\n@NBalagura\n,\nThe configuration for the SMTP server is done on the server side, inside the application.properties file of the MMM-backend service. If you have a self managed deployment, then it must be configured there by a network admin, if you have a PaaS or PaaS+ deployment then it is configured via a support ticket request.\nRegards,\nRafeeq"
      },
      {
        "author": "NBalagura",
        "timestamp": "3 months ago",
        "content": "Thank you, ​\n@rafeeq.durowoju"
      }
    ]
  },
  {
    "title": "v15 Data slices - multiple slices in the same catalog item in the same project?",
    "url": "https://community.ataccama.com/data-quality-catalog-94/v15-data-slices-multiple-slices-in-the-same-catalog-item-in-the-same-project-1578",
    "question": {
      "author": "may_kwok",
      "timestamp": "[No timestamp]",
      "content": "Hello all,\nI am on v14.5.3, so I haven’t seen the Data Slice feature hands on yet. Been reading the documentation on the Data Slice and its use in the monitoring project.\nSay for example, I have a table of transactions, and I have some statuses:\nPending\nActive\nInactive\nCancelled\nLet’s say I have the following rules and the respective slices they need to be applied on:\nRule 1\nStatus = ‘Active’\nRule 2\nStatus in (‘Active’, ‘Pending’)\nRule 3\nStatus = ‘Pending’\nIs it then possible to create an ‘Active’ slice, and a ‘Pending’ slice?\nAnd then in my monitoring project, I add my catalog item, then I apply my Rule 1 to the Active slice, Rule 3 to the Pending slice, and Rule 2 to both the Active and Pending slices? Or something like that?\nTrying to gauge how we can use it, how much maintenance overhead there will be and the effort to try and start using this feature.\nAny insights from people who’s already using v15 much appreciated!"
    },
    "answers": [
      {
        "author": "rafeeq.durowoju",
        "timestamp": "3 months ago",
        "content": "Hey ​\n@may_kwok\n,\nFor the data slice feature, you can only have one active data slice per catalog item in the Monitoring Project, so if you wish to have multiple data slices, you will need to create separate Monitoring Projects to that affect. You do have the ability to create a data slice with multiple values though.\nYou would then have your applicable rules applied to the respective Monitoring Projects.\nLet me know if you have any other questions.\nRegards,\nRafeeq"
      },
      {
        "author": "may_kwok",
        "timestamp": "3 months ago",
        "content": "Thanks Rafeeq for the confirmation!"
      }
    ]
  },
  {
    "title": "Data Reconciliation Best Practices 👾",
    "url": "https://community.ataccama.com/data-quality-catalog-94/data-reconciliation-best-practices-833",
    "question": {
      "author": "Cansu",
      "timestamp": "[No timestamp]",
      "content": "Hi Community!\nIn the last few months the focus of our articles was majorly on features of ONE Web platform, from the basics to more complicated practices and now it’s time to move on to the next step. From next week we are moving on to ONE Desktop🖥️\nToday we will focus on Data Reconciliation, and cover how you can utilize this feature effectively for seamless data consistency across multiple sources. Let’s get to it!\nIn a nutshell, the Data Reconciliation feature confirms that your migrated data from one database to another is intact and accurate, without any data leaks or glitches. It conducts high-level checks on data consistency by comparing attribute profile statistics and even attributes fingerprints (except for Snowflake data).\nStep-by-Step Guide to Data Reconciliation\n1. Create a Reconciliation Project in Data Quality:\nTo kick off your data reconciliation journey, navigate to\nData Quality\nand then select\nReconciliation Projects\n. Here, you can either create a new project or edit existing ones. This is your control center for all things data reconciliation.\n2. Choose Data Origin and Target:\nThe heart of data reconciliation lies in mapping data between origin and target sources. You can select entire sources, specific locations, or even catalog items for comparison. Ataccama ONE helps you match as many catalog items as possible based on names and data structures. Unmapped data can be manually mapped, and automatic mappings can be overridden if needed.\n3. Explore Mapping Possibilities:\nYou can compare single or multiple data sources, multiple locations, catalog items, or various combinations. Remember, locations and catalog items should belong to one origin and one target data source. For comparing data from more than two sources, you'd need to create multiple mappings.\n4. Tweak Settings for Optimal Results:\nFine-tune your data reconciliation settings. Consider enabling or disabling options like auto-mapping, data type issue detection, and use of existing profiling, based on your specific requirements.\n5. Run Data Reconciliation and Get Notified:\nInitiate data reconciliation by selecting\nPublish and reconcile\nafter mapping or by choosing an existing project and selecting\nCompare data\n.\nOnce done, results can be viewed in the\nResults\ntab, where you'll find a comprehensive listing of issues. Configure notifications to keep you informed about any data reconciliation glitches via channels like in-app messages, email, Slack, or Microsoft Teams.\nData Reconciliation data consistency checks are easier and ensure your data migrations are flawless, accurate, and transparent.\nShare your experiences, insights, and discoveries with the community in the comments 👇"
    },
    "answers": [
      {
        "author": "Rianna",
        "timestamp": "1 year ago",
        "content": "Great feature! This will be a game changer for us! What version is this available on please?"
      },
      {
        "author": "Cansu",
        "timestamp": "1 year ago",
        "content": "Hi\n@Rianna\nthis is available from version 14.4 💜"
      },
      {
        "author": "sumeyra",
        "timestamp": "1 year ago",
        "content": "Hello Cansu,\nThanks for sharing the best practice of this feature.\nI have a question regarding Results page. In my case, there is no difference in between catalog items. This is really good for us, however I would expect to see numbers of attributes. I mean, normally when we have inconsistency, there is a list containing attributes I have selected to compare. And I would see their profiling. My question is, why we can’t see attributes profilings if there are no issue in between catalog items.\nThanks a lot!"
      },
      {
        "author": "Bharathi",
        "timestamp": "4 months ago",
        "content": "Hi,\nThis information is helpful. However, I have a question on results. We can see issues found in overall. But, is there is a way to see data records, which is not matching between source and target. This will be very much needed during data migration validation between source and target data.\nAny input on this would be very helpful!\nThanks!"
      },
      {
        "author": "anna.spakova",
        "timestamp": "3 months ago",
        "content": "Hi ​\n@Bharathi\n, the reconciliation projects compare only the summary statistics from profiling; they do not compare the individual records. For that reason, it is not currently possible to obtain a detailed list of records that differ between the sources.\nTo get this detail, I would suggest creating a monitoring project that can actually check every single record against another and get the results using post-processing.\nKind regards,\nAnna"
      },
      {
        "author": "Radziah",
        "timestamp": "3 months ago",
        "content": "Hi, I wonder whether we can do data reconciliation on ONE Desktop instead of ONE Web and the push results to ONE Data?"
      },
      {
        "author": "anna.spakova",
        "timestamp": "3 months ago",
        "content": "Hi ​\n@Radziah\n, AFAIK there is no import into the ONE Data from ONE Desktop available at the moment, you would need to send the data into e.g. some database, connect it in the web application and then via the web app either import it from Catalog Item into the ONE Data or using the transformation plan.\nhttps://docs.ataccama.com/one/15.1.0/one-data/import-data-from-a-catalog-item.html\nhttps://docs.ataccama.com/one/15.1.0/monitoring-projects/data-transformation-plans.html\nKind regards,\nAnna"
      }
    ]
  },
  {
    "title": "Selecting multiple checkboxes",
    "url": "https://community.ataccama.com/data-quality-catalog-94/selecting-multiple-checkboxes-1575",
    "question": {
      "author": "Arno",
      "timestamp": "[No timestamp]",
      "content": "Hi everyone,\nSometimes I'd like to select a large number of checkboxes in Ataccama, for example to profile a large number of Catalog items. Of course creating a plan is also an option, but not always the least time consuming.\nHow do you do this with many checkboxes involved? There are browser extensions available for many browers, but not sure if those are secure enough. Any recommendations or tips?"
    },
    "answers": [
      {
        "author": "rafeeq.durowoju",
        "timestamp": "3 months ago",
        "content": "Hey Arno,\nMay I ask what version of the tool you’re in.\nIn the newer versions (i.e v15.4) there is an option to select multiple items in bulk.\nIf possible, can you upgrade to the latest LTS version?\nRegards,\nRafeeq"
      },
      {
        "author": "Arno",
        "timestamp": "3 months ago",
        "content": "Hi Rafeeq,\nThank you for your response. We've just upgraded to v14.5.\nKind regards,\nArno"
      }
    ]
  },
  {
    "title": "Data Source - BOX.COM Connectivity",
    "url": "https://community.ataccama.com/data-quality-catalog-94/data-source-box-com-connectivity-1591",
    "question": {
      "author": "Catherine",
      "timestamp": "[No timestamp]",
      "content": "Please advise if “BOX.COM” is a data source that is supported out of the box, being that ONE can integrate with “BOX.COM” using an existing JDBC or API connector."
    },
    "answers": [
      {
        "author": "rafeeq.durowoju",
        "timestamp": "3 months ago",
        "content": "Hey ​\n@Catherine\n,\nBox is not one of the data sources we support out of the box. If the tool has an API endpoint then it is something that we can connect to from the Ataccama side.\nThough I might recommend trying the “cdata” driver approach if that is something available for your team.\nRegards,\nRafeeq"
      }
    ]
  },
  {
    "title": "Atacama \"Security Terms\" written to",
    "url": "https://community.ataccama.com/data-quality-catalog-94/atacama-security-terms-written-to-1592",
    "question": {
      "author": "Catherine",
      "timestamp": "[No timestamp]",
      "content": "Use Case:  For our Data Lose Protection initiative, we are profiling and classify OneDrive, SharePoint, etc and we have a requirement to write the “security term (data classification - restricted, confidential, etc)” to the asset sensitivity labels (metadata)"
    },
    "answers": [
      {
        "author": "rafeeq.durowoju",
        "timestamp": "3 months ago",
        "content": "Hey ​\n@Catherine\n,\nApologies but I’m not sure I understand what the question is here.\nSeems to me you’re trying to implement some auto-mapping of a term or fill in another property based on term mapping?\nRegards,\nRafeeq"
      }
    ]
  },
  {
    "title": "Notification on Policy Update or Creation",
    "url": "https://community.ataccama.com/data-quality-catalog-94/notification-on-policy-update-or-creation-1576",
    "question": {
      "author": "RAZORX",
      "timestamp": "[No timestamp]",
      "content": "Hi Community,\nIs it possible to have a email notification to a group of people in case of an update in the existing policies in One Web?"
    },
    "answers": [
      {
        "author": "AKislyakov",
        "timestamp": "3 months ago",
        "content": "Hi ​\n@RAZORX\nThere are no built-in notifications for Policy object. However, you can always employ\nNotifications Handler :: Ataccama ONE\nto subscribe to events (like create/update/delete) on the Policy object and send email notifications as needed."
      },
      {
        "author": "RAZORX",
        "timestamp": "3 months ago",
        "content": "Hi ​\n@AKislyakov\n,\nThanks for the reply, but is it possible to send a custom notification using the Notifications Handler while making any change to any items.\nFor Example: I edited a policy or a term or a catalog item, and now the catalog item is in draft state and it went for review to the respective steward assigned which is already available in the One Web as a task.\nApart from that i want to send a custom notification to the user groups using a email or a notification popup in one web itself. Is it achievable?\nAppreciate your input on this."
      },
      {
        "author": "AKislyakov",
        "timestamp": "3 months ago",
        "content": "The notification handler only receives events for completed actions. That is, once the update is published, you will receive an event and can send notification emails, but not during the review phase."
      }
    ]
  },
  {
    "title": "Reading array of response using Json call",
    "url": "https://community.ataccama.com/data-quality-catalog-94/reading-array-of-response-using-json-call-1324",
    "question": {
      "author": "Karthikeyan",
      "timestamp": "[No timestamp]",
      "content": "Hi Team,\nI am trying to json call to read the response of api and process it for further results. This is my example response\n{\n    \"\nData\n\":\n[\n        {\n            \"\nName\n\":\n\"Amount\"\n,\n            \"\nId\n\":\n\"12345\"\n,\n            \"\nValueListAttribute_5d\n\":\n[\n                {\n                    \"\nterm\n\":\n\"Attribute\"\n,\n                    \"\nterm_Id\n\":\n\"67\"\n}\n            ]\n,\n            \"\nStringAttribute_00\n\":\n[\n                {\n                    \"\ndefinition\n\":\n\"Dollar amount\"\n,\n                    \"\ndefinition_id\n\":\n\"5\"\n}\n            ]\n,\n            \"\nBooleanAttribute_18e\n\":\n[\n                {\n                    \"\nindicator\n\":\n\"true\"\n,\n                    \"\nindicatorid\n\":\n\"6\"\n}\n            ]\n,\n        },\n        {\n            \"\nName\n\":\n\"Amount\"\n,\n            \"\nId\n\":\n\"12345\"\n,\n            \"\nValueListAttribute_5d\n\":\n[\n                {\n                    \"\nterm\n\":\n\"Attribute\"\n,\n                    \"\nterm_Id\n\":\n\"67\"\n}\n            ]\n,\n            \"\nStringAttribute_00\n\":\n[\n                {\n                    \"\ndefinition\n\":\n\"Dollar amount\"\n,\n                    \"\ndefinition_id\n\":\n\"5\"\n}\n            ]\n,\n            \"\nBooleanAttribute_18e\n\":\n[\n                {\n                    \"\nindicator\n\":\n\"true\"\n,\n                    \"\nindicatorid\n\":\n\"6\"\n}\n            ]\n,\n        },\n    ]\n,\n    \"\nDisplayRecords\n\":\n2\n,\n    \"\nRecords\n\":\n2\n}\nI am trying to read these values using Json call and use them for further processing in my component. But I am not sure how i can read the values. For e.g.: If I need to get indicator how can I get that using Json call."
    },
    "answers": [
      {
        "author": "Adrian Anderson",
        "timestamp": "9 months ago",
        "content": "Hi Karthikeyan.\nGreat question!\nThere is an example in the Tutorials, \"01.05 Read json file.plan\" (under \"01 Reading input\"), that I often refer to help with parsing json documents. Even though it uses a Json Reader step the same concepts apply.\nTo parse embedded arrays, additional Data Streams are used.\nIn the provided json example we have the array, BooleanAttribute_18e, which contains the \"indicator\" attribute that we're interested in, as part of the \"Data\" array. In the step's configuration we configure another Data Stream under the \"Data\" Data Stream.\nWe can then link the sub-array contents back to their parents, if needed, using Shadow Columns.\nPlease let us know if this helps.\nCheers,\nAdrian"
      },
      {
        "author": "Karthikeyan",
        "timestamp": "9 months ago",
        "content": "Hi\n@Adrian Anderson\nThanks for the response.\nI am using Json call here but when I run the component, I am getting error like - “The endpoint 'BooleanAttribute_18e' needs at least 1 input/s associated with it. [Json Call (Json Call)]\nCould you please let me know if I am missing something here."
      },
      {
        "author": "Adrian Anderson",
        "timestamp": "9 months ago",
        "content": "Hi\n@Karthikeyan\n.\nSince there could be many ‘BooleanAttribute_18e's sections for each Data section, the Json Call step will now have a third output that will contain records with the ‘indicator’ values, similar to the example provided in the original reply:\nWhat we do with this new ‘BooleanAttribute_18e’ output will depend on how we want to process the indicator values in relation to the other values.\nBy using a Shadow Column to bring the Id from the Data section (as mentioned in the original reply) we could, for example, join this back to the Data output:\nNote: The Id of the second Data section was amended from 12345 to 67890 in your original example json to highlight the difference.\nIf this still doesn’t work, perhaps try just adding a third Text File Writer step and joining it to the new output of the Json Call step to see what information is available there.\nCheers,\nAdrian"
      },
      {
        "author": "Cansu",
        "timestamp": "9 months ago",
        "content": "Hi\n@Karthikeyan\n, I’m closing this thread for now, if you have any follow up questions please feel free to share them in the comments or create a new post 🙋‍♀️"
      },
      {
        "author": "Karthikeyan",
        "timestamp": "9 months ago",
        "content": "Thanks\n@Adrian Anderson\nand\n@Cansu"
      },
      {
        "author": "Radziah",
        "timestamp": "3 months ago",
        "content": "Hi ​\n@Adrian Anderson\nI have been following this thread because I have a similar use case, but for some reasons, I am unable to retrieve data from the child operator. I tried following your tutorial here and in ONE Desktop but still not able to retrieve the child data.\nThe main is called ‘value’ and I have no problem in retrieving the data beneath it, but not the child for example under the InfoBlock.\nI followed the path like what you did above but non of the attributes produce any values.\nFor the value data stream, the path is defined as $.value\nWhat is your advise? Thanks!"
      },
      {
        "author": "may_kwok",
        "timestamp": "3 months ago",
        "content": "Hey ​\n@Radziah\n,\nBecause your InfoBlock is already nested under value, try the path for InfoBlock just “InfoBlock”, rather than “$.value.InfoBlock”?"
      },
      {
        "author": "Radziah",
        "timestamp": "3 months ago",
        "content": "I have tried that before trying to put $.value.InfoBlock ​\n@may_kwok\n, but still no output"
      },
      {
        "author": "Adrian Anderson",
        "timestamp": "3 months ago",
        "content": "Hi ​\n@Radziah\n.\nLooking at the JSON in the screenshot, I’m not sure that it’s well-formed. After InfoBlock there is an opening square bracket but there doesn’t appear to be an associated closing one.\nI’ve put together a simple Json Reader step (file-based) version that hopefully helps, with some well-formed JSON (attached in .zip) as input. The concepts are the same for the Json Parser step.\nSee the attached project .zip for more details.\nPlease let us know if this helps.\nCheers,\nAdrian"
      }
    ]
  },
  {
    "title": "Validate a string in the format YYYYMMDD",
    "url": "https://community.ataccama.com/data-quality-catalog-94/validate-a-string-in-the-format-yyyymmdd-1579",
    "question": {
      "author": "TimBrown74",
      "timestamp": "[No timestamp]",
      "content": "How would I validate a string in the format YYYYMMDD is a valid date?  I assume the range of valid years is 1940 to 2050."
    },
    "answers": [
      {
        "author": "may_kwok",
        "timestamp": "3 months ago",
        "content": "I’d approach it first by making sure the string is actually a date, and then picking out the year component from it and testing whether it is earlier or later than the range.\nSo first, in the first condition I use:\ntoDate(date_string,\n'yyyyMMdd'\n)\nis\nnull\nIf the string is not a valid date, it will fail to convert and return null. Note that Ataccama expression language uses lower case yyyy for year and lower case dd for date.\nThen, I take the first 4 chars of the text, to determine if it is smaller than 1940 or larger than 2050.\nleft(date_string,\n4\n)<\n'1940'\nor\nleft(date_string,\n4\n)>\n'2050'\nThis approach will be able to weed out any random chars that are not dates (the abcdefgh), and also date formats that are wrong (the 20252901 or 29012025)\nDoes this help?"
      },
      {
        "author": "TimBrown74",
        "timestamp": "3 months ago",
        "content": "I like your approach and will give it a try.\nBut let me share that I approached the problem by using subStr to separate the value into variables YYYY, MM and DD.  I actually did pretty well, but I could not get (YYYY < “1940”) or (YYYY  >  “2050”) to return correct answers, even if I split it into two separate conditions.  Would you know why (YYYY < “1940”) would not work?\nThanks for your time!"
      },
      {
        "author": "may_kwok",
        "timestamp": "3 months ago",
        "content": "Hi ​\n@TimBrown74\n,\nCould you give a screenshot similar to the one that I pasted? This way we can see all the configurations and usage of expressions?"
      },
      {
        "author": "TimBrown74",
        "timestamp": "3 months ago",
        "content": "Please see attachment."
      },
      {
        "author": "may_kwok",
        "timestamp": "3 months ago",
        "content": "Pasting the screenshot here for easier viewing:\nThe substr expression in Ataccama takes the first digit as position 0.\nSo, if you have string = ‘20250130’, your expression substr(field_4,1,4) will evaluate to ‘2\n0250\n130’ instead of ‘\n2025\n0130’.\nIf you wanted the first 4 chars and you wanted to use the substr expression, then it needs to be substr(field_4,0,4).\nIf you wanted to allow nulls, you can edit the rule so that:\nCondition 1 is that if it is empty, it is valid (instead of invalid). Then,\nCondition 2 is make sure the string can convert to a date. Then,\nCondition 3 is the year is within range.\nThen you can avoid all the logic like “month cannot be greater than 12”, 28 / 29 / 31 days, which can just be validated against the calendar.\nDoes this help?"
      },
      {
        "author": "TimBrown74",
        "timestamp": "3 months ago",
        "content": "Hi ​\n@may_kwok\n,\nI went back to implement the technique in your first recommendation using toDate and I was surprised when I could not get it to work.  Here are my details.  What did I miss?\nBy the way, is there any way to clean up the fields which I deleted?\nThanks!\nTim"
      }
    ]
  },
  {
    "title": "SQL Catalog Item Creation Issue",
    "url": "https://community.ataccama.com/data-quality-catalog-94/sql-catalog-item-creation-issue-1580",
    "question": {
      "author": "kellymremzilyb",
      "timestamp": "[No timestamp]",
      "content": "I am trying to create a SQL catalog item to filter out the inactive records.  When I create the SQL catalog item I get the following error message:\nSQL compilation error: Object 'DEV_SOURCE.SAPMDG.VTLFA1' does not exist or not authorized.\nI can see the data tab for this table.  I can run profiling/DQ insights tab so I’m not sure what is happening.  I have created the query with the schema name and without the schema name.\nShouldn’t the credentials be the same to create the SQL catalog item?"
    },
    "answers": [
      {
        "author": "C_R",
        "timestamp": "3 months ago",
        "content": "Could this be related to credential configuration? Maybe DBA can grant access?"
      },
      {
        "author": "kellymremzilyb",
        "timestamp": "3 months ago",
        "content": "I figured out the issue.  Snowflake requires quotes around the table name."
      }
    ]
  },
  {
    "title": "Introducing the end-to-end Data Quality Guide: Laying the foundation – understanding your data landscape",
    "url": "https://community.ataccama.com/data-quality-catalog-94/introducing-the-end-to-end-data-quality-guide-laying-the-foundation-understanding-your-data-landscape-1582",
    "question": {
      "author": "Cansu",
      "timestamp": "[No timestamp]",
      "content": "Can you truly trust your data?\nIf you or your team hesitates before answering, you’re not alone. Many organizations struggle with understanding what data they have, where it resides, and whether it’s reliable. Without proper documentation, data quickly becomes fragmented, outdated, or even misleading, and potentially leading to missed opportunities and poor decision-making.\nAnd that’s why we’re introducing our\nEnd-to-End Data Quality Series!\n🚀\nData is one of the most valuable assets for any organization, but without a strong quality framework, it can quickly become unreliable and untrustworthy. In this four-part series, we’ll walk through the essential steps of achieving\nhigh-quality, trustworthy data\n—from documenting your data landscape to preventing issues before they arise. Each post will break down a key stage of the framework and providing actionable insights.\nPlease do join in the discussion in the comments, let us know if you have followed any of these tips, or what helped you in your DQ journey 👇\nLet’s kick things off with the first step: understanding and documenting your data!\nData quality starts with knowing what you have.\nBefore you can trust and leverage your data, you need to document it—understanding where it resides, who owns it, and how it flows through your systems​. This process helps uncover sensitive data (like PII), establish ownership, and create a single source of truth.\nKey capabilities that support this include a\ndata catalog\n, which acts as a central hub for data discovery, and a\nbusiness glossary\n, which links technical metadata to business terms. Additionally,\ndata lineage (keep an eye on\nProduct Updates\nsoon for some exciting news🤫)\nprovides a visual map of how data moves through the organization, ensuring transparency and compliance​.\nBy investing in structured data documentation, organizations can set a solid foundation for quality and governance. But it’s not just about documentation—it’s about making data findable, understandable, and actionable.\nHow does your organization handle data discovery and ownership? Are there any challenges in achieving full transparency? Let’s discuss!"
    },
    "answers": []
  },
  {
    "title": "Send files to sftp location",
    "url": "https://community.ataccama.com/data-quality-catalog-94/send-files-to-sftp-location-1570",
    "question": {
      "author": "Ajeesh_G",
      "timestamp": "[No timestamp]",
      "content": "Hi,\nHow can I send a file to SFTP folder using below task item? I am using Desktop version.\nCan someone help me how to get the required fields from the below screenshot? I have the location where I want to drop the file."
    },
    "answers": [
      {
        "author": "Phil Holbrook",
        "timestamp": "3 months ago",
        "content": "Hi ​\n@Ajeesh_G\nYou should find all the information you need in the online documentation for the SFTP workflow step at\nhttps://docs.ataccama.com/runtime-server/latest/workflow-and-scheduler-reference/sftp-upload-file.html\nLet us know if anything isn’t clear or you have problems"
      }
    ]
  },
  {
    "title": "Validating Date",
    "url": "https://community.ataccama.com/data-quality-catalog-94/validating-date-1574",
    "question": {
      "author": "Bhuvaneshwari Balasubramaniam",
      "timestamp": "[No timestamp]",
      "content": "Hi,\nI am trying to validate the date given between 100 years ago and 15 years ago are valid , everything else is invalid. I am able to achieve this validation in YEAR part. I am looking for validating complete date instead of only Year part.\nBelow are few screenshots."
    },
    "answers": [
      {
        "author": "AKislyakov",
        "timestamp": "3 months ago",
        "content": "AI gives pretty good solution ​\n@Bhuvaneshwari Balasubramaniam\nAlthough if you need pushdown-enabled version, you’ll have to rework it a bit"
      },
      {
        "author": "Bhuvaneshwari Balasubramaniam",
        "timestamp": "3 months ago",
        "content": "Thank you for the solution. It worked well for my problem."
      }
    ]
  },
  {
    "title": "Policy Creation/Update Date",
    "url": "https://community.ataccama.com/data-quality-catalog-94/policy-creation-update-date-1568",
    "question": {
      "author": "RAZORX",
      "timestamp": "[No timestamp]",
      "content": "Hi Community,\nIs it possible to fetch the policy creation or last update date for Policies, in order to make a DQ rule around it in Ataccama One? My use case is to build a DQ rule which will notify the stakeholders based on a timeframe after which the policy has been created or last updated, but unfortunately i am not seeing any property to fetch the Created or Last Update date."
    },
    "answers": [
      {
        "author": "anna.spakova",
        "timestamp": "3 months ago",
        "content": "Hello ​\n@RAZORX\n,\nif I understand correctly what you are asking, you would like to check the Created/Update dates for the Policies - the entities in Ataccama? If so, you cannot use the DQ rules, because DQ rules are applied on the data coming from the Data sources connected to Ataccama. Policy in this case is just a metadata entity in the platform, so it cannot be processed by DQ rules.\nBut you can monitor the dates using the orchestration server - you can obtain the dates using API and run the check in the ONE Desktop plan and workflow.\nPlease let me know if this answers your question.\nKind regards,\nAnna"
      },
      {
        "author": "RAZORX",
        "timestamp": "3 months ago",
        "content": "Perfect, thanks ​\n@anna.spakova\nfor the clarification. It indeed answers my question."
      }
    ]
  },
  {
    "title": "#3 Business Terms",
    "url": "https://community.ataccama.com/data-quality-catalog-94/3-business-terms-313",
    "question": {
      "author": "Cansu",
      "timestamp": "[No timestamp]",
      "content": "Glossary terms are used to add more metadata to different assets in the Ataccama ONE, they are necessary for data quality evaluation of Catalog Items after profiling.\n👉\nGet Certified with Ataccama article series\n#1 Adding Data Sources\n#2 Add a Data Source to the Catalog\n🔎\nLET’S TEST YOUR SKILLS\nUsing the ONE Web App, in the ONE Business Glossary module, create business terms for '<your_name>_Blood Type', '<your_name>_Rhesus Status', '<your_name>_Blood Group'.\nUsing the ONE Web App, in the ONE Business Glossary module, configure the business terms '<your_name>_Blood Type' and '<your_name>_Rhesus Status' as child terms of the parent term '<your_name>_Blood Group'.\n💡\nHow to do it?\nNavigate to\nBusiness Glossary\nin the left navigation menu and click on\nCreate\nin the top right menu\nSelect Type -\nBusiness Term\n, fill in\nName\nand\nSave\nYou will see that your business term has been successfully created, repeat the same process for remaining business terms\nOnce you have all terms created, open of them and click on\nSend for Approval\nFill in description and confirm with\nCreate a new request\nAdd remaining business terms to this request -\nAppend to existing request\n-\nAppend\nWith the last one, confirm the request with\nApprove & Publish\nCheck that your business terms are set\nOpen\n<yourname>_Blood Group\nand in section\nRelations\nselect\nParent of\nAdd\n<yourname>_Blood Type\nand\n<yourname>_Rhesus Status\nterms\nDo you have any question or tips&tricks? Let us know in comments section!"
    },
    "answers": [
      {
        "author": "thomas.prociuk",
        "timestamp": "2 years ago",
        "content": "Hi,\nIs there any way to bulk upload, or bulk import new custom business terms?"
      },
      {
        "author": "Cansu",
        "timestamp": "2 years ago",
        "content": "Hi\n@thomas.prociuk\n, it’s possible to import business terms to Ataccama ONE using ONE Desktop, you can find instructions on how to import terms\nin this article.\nPlease be sure to create the relationships between terms in the process which is done in a similar way to the create term step."
      },
      {
        "author": "thomas.prociuk",
        "timestamp": "2 years ago",
        "content": "Hi\n@Cansu\nthank you for the quick response, but it says that I do not have the rights to view the page. How should I get access? Thanks"
      },
      {
        "author": "Cansu",
        "timestamp": "2 years ago",
        "content": "Please login with your Ataccama credentials to view our documentation page, let me know if you are having any issues\n@thomas.prociuk\n🙋‍♀️\nhttps://support.ataccama.com/"
      },
      {
        "author": "thomas.prociuk",
        "timestamp": "2 years ago",
        "content": "@Cansu\nI am logged in and it is still saying access is denied. Is there anyway that I can get it as a pdf?"
      },
      {
        "author": "Cansu",
        "timestamp": "2 years ago",
        "content": "@thomas.prociuk\ncould you please try\nthis link?\nI’ll try to work out an alternative solution it this doesn’t work."
      },
      {
        "author": "thomas.prociuk",
        "timestamp": "2 years ago",
        "content": "@Cansu\nThat link worked! Thank you so much!"
      },
      {
        "author": "kellymremzilyb",
        "timestamp": "3 months ago",
        "content": "​\n@Cansu\nthat link seems to be broken.  It takes me to the main page of the help documentation and not a specific page.  Is that correct?"
      },
      {
        "author": "Cansu",
        "timestamp": "3 months ago",
        "content": "Hi ​\n@kellymremzilyb\n, welcome to the community and thank you for posting! Last year\nwe changed our documentation portal\nso unfortunately some of our documentation links are now unavailable. You can access these in PDF format at\nhttps://support.ataccama.com/downloads\nby using your existing credentials.\nPlease let me know if this helps!"
      }
    ]
  },
  {
    "title": "Missing input attributes for DQ rule",
    "url": "https://community.ataccama.com/data-quality-catalog-94/missing-input-attributes-for-dq-rule-1554",
    "question": {
      "author": "JTH",
      "timestamp": "[No timestamp]",
      "content": "Hi,\nIn one of my Monitoring Projects I am apparently missing input attributes for a DQ rules which doesn't appear anymore in my MP. The error, however, is still there. How can I get rid of this?"
    },
    "answers": [
      {
        "author": "Cansu",
        "timestamp": "3 months ago",
        "content": "Hi ​\n@JTH\n, apologies for my delayed reply here. Could you please create a support ticket for this? Looks like a bug and our team will get in touch as soon as possible."
      },
      {
        "author": "JTH",
        "timestamp": "3 months ago",
        "content": "​\n@Cansu\ndone! Thank you 😄"
      },
      {
        "author": "Cansu",
        "timestamp": "3 months ago",
        "content": "Anytime! Thank you for the feedback 💪"
      }
    ]
  },
  {
    "title": "Date Field Format Data Quality Check",
    "url": "https://community.ataccama.com/data-quality-catalog-94/date-field-format-data-quality-check-1494",
    "question": {
      "author": "evan.sasowsky",
      "timestamp": "[No timestamp]",
      "content": "Hey y’all,\nI was wondering if anyone had a quality rule or advice on how to construct a quality rule to check a date type field for formatting. I know there is the “matches” function for strings and that you can convert a date field to a string using “toString”, but that requires you to specify a format, which invalidates the point of the rule.\nThe desired format is “MM-dd-yyyy”, as shown in what I’ve got so far.\nmatches(\"^\\d{4}-(0[1-9]|1[0-2])-(0[1-9]|[12][0-9]|3[01])$\",toString(ATTRIBUTE_1, \"MM-dd-yyyy\"))"
    },
    "answers": [
      {
        "author": "anna.spakova",
        "timestamp": "6 months ago",
        "content": "Hi ​\n@evan.sasowsky\n,\nlet me just comment regarding the data type. In case your date is stored in a date or datetime field, it doesn’t make sense to perform a format check because the formatting is based on the settings of your database and Ataccama has also some specific format how it displays the dates in the web application. But it is all just the visualization of the dates. So the format check makes sense only in case the dates are stored in string data types.\nKind regards,\nAnna"
      },
      {
        "author": "Cansu",
        "timestamp": "4 months ago",
        "content": "Hi ​\n@evan.sasowsky\n, I’m closing this thread for now, if you have any follow-up questions please feel free to share them in the comments or create a new post🙋🏻‍♀️"
      },
      {
        "author": "evan.sasowsky",
        "timestamp": "4 months ago",
        "content": "Hey ​\n@anna.spakova\n,\nThat makes sense! I appreciate the explanation.\nI guess the only exception to a specified format would be if you were bringing in a csv or and xlsx file. Would Ataccama just assume a string format for those?"
      },
      {
        "author": "anna.spakova",
        "timestamp": "3 months ago",
        "content": "Hi ​\n@evan.sasowsky\n, yes, CSV and Excel files import all fields as STRING so in those cases it makes sense to have the date format check.\nKind regards,\nAnna"
      }
    ]
  },
  {
    "title": "Custom Entity Occurence",
    "url": "https://community.ataccama.com/data-quality-catalog-94/custom-entity-occurence-1553",
    "question": {
      "author": "RAZORX",
      "timestamp": "[No timestamp]",
      "content": "Hi Community,\nI have created a custom entity with parent type as Term as “Class”. Now when i linked it to the catalog items and attributes using Reference Array writer its showing in my Custom Class section in the attributes as well as in the Catalog Item. But unfortunately i am not able to see it in the occurrence tab of this Custom Entity. Is there any way to add some properties in the Occurrence tab so that i can see the link for custom reference arrays. Any help is highly appreciated."
    },
    "answers": [
      {
        "author": "Albert de Ruiter",
        "timestamp": "4 months ago",
        "content": "Hi ​\n@RAZORX\n,\nWe have also created custom entities with\nterm\nas parent, but we can see the linked catalog item attributes in the occurrence tab.\nI am not sure what you mean with the Reference Array writer. Do you use a Desktop plan to connect the catalog item attributes to the custom term entity? If so, do you use a Metadata Writer to write to the termInstance entity for that purpose?\nWhen you connect a catalog item attribute manually to a term instance in the data catalog, then do you see it reflected in the occurrence tab?\nKind regards,\nAlbert"
      },
      {
        "author": "RAZORX",
        "timestamp": "4 months ago",
        "content": "Hi Albert,\nWhat i meant was, i haven’t added this custom term in the glossary section of linking in catalog items, i have created a custom section like policy and there i have added it. Now when i am adding it in the glossary section as a term, i can see it from the occurrence tab but when i am adding it in a separate section i am not able to see it like in the screenshot. Any help on this?"
      },
      {
        "author": "Albert de Ruiter",
        "timestamp": "4 months ago",
        "content": "Hi ​\n@RAZORX\n,\nIt looks like I cannot reproduce this from my end. So Classification is a new section in the left-bar menu and it contains the instances of the new custom entity Class based on term. I have a similar kind of example on my end, but after clicking on the instance (so Confidential in your example) I can see the Occurrence tab, also with the linked catalog item attribute."
      },
      {
        "author": "SamWrigley",
        "timestamp": "4 months ago",
        "content": "Hi ​\n@RAZORX\n,\nI experimented with something similar in the past, I believe the issue for you could be the referenced entity.\nIf the object you are referencing is Class, you would need to change it to termInstance.\nThe downside of doing this is all terms will now be selectable in this property.\nThere is a way this can be reduced by using a list of values that references the terms you want selectable.\nI don’t have this in my build anymore to be certain this will show in the occurrence tab. In theory it should, provided the termInstance is used as the reference."
      },
      {
        "author": "Cansu",
        "timestamp": "3 months ago",
        "content": "Hi ​\n@RAZORX\n, could you please let us know whether any solutions from ​\n@Albert de Ruiter\nor ​\n@SamWrigley\nhelped here or if you have any follow up questions?"
      }
    ]
  },
  {
    "title": "Retrive KeyCloak user information using Ataccama One plan",
    "url": "https://community.ataccama.com/data-quality-catalog-94/retrive-keycloak-user-information-using-ataccama-one-plan-929",
    "question": {
      "author": "pdanpoonkij",
      "timestamp": "[No timestamp]",
      "content": "Hello community, I need to retrieve KeyCloak user information (e.g., email address) based on their\nusername\nusing Ataccama One Desktop. Has anyone done this before? If so, can you please share your experience or point me to some documentation?\nThank you in advance for your help!"
    },
    "answers": [
      {
        "author": "pdanpoonkij",
        "timestamp": "1 year ago",
        "content": "Hello community,\nI was able to find a simple and effective workaround to retrieve KeyCloak user information by utilizing the ONE Metadata Reader. This method allows you to extract all metadata stored within Ataccama One, including user details from Keycloak."
      },
      {
        "author": "Cansu",
        "timestamp": "1 year ago",
        "content": "Hi\n@pdanpoonkij\n, great to hear that and thank you so much for taking the time to share your solution! 🙋🏻‍♀️"
      },
      {
        "author": "Joeri",
        "timestamp": "4 months ago",
        "content": "Hi ​\n@pdanpoonkij\nand ​\n@Cansu\n. Where do you define the server then? And how?"
      },
      {
        "author": "may_kwok",
        "timestamp": "3 months ago",
        "content": "Hey ​\n@Joeri\n,\nKeycloak contains user information, and Ataccama connects to Keycloak and holds its own set of users.\nKeycloak itself does have REST API to allow you to retrieve data, but I think ​\n@pdanpoonkij\n‘s solution is to query Ataccama ONE’s copy instead, connecting to ONE Platform.\nFor me if I were implementing this, I’d want to ensure Keycloak and Ataccama are synced, before querying Ataccama.\nhttps://docs.ataccama.com/one/latest/user-access-management/users.html#synchronize-changes-with-iam-system:~:text=the%20changes.-,Synchronize%20changes%20with%20IAM%20system,-To%20synchronize%20changes"
      }
    ]
  },
  {
    "title": "How to create a metric with decimal places in Data Story?",
    "url": "https://community.ataccama.com/data-quality-catalog-94/how-to-create-a-metric-with-decimal-places-in-data-story-1555",
    "question": {
      "author": "GastonQ",
      "timestamp": "[No timestamp]",
      "content": "Hi, I’m working with data stories, and I’ve created a custom metric to represent by using percentage format.\nThis is the metric expression:\niff(sum(\n\"Valid\"\n) + sum(\n\"Invalid\"\n) >\n0\n, round(sum(\n\"Valid\"\n) *\n100\n/ (sum(\n\"Valid\"\n) + sum(\n\"Invalid\"\n)),\n2\n) /\n100\n,\n0\n)\nBoth “Valid” and “Invalid” are defined in Snowflake as REAL datatype.\nI’ve read this article\nCreate Custom Attributes :: Ataccama ONE\nand I’m using round function, but it doesn’t work.\nThanks in advance,\nGaston"
    },
    "answers": [
      {
        "author": "GastonQ",
        "timestamp": "4 months ago",
        "content": "Solution\n: if you want to control the decimal places...do not use “round” 😕\n​​​​​​​iff(sum(\"Valid\") + sum(\"Invalid\") > 0, sum(\"Valid\") * 100/ (sum(\"Valid\") + sum(\"Invalid\")) / 100, 0)\nI think this is a bug, because if I need to display 3 or more decimal places...I have to use round."
      }
    ]
  },
  {
    "title": "Updating Terms and Terms Relationships from One Desktop in Bulk",
    "url": "https://community.ataccama.com/data-quality-catalog-94/updating-terms-and-terms-relationships-from-one-desktop-in-bulk-1552",
    "question": {
      "author": "RAZORX",
      "timestamp": "[No timestamp]",
      "content": "Hi Team,\nIs there a way to update the existing Term name or description or any property like termInstance from One Desktop metadata writer? I can only see option to add or delete but no update option. Any idea is highly appreciated."
    },
    "answers": [
      {
        "author": "Albert de Ruiter",
        "timestamp": "4 months ago",
        "content": "Hi ​\n@RAZORX\n,\nWe have Desktop plans that update term details. Following screenprints may be of help.\nAnd with the value of ‘Id Column Name’ the properties can be updated.\nKind regards,\nAlbert"
      },
      {
        "author": "RAZORX",
        "timestamp": "4 months ago",
        "content": "Perfect Thank you very much ​\n@Albert de Ruiter\n. But this attributeTerm is it a Out of the box entity? I am not sure if its a out of the box entity as its not present in my one desktop."
      }
    ]
  },
  {
    "title": "Assigning Business Terms To Custom Folders",
    "url": "https://community.ataccama.com/data-quality-catalog-94/assigning-business-terms-to-custom-folders-1537",
    "question": {
      "author": "RAZORX",
      "timestamp": "[No timestamp]",
      "content": "Hi Team,\nI am trying to add my Business Terms in two separate child folders inside my Business Term Folder(Not Data Domains). Just wanted to know if its possible or not?\nI have created two child folders inside from navigation in settings\nBusiness Glossary→ Terms →  Business Terms → Folder1\nBusiness Glossary→ Terms →  Business Terms → Folder2\nBut unfortunately i am not sure how to add Business Terms in those two folders specifically, the normal Business Terms are added directly in the parent folders. Do i need to do some changes in the metadata model for this one or need to create any AQL filter for this folders(Same as Datadomains). Any tips or help is highly appreciated."
    },
    "answers": [
      {
        "author": "Albert de Ruiter",
        "timestamp": "5 months ago",
        "content": "Hi ​\n@RAZORX\n,\nI can think of two approaches here.\nVia the Navigation pane you can create a business term folder 1 and a business term folder 2. To both folders applies that they relate to node term. Following, in the actual list screens, you can apply a filter in order to distinguish between folder 1 and 2 type of terms.\nIn case to both type of terms many different properties apply, you can also create a new term type in the metadata model. The create a new folder for this new term type.\nIf you would require a somewhat more detailed explanation please let me know.\nKind regards,\nAlbert"
      },
      {
        "author": "RAZORX",
        "timestamp": "5 months ago",
        "content": "Hi ​\n@Albert de Ruiter\n,\nI tried to create using the Navigation Pane, but when putting the AQL expression for the filter i am getting some error “One or more of the referenced objects in the implicit AQL expression does not exist. ”.\nI have use the AQL expression in this syntax\nname is \"Business term Name(Parent)\" OR @termRelationship(target).any(type.$id = 'Type ID' AND source.$id = 'TermID')\nWhere Type ID is the  relation type id and TermID is the ID of the First BT or Parent BT am i doing it in a right way?\nIs there any way to find out the detailed error logs ?"
      },
      {
        "author": "Albert de Ruiter",
        "timestamp": "5 months ago",
        "content": "Hi ​\n@RAZORX\n,\nUnfortunately I have not much experience with using AQL expressions.\nBut a good approach would be to breakdown the expression and check when it fails.\nThe page that you created applies to term right? So name is \"Business term Name(Parent)\" is a hardcoded filter on a term literally named \"Business term Name(Parent)\"? Or was this meant as a placeholder for actual term names?\nWith @termRelationship(target).any you intend to filter terms that are referenced by node\ntermRelationship\nvia property\ntarget.\nCorrect? And then filtered by type.$id = 'Type ID' AND source.$id = 'TermID'. Should that not simply be type = 'Type ID' AND source = 'TermID', because the type and source properties already contain ID values? I believe type.$id would rather mean the ID of an instance of node type, which means something else.\nI believe no logs are being created while defining an expression.\nHope this is of any help after all.\nKind regards,\nAlbert"
      },
      {
        "author": "Cansu",
        "timestamp": "4 months ago",
        "content": "Hi ​\n@RAZORX\n, may I ask if any of the suggestions from Albert helped here? If you have any follow up questions please do let us know."
      },
      {
        "author": "RAZORX",
        "timestamp": "4 months ago",
        "content": "Hi ​\n@Cansu\n,\nYes indeed it helped, thanks a lot ​\n@Albert de Ruiter\nfor helping."
      },
      {
        "author": "Albert de Ruiter",
        "timestamp": "4 months ago",
        "content": "You're welcome ​\n@RAZORX\n!"
      }
    ]
  },
  {
    "title": "Building a nested GraphQL API call in ONE Desktop",
    "url": "https://community.ataccama.com/data-quality-catalog-94/building-a-nested-graphql-api-call-in-one-desktop-987",
    "question": {
      "author": "Phil Holbrook",
      "timestamp": "[No timestamp]",
      "content": "ONE Desktop provides two ways to access the ONE platform data. Usually, it is easiest to use the Metadata Reader step, but sometimes you need a more complex query, or perhaps you are developing a Json API call to be used from another application entirely.  In this case, you will want to start from a Json Call step.\nThe plan I will describe is this:\nI’ve used a random record generator to “inject” a single row into the data flow to get things started because the Json Call executes once for each row on its input.\nThe “out” output from the Json Call carries all columns from the input; and can also be used to carry connection error messages. I’ve not set that up, so I just sent all that to Trash.\nI’ve named each of the remaining outputs using a sequential numbered prefix.  This manipulates the automatic alphanumeric sorting of the output connections so that the ordering is more logical than a list of element names would be. I’ve also set the layout of the step to horizontal so I can read the labels.\nNever try to write a plan like this all in one go! You’ll need to start at the top level and work down getting each stage to output the correct data to file. You also need to have an active data source set up as a Generic server:\nGeneric Connection for API\nThe objective of this plan is to output a summary of the defined rules and the associated terms; and to list the inputs and conditions for each rule.\nStarting at the top level, we need to explore the Rules collection.\nLook at the ONE Metadata Explorer, making sure the presentation is set to Metadata Model:\nScroll down to the entry for\nrule\nNotice that the entity is “rule” but the metadata holds many of them, so we actually request a collection called “rules.”  Each of the attributes is listed with its type. Some have a simple base datatype like string, others refer to different metadata entities.\nThe attribute\nataccamaGen1RuleUid\nis a custom attribute, used for a customer as part of their migration from our Gen 1 product to Gen 2. Any custom attributes defined in this way through the ONE WebApp metadata editor will show up in ONE Desktop.\nFor a first try, we can set the query string in the Json Call to bring back the rules as the top level data stream:\nImportant\n: Notice that the Input Template uses a string value that contains the query.  As the query gets more complicated it really helps to use proper indentation,\nbut\nthe query string must not include [TAB] characters - the call will simply return no data.  Instead, I have used two spaces for each level of indentation.  If you prototype the query in Playground then it will insert tab characters automatically, which need to be replaced with spaces.\nFor this project, I happen to be interested only in the published version of each rule, but I do want to know whether a draft version exists.\nThe first line of the GraphQL query selects the root entity collection, instructs the engine to include draft versions and sets up the standard specification for an array of values in GraphQL:\nedges { node {\nEssentially,\nedges\ndelimits the start of an array, and\nnode\nmarks a row. The structure of the query matches the structure of the result.\nEvery entity has a unique gid assigned, for use as a primary key.\ndraftVersion\ncontains all the fields of\npublishedVersion\n, but the only value I want is\n_draftType\n.\nThe value of the\n_draftType\nfield may be:\nCHANGE\nDraft proposal to change an existing object.\nDELETE\nDraft proposal to delete an existing object.\nNEW\nDraft proposal to create a new object.\nRESURRECT\nDraft proposal to resurrect already deleted object.\nVIRTUAL\nPlaceholder for a non-existent draft. This represents a published version.\nWhen the\n_draftType\nvalue is VIRTUAL then there is no active draft.\nFinally, we have the\npublishedVersion\nsection, just listing some simple attributes for the rule.\nTo read the returned Json response, we set up the attributes for the root level data stream, defining the base path for the Data Stream to be $.data.rules.edges.  Each Attribute is then populated based on the path relative to that of the stream.\nYou can use whatever name you like for each attribute column, but as the flow logic gets more complex it’s a good idea to use some sort of prefixing standard, like “rule_” here.\nThat is enough to have a working plan:\nThis is a good time to run the plan and make sure the expected output is written to file.  If things aren’t working, it’s worth setting up request and response debug files in the Advanced tab:\nNext, I want to look into the implementation of each rule. In the Metadata Explorer,\nrule\nhas an attribute\nimplementation\nwhich is of type\nruleImplementation.\nThis is a singular noun – there is only one\nimplementation\nfor any given\nrule\n– so we don’t need to iterate through an array of values, and we can include the contents of the\nimplementation\nin the [1 – rules] output.\nThe entry for the\nimplementation\nattribute has a blue rectangle ‘bullet’:\nIf we look at the\nruleImplementation\nentity, we see this has a set of entries with blue circle bullets:\nThese are not attributes. Instead, this means that the\nruleImplementation\nmay be of any of these types. We can identify the type for each\nimplementation\nwith the\ntype\nsystem property, which is present on all entities. We can add a filter to the query to restrict it to return only DQ rules.\nType\nis a system property and is represented in the query syntax as\n$type\n. The filter uses quotation marks, which have to be escaped since they have to be contained in a Json string value, and is added to the conditions for the rules element, so that it filters all rules by their implementation type:\n{\n\"query\"\n:\n\"query q {\nrules (versionSelector: {draftVersion: true} filter: \\\"implementation.$type =='ruleDqImplementation'\\\") \n{...} }\"\n}\nThe attributes for the\nruleDqImplementation\nentity are:\nExpanding the Json call to include these changes gives:\nIn the ONE Metadata Explorer we can see each rule contains a collection of\ntermInstances\n. The attribute name is plural so we know it is an array, and the individual rows are of type\ntermInstance\n.\ntermInstance\nhas several subtypes, but we’re not really interested in those. We just want to link through to the term entity, which is referenced by the attribute\ntarget\n:\nThe definition for\nterm\nis:\nWe can extend the GraphQL query and add a new data stream:\nHide content\nShow content\n{\"\nquery\n\":\n\"query q {\nrules (versionSelector: {draftVersion: true} filter: \\\"implementation.$type =='ruleDqImplementation'\\\") {edges {node {\n  gid draftVersion { _draftType }\n  publishedVersion {\n    name description ruleDefinitionSource\n    implementation {\n      gid type\n      publishedVersion {\n        aggregationRule\n        component\n        }\n      }\n    termInstances {edges {node {\n      gid\n      publishedVersion {\n        displayName\n        target {\n          gid type\n          publishedVersion {\n            name\n            abbreviation\n            synonym\n            termDefinitionSource\n            pushdownEnabled\n            }\n          }\n        }\n      }\n  }\n  }}}\n}\"\n}\nThe attributes for a\ntermInstance\nare based at each node of the\ntermInstances\ncollection, which in turn are relative to each node of the rules collection.\nNow that we have a child data stream for\ntermInstances\n, we will need to be able to link it back to the corresponding rules. This can be accomplished in three ways – all supported here.\nFirst, we can use a Record Descriptor. This is a standard approach to grouping records in Ataccama solutions. It contains a key value, a count of grouped terms, and an ordinal position in the group, separated by colons. In this case, the key value will be the value referenced in the Parent Id Column field. Since the parent data stream is\n[1 – rules]\nand the id field value is stored into the column\nrule_id\n, this value is represented as\n[1 – rules].rule_id\n.  For example, if a particular rule has two\ntermInstances\n, then the values of\nrd_termInstance\nmight be,\n15bbce34-0000-7000-0000-0000001676a5:2:1\n15bbce34-0000-7000-0000-0000001676a5:2:2\nThere is a dedicated Record Descriptor Join step that can be used to join rows with a record descriptor to their parent rows based on this key value.\nThe column\nrd_termInstance\nis declared in the Shadow Columns tab:\nThis figure also shows the second join approach: defined here is a rule_id column which is again assigned to the value of the parent key using the same expression,\n[1 – Rules].rule_id\n. The two data streams can be joined directly on this key using a Join step.\nAll Shadow Columns defined for a given data stream will be emitted as columns on that stream output. This technique can be used to reproduce any data from the parent stream onto the child, effectively a third way to perform a join within the Json Call step itself.\nThe plan now looks like this:\nThe plan can be extended in the same way to Conditions:\nand to inputGroups:\nThis completes the definition of the output Data Streams, and the full GraphQL query template is below\nHide content\nShow content\n{\"\nquery\n\":\n\"query q {\nrules (versionSelector: {draftVersion: true}) {edges {node {\n  gid draftVersion { _draftType }\n  publishedVersion {\n    name description ruleDefinitionSource\n    implementation {\n      gid type\n      publishedVersion {\n        fallback { publishedVersion {result {gid publishedVersion {name} }} }\n        aggregationRule\n        component\n        conditions {edges {node {\n          gid type\n          publishedVersion {\n            name\n            order\n            aggregated\n            score\n            disabled\n            expression {\n              publishedVersion {\n                useFriendly\n                stringExpression\n                friendlyExpression\n                }\n              }\n            }\n          }}}\n        dqDimension {\n          publishedVersion {\n            name\n            }\n          }\n        }\n      }\n    termInstances {edges {node {\n      gid\n      publishedVersion {\n        displayName\n        target {\n          gid type\n          publishedVersion {\n            name\n            abbreviation\n            synonym\n            termDefinitionSource\n            pushdownEnabled\n            }\n          }\n        }\n      }}}\n    inputGroups {edges {node {\n      gid\n      publishedVersion {\n        name\n        order\n        inputs {edges {node {\n          gid\n          publishedVersion {\n            name\n            order\n            description\n            dataType\n            }\n          }}}\n        }\n      }}}\n    }\n  }}}\n}\"\n}"
    },
    "answers": [
      {
        "author": "nitish2507",
        "timestamp": "11 months ago",
        "content": "Hi\n@Phil Holbrook\n- Would be possible to take a similar approach to author (add) new DQ rules to Ataccama?\nIs that something that the GraphQL layer supports?\nI am looking for options to programmatically update DQ rules."
      },
      {
        "author": "Antonio DeChausay",
        "timestamp": "11 months ago",
        "content": "Hi\n@nitish2507\n, creating rules using GraphQL queries is possible, but there is quite a lot of complexity and customisation available. Due to this complexity - updating the ‘implementation’ of a DQ rule should probably be done through the UI rather than through GraphQL.\nHere’s a query that creates new rules:\nmutation CreateEntity_rule {\n  ruleCreate(\n    parentNode:\n\"metadata\"\nparentGid:\n\"00000000-0000-0000-0000-000000000001\"\nparentProperty:\n\"rules\"\nnew\n: {\n    name:\n\"test_rule_graphql\"\n,\n    description:\n\"[{\\\"type\\\":\\\"paragraph\\\",\\\"children\\\":[{\\\"text\\\":\\\"description -- could be anything\\\"}]}]\"\n,\n    ruleDefinitionSource:\nnull\n,\n    _type:\n\"rule\"\n}\n  ) {\n    success\n  }}\nQuery to update rule properties where ‘gid’ is the ID of the rule:\nmutation SaveEntity_rule {\n  ruleUpdate(gid:\n\"04c18bff-0000-7000-0000-000000120c55\"\n, patch: {\n    name: {\n      kind: UPDATE,\n      old:\n\"test_rule_graphql\"\n,\nnew\n:\n\"test_rule_graphql_update\"\n},\n    description: {\n      kind: UPDATE,\n      old:\n\"[{\\\"type\\\":\\\"paragraph\\\",\\\"children\\\":[{\\\"text\\\":\\\"description -- could be anything\\\"}]}]\"\n,\nnew\n:\n\"[{\\\"type\\\":\\\"paragraph\\\",\\\"children\\\":[{\\\"text\\\":\\\"description -- could be anything_ update\\\"}]}]\"\n}\n  }) {\n  success\n  }\n}\nThese queries can be run as-is through the playground by adding\n/playground/\nto the URL line of the page, make sure the HTTP Headers are set beforehand -\nhttps://docs.ataccama.com/one/latest/one-apis/one-api.html#http-headers"
      },
      {
        "author": "greglvaughan",
        "timestamp": "9 months ago",
        "content": "Solved!~  tabs were in my query.. doh!  sorry for the red herring!\nHi,\nI can’t get any of these JSON calls to work.  I can remove the query from the request file and it works but when I try and run this from ONE Desktop I get the following error in the response file.  Is this a permissions issue? We have been unable to get introspection to work as well but I am not sure if that is related.\nthanks ,\nGreg\nHTTP/? 400\nX-CORRELATION-ID: 4c89d2\nX-MMD-MODEL-VERSION: 5029\nX-FRAME-OPTIONS: deny\nCONNECTION: keep-alive\nSTRICT-TRANSPORT-SECURITY: max-age=15768000; includeSubDomains\nDATE: Thu, 15 Aug 2024 16:09:14 GMT\nX-XSS-PROTECTION: 1; mode=block\nSERVER: nginx\nREFERRER-POLICY: strict-origin\nX-PERMITTED-CROSS-DOMAIN-POLICIES: none\nX-CONTENT-TYPE-OPTIONS: nosniff\nVARY: Origin\nVARY: Access-Control-Request-Method\nVARY: Access-Control-Request-Headers\nCONTENT-LENGTH: 0"
      },
      {
        "author": "Cansu",
        "timestamp": "9 months ago",
        "content": "Great to hear that all is good now\n@greglvaughan\n! Thanks for coming back to let us know 🙌"
      },
      {
        "author": "nitish2507",
        "timestamp": "8 months ago",
        "content": "@Antonio DeChausay\n- Thanks for your guidance.\nWhile I am able to update rules with the method above, I don’t seem to be able to create new rules.\nThe API is a success but I do not see the rule when I check the app or when I pull the count of rules using graphql.\nCan you help me understand what could be causing this?"
      },
      {
        "author": "Antonio DeChausay",
        "timestamp": "8 months ago",
        "content": "Hi\n@nitish2507\nCan you share with me what response you are getting when running the graphql query?\nIf running through the playground can you share the entire response shown in the UI, and if running through a Json Call step can you share the response file that’s generated?"
      },
      {
        "author": "nitish2507",
        "timestamp": "8 months ago",
        "content": "​​​\n@Antonio DeChausay\n- The above is for create. Similar for update based on the query shared earlier.\nI can confirm that the update works."
      },
      {
        "author": "nitish2507",
        "timestamp": "8 months ago",
        "content": "Hi\n@Antonio DeChausay\n- Updates from our side. We found on more investigation that the rules are getting created.\nThere are two things though\nBy default the rules are unpublished, how can we publish them during creation?\nBy default the rules are only visible to the admins and the owner, how can we share to a pre-defined group?\nThe challenge is that when we create the rule, we do not even have a gid in the response which we could use to make two other calls for publishing and sharing as per the docs.\nCan you pls help on this?"
      },
      {
        "author": "nitish2507",
        "timestamp": "4 months ago",
        "content": "Hi ​\n@Antonio DeChausay\nand ​\n@Phil Holbrook\n- How can i pass the input that i have in input groups to a friendly expression dynamically?"
      },
      {
        "author": "Phil Holbrook",
        "timestamp": "4 months ago",
        "content": "Hi ​\n@nitish2507\n- You’re really a long way off topic for this thread and I’m not sure what you’re trying to ask.  It would probably be best if you create a new thread with the exact context for your question."
      }
    ]
  },
  {
    "title": "Uniqueness DQ Evaluation rule",
    "url": "https://community.ataccama.com/data-quality-catalog-94/uniqueness-dq-evaluation-rule-1499",
    "question": {
      "author": "Bhuvi",
      "timestamp": "[No timestamp]",
      "content": "I am currently using 14.4.0.one-44751-landing-pa-230616-287787-f83b54a4 ONE web application version. While creating a Uniqueness Rule I am not getting WHEN condition as “is unique” and “is not unique” . Could someone help me understand why I am not getting the uniqueness condition  instead Validity condition."
    },
    "answers": [
      {
        "author": "anna.spakova",
        "timestamp": "6 months ago",
        "content": "Hi ​\n@Bhuvi\n,\ncan you check if you set up the rule type as Aggregation rule?\nWithout that you cannot see the “is unique” option as it requires aggregation rule.\nKind regards,\nAnna"
      },
      {
        "author": "Cansu",
        "timestamp": "4 months ago",
        "content": "Hi ​\n@Bhuvi\n, I’m closing this thread for now, if you have any follow-up questions please share them in the comments or create a new post🙋🏻‍♀️"
      }
    ]
  },
  {
    "title": "📚 Guide: How to run GraphQL requests - Part I",
    "url": "https://community.ataccama.com/data-quality-catalog-94/guide-how-to-run-graphql-requests-part-i-1543",
    "question": {
      "author": "OGordon100",
      "timestamp": "[No timestamp]",
      "content": "Hi community 👋\nEver wondered about the best ways to authenticate GraphQL requests while maintaining security and efficiency?\nWhether you’re new to Ataccama ONE or looking to optimize your workflows, here’s an approachable guide to using Service Principle and various authentication methods.\nIn this first post of two part GraphQL series, I will share authentication types & which one you should use and how to authenticate GraphQL requests. Second part will be on how to work with MMM in GraphQL - so watch out this space for that!\nLet’s get into it👇\n⚙️\nAPI authentication\nUser Accounts vs Service Principles\nUser accounts and service principals serve different purposes in identity and access management.\nUser accounts are designed for (unsurprisingly!) human users, providing them with access to applications and resources based on their roles and permissions. These accounts typically require interactive login with credentials like usernames and passwords.\nService Principals, on the other hand, are essentially user accounts where the user is a program. They are ideal for scenarios where automated processes need to interact with APIs or other services without relying on human intervention. These continue to work when people leave a company, and reduce security attack vectors when properly set up with only the minimal permissions required for its exact task (instead of for everything an end user requires), and can be monitored as such.\nFor GraphQL, DPM runtimeConfig connections, etc, it is therefore best practice to use a Service Principle.\nPractical Example: Making a Service Principal 🚀\nTo set up a Service Principal in Ataccama ONE:\nGo to the Keycloak realm, navigate to\nClients\n, and click\nCreate Client\n.\nFill in a Client ID, enable\nService Accounts Roles\n, and save.\nNow, we need to set this up as a userless Service Account. Ensure that:\n-\n​​​​​​\nClient Authentication = On\n- Standard Flow = Off\n- Service Accounts Roles = On\nPress\nNext\n, do not change the\nLogin Settings\n, and\nsave\n. In the Credentials\ntab, you should see that the\nClient Authenticator\nis already set to\nClient ID And Secret\n. The\nClient Secret\n(aka password) will have been auto generated for you.\nNow all we have to do is give it permission to use GraphQL! Navigate to\nService Account Roles\n, and apply the roles\nDMM_Public_API.\nIf you also wish to see the GraphQL schema (which you should!), also apply the role\nMMM_graphql-introspection.\n💡In non Portal environments, you may need to go to MMM settings and set ataccama.one.mmm.api.introspection.enabled = True\nataccama.one.mmm.api.introspection.roles = MMM_graphql-introspection.\nAuthentication options: What works best? 🔒\nAtaccama provides several authentication options tailored for different use cases. Let’s dive into the most popular methods:\n1. Basic user + password\nBasic user/pass authentication is the simplest way of authenticating anything. However, it is only suitable for ad-hoc testing/development, and\nnever in production environments.\nFor situations like ONE Desktop connecting to ONE Platform, you can enhance this by selecting the OpenID auth type instead. On DPM runtimeConfig, you can use a service principle with client id/secret as the ONE Desktop password, instead of a generic user account. If you are using ONE Desktop, you may find it more secure to use Web Login, which allows authentication via SSO against an Active Directory via Keycloak.\n2. Bearer Token Authentication\n🛡️\nBearer Tokens are a secure and time-limited way to grant access reducing the risk of credential theft. Additionally, tokens can be scoped to specific permissions, adhering to the principle of least privilege, and they eliminate the need to store or transmit user passwords, further enhancing security. Here’s how you can generate one manually:\nUse your\nClient ID\nand\nClient Secret\nto authenticate against Keycloak’s API at  <env_url>/auth/realms/ataccamaone/protocol/openid-connect/token\nYou’ll receive a token which can then be included in the HTTP headers of subsequent API requests as\nAuthorization: Bearer <token>\n.\n💡Pro Tip: Some programs (e.g. ONE Desktop and Postman) can automatically generate, authenticate and refresh bearer tokens in the background.\nAuthenticating (GraphQL) requests\n1️⃣ via Postman\nPostman is a great way to debug and ensure that your API requests are set up/working properly, especially as ONE Desktop is difficult to use and configure correctly.\nTo get a bearer token manually, make a\nPOST\nrequest to\n<env_url>/auth/realms/ataccamaone/protocol/openid-connect/token\nwith Body Type of\nx-www-form-urlencoded\n, and the following Body:\ngrant_type : client_credentials\nclient_id : <your_client_id>\nclient_secret : <your_client_secret>\nYou can then copy the\naccess_token\nfrom the response\nTo run a GraphQL query, make a new\nPOST\nrequest to\n<env_url>/graphql\n. Set the\nAuthorization Auth Type\nto\nBearer Token\n, and paste in your access token (without quote marks). Then set the\nBody\nto\nGraphQL\n, and write your query:\nAlternatively, you can skip the need to get a bearer token via a separate query, by setting Auth Type = OAuth 2.0, and setting the Access Token URL, Client ID and Client Secret fields accordingly.\n2️⃣ GraphQL Playground\n🎢\nMost deployments have an inbuilt GraphQL playground at\n<env>/playground\n, which you can use to run GraphQL queries without having to use any extra programs. This can be super useful for development/ad-hoc purposes.\nWhen using the playground, you will get an Unauthorized response unless you provide some authorization via the HTTP Headers section at the bottom.\nSince this is just a playground, a service principle + bearer token is often too overbearing, and so you may want to just use a traditional user with username/password authentication (e.g. the admin account). You would provide this by:\nBase64 encoding\na string in the form of\n<user>:<password>\n(for example,\nadmin:123456\nis base64 encoded as\nYWRtaW46MTIzNDU2\n)\nUse the HTTP Header\n{“Authorization”: “Basic YWRtaW46MTIzNDU2”}\n⚠️Be aware that quotation marks may not copy/paste correctly.\n3️⃣ via ONE Desktop\nFor most production use cases today, we are limited to ONE Desktop’s way to send API requests.\nIn order to ensure that passwords do not get stored plaintext via plan steps, we must first set up a server (and if not running locally, generate a runtimeConfig and apply it to DPM). Set Authentication to OpenID Connect, use the client id/secret, and set the token endpoint to <url>/auth/realms/ataccamaone/protocol/openid-connect/token.\nIf this plan will be run on the environment itself, be sure to update runtimeProperties on the DPM.\n💡You can also get a bearer token by setting the server to have\nBasic\nauthentication, and making a JSON call step with\ngrant_type=client_credentials\nas the Input Template and\nContent-Type\n=\napplication/x-www-form-urlencoded\nin the headers.\nNext, hit an API by using the workaround in which we make a Random Record Generator step with 1 record, then connecting this as a meaningless input into a JSON Call step.\nI\nn the Input Template section, write your query in the form\n{\n\"query\"\n:\n“\n<graphql_query>”}\nIt is recommended to use the playground to develop your query, then use the Copy Curl button and remove the extra generated text. Your query will then look like:\n'{\"query\":\"query getProfileData {\\n  catalogItems(\\n    versionSelector: { publishedVersion: true }\\n    filter: \\\"name = '\ncustomers\n'\\\"\\n  ) {\\n    edges {\\n      node {\\n        publishedVersion {\\n          catalogItemName: name\\n          profilingConfigurationInstances(size: 1) {\\n            edges {\\n              node {\\n                publishedVersion {\\n                  profiles(\\n                    orderBy: { property: \\\"profiledAt\\\", direction: DESC }\\n                    size: 1\\n                  ) {\\n                    edges {\\n                      node {\\n                        publishedVersion {\\n                          attributeProfiles(\\n                            filter: \\\"displayName = '\ncustomernumbe\nr'\\\"\\n                          ) {\\n                            edges {\\n                              node {\\n                                publishedVersion {\\n                                  attributeName: displayName\\n                                  attributeProfileData {\\n                                    publishedVersion {\\n                                      numMax\\n                                      numStdDeviation\\n                                    }\\n                                  }\\n                                }\\n                              }\\n                            }\\n                          }\\n                        }\\n                      }\\n                    }\\n                  }\\n                }\\n              }\\n            }\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\n\"}'\n4️⃣ via Python\n🐍\nUnlike ONE Desktop, sending API requests is basic Python, and as such, you can use either the built-in library urllib or the fantastic and industry-standard\nrequests\npackage.\nYou can also configure custom logic to refresh the Bearer Token should it expire mid-execution. The following Python SDK will do this automatically for you.\nimport\nrequests\n# Get the Bearer Auth Token\ntoken_resp = requests.post(\n\"<url>/auth/realms/ataccamaone/protocol/openid-connect/token\"\n, \n\tdata={\n\"client_id\"\n: client_id,\n\"client_secret\"\n: client_secret,\n\"grant_type\"\n:\n\"client_credentials\"\n},\n\theaders={\n\"Content-Type\"\n:\n\"application/x-www-form-urlencoded\"\n}\n\t)\ntoken_resp.raise_for_status()\ntoken = token_resp.json()[\n\"access_token\"\n]\n# Make a GraphQL Request\nreq_resp = requests.post(\n\"<url>/graphql\"\n,\n\t\t\t\tjson={\n\"query\"\n: <graphql_query>},\n\t\t\t\theaders={\n\"Authorization\"\n: f\n\"Bearer {bearer_token}\"\n})\nreq_resp.raise_for_status()\noutput = resp.json()[\n\"data\"\n]\nThis is all from the first part! Next up is working with MMM in GraphQL. If you have any questions or tips please share in the comments below👇"
    },
    "answers": []
  },
  {
    "title": "How can we handle csv output from API response",
    "url": "https://community.ataccama.com/data-quality-catalog-94/how-can-we-handle-csv-output-from-api-response-821",
    "question": {
      "author": "mahesh Ar",
      "timestamp": "[No timestamp]",
      "content": "How can we handle csv output from API response?\nI tried to use json call ,SOAP call but it didn't work.Anyway, can we handle in One desktop csv output from API."
    },
    "answers": [
      {
        "author": "AKislyakov",
        "timestamp": "1 year ago",
        "content": "Hi\n@mahesh Ar\n,\nAtaccama One desktop does not have built in step to handle CSV APIs.\nI can suggest one of two ways.\nFirst one is to use\nHTTP Get\nstep in the workflow to save a result of an API call as a text file and then process it using plan and\nText File Reader\nstep.\nThe second one is to use\nWeb lookup\nstep. The Web lookup step will obtain the CSV data and will warp it into an XML object which you can subsequently parse using\nSplitter\nand\nTokenizer\nsteps. The result should be similar to this:\nHide content\nShow content\n<?xml version='1.0' encoding='UTF-8'?>\n<\nhtml\n>\n<\nhead\n>\n<\nmeta\nname\n=\n\"generator\"\ncontent\n=\n\"HTML Tidy for Java (vers. 2009-12-01), see jtidy.sourceforge.net\"\n/>\n<\ntitle\n/>\n</\nhead\n>\n<\nbody\n>\nsrc_full_name;pur_full_name;pur_first_name_orig;pur_last_name_orig;std_first_name;std_last_name;std_titles_prefix;pat_full_name;score_full_name;exp_full_name;pur_full_name_DQ_status;pur_full_name_status John Smith;John Smith;John;Smith;John;Smith;;F! L!;0;;NAME_OK;OK\n</\nbody\n>\n</\nhtml\n>\nI’d recommend going with the first approach if it fits your requirements as it more straightforward and easier to debug."
      },
      {
        "author": "mahesh Ar",
        "timestamp": "1 year ago",
        "content": "Hi\n@AKislyakov\n,\nThanks for answering to above questions.\nI am trying to implement the first approach and looking for the\nHTTP Get\nstep.\nI don't see\nHTTP Get\nstep in one desktop .May i know is step name right?\nThanks,\nMahesh Arigela"
      },
      {
        "author": "AKislyakov",
        "timestamp": "1 year ago",
        "content": "The stem name is HTTP Get. This is a\nworkflow\nstep and should be available in the workflow (ewf) context."
      },
      {
        "author": "may_kwok",
        "timestamp": "5 months ago",
        "content": "Hi ​\n@AKislyakov\n,\nI am running into a use case where my only option is to do an API with a csv response, and we need to put it into a VCI, so would like to avoid using a .ewf.\nIf we just do a pure http get it does work. Can you explain a bit more on how to make the step convert the incoming csv into xml?\nThanks!"
      },
      {
        "author": "AKislyakov",
        "timestamp": "5 months ago",
        "content": "Hi ​\n@may_kwok\n,\nYou don’t need to do anything specific the step will automatically wrap csv into xml tags, as in my example. Parsing the result though might be a little challenging."
      },
      {
        "author": "may_kwok",
        "timestamp": "5 months ago",
        "content": "After a day of tinkering, I’ve got something.\nFruit.csv\nA csv file that I just faked out to get some data. As business has progressed this includes vegetables too in addition to fruits....\nApi.plan\nA plan that gives out the above csv.\nApi.online\nAn online file that allows ONE desktop to start this as an API. To start it, click this button:\nShould look like this once started:\nTest.wstest\nAllows you to run simple HTTP call. This is what it looks like after I run it, so I know what the raw response is going to be:\n(You can see the header tabs to send calls with authorisation headers etc)\nGet.comp\nThe actual logic. Calls the API, splits the data, splits the columns, and gets the data. This solution does not care if the metadata changes. It’ll just find the columns you tell it to find. If it’s there it’ll parse it, if it’s not there it’ll just be null. Extra columns are just ignored.\nWeb lookup step configuration is very simple:\nI make an assumption\nthat every time I encounter “ “ (double quote space double quote) it is an end of line.\nThe orig output file on the bottom left looks like this:\nfeed column is the entire feed repeated throughout the rows.\nSplit feed is split out into rows\nAttr splits even further down to attributes.\nGet col name:\nWe know that row 1 of the set is the header. So we populate the value of the row 1 into the same positions for all rows.\nInstance attributes output does not aggregate, so we are populating the “best” attribute, based on order by “row”, group by attr_pos.\n(For those well versed in window functions in sql, this is similar to a rank() over (partition by attr_pos order by row), and then picking the value of attr that is of rank 1)\nFind last column position: (so we can get rid of a legit trailing double quote)\nGroup by 1, because I want to just check the entire table.\nGet the max attribute position:\nGet rid of beginning and ending double quotes:\n(If first column, get rid of first char. If last column, get rid of last char)\nGet rid of the header row by filtering out row=1\nPivot it back to the way we want it\nGroup by rd2. Rd2 was the record descriptor that we used to get from row to row/column unpivot. We use that to get back to the row granularity.\nI’ve got same config for all 3 columns, just showing 1 for brevity:\nThe step is configured to recognise id, name and type. The column “colour” is not required and not mentioned in the step.\nOut file looks like this:\nOut 2 (which is going to be the actual data I need):\nI hope this helps someone!"
      },
      {
        "author": "may_kwok",
        "timestamp": "5 months ago",
        "content": "I’ve just noticed that I’ve been over-ambitious to remove the last double quote on each row that I’ve removed 1 extra char, but hopefully this gives the idea….\nChanging the column assigner step for fixing the quotes to this should do the trick:\ncase\n(attr_pos=\n'1'\nand\nleft(data_col_name,\n1\n)=\n'\"'\n, substr(data_col_name,\n1\n),\ncase\n(attr_pos=last_col_pos\nand\nright(data_col_name,\n1\n)=\n'\"'\n, left(data_col_name,length(data_col_name)-\n1\n),data_col_name))\nand\ncase\n(attr_pos=\n'1'\nand\nleft(attr,\n1\n)=\n'\"'\n, substr(attr,\n1\n),\ncase\n(attr_pos=last_col_pos\nand\nright(attr,\n1\n)=\n'\"'\n, left(attr,length(attr)-\n1\n),attr))"
      },
      {
        "author": "Cansu",
        "timestamp": "5 months ago",
        "content": "Thank you ​\n@may_kwok\nfor the elaborate solution! ​\n@mahesh Ar\ndoes this answer by ​\n@may_kwok\nhelp? Let us know please 🙋‍♀️"
      }
    ]
  },
  {
    "title": "Execution Error",
    "url": "https://community.ataccama.com/data-quality-catalog-94/execution-error-1533",
    "question": {
      "author": "Ajeesh_G",
      "timestamp": "[No timestamp]",
      "content": "Hi,\nI am running a workflow that includes a task called 'Execute SQL'. This task updates some values in the SQL database, but the code is quite heavy. When the workflow reaches this task, I encounter the following error message. Can anyone assist?"
    },
    "answers": [
      {
        "author": "may_kwok",
        "timestamp": "5 months ago",
        "content": "I’m no DBA, but this suggests that the Database side decides it doesn’t want to play anymore and close the connection their side.\nIf I see this I’d call the DBA to see if there’s anything they can do to help. Maybe there’s some timeout setup on the db side that they could adjust? Otherwise can that SQL be tuned to run faster…?"
      },
      {
        "author": "AKislyakov",
        "timestamp": "5 months ago",
        "content": "If a problem occurs at the very beginning of the step execution, it is likely due to a stale connection. The Runtime Server, by default, keeps a pool of up to 10 open database connections. Some database engines do not like this behavior and close connections after a period of inactivity. To address this, you can either:\nSet\nmaxIdleSize\nto 0, so connections are not reused.\nSet\nmaxAge\nto close connections after a period of time.\nInclude\nvalidationQuery\n, so each connection is checked before use.\nMore details could be found in\nRuntime Configuration :: Ataccama ONE"
      },
      {
        "author": "Albert de Ruiter",
        "timestamp": "5 months ago",
        "content": "In addition you can also try to run the SQL via a query tool and see how that goes. If you then also encounter the problem you better contact a DBA, otherwise the suggestion by Kislyakov seems most appropriate."
      },
      {
        "author": "Cansu",
        "timestamp": "5 months ago",
        "content": "Thank you for all the great ideas & solutions provided ​\n@may_kwok\n, ​\n@AKislyakov\n, and ​\n@Albert de Ruiter\n🙌🙌 ​\n@Ajeesh_G\ndoes this help? if yes, please mark a best answer 🙋‍♀️"
      }
    ]
  },
  {
    "title": "Tagging of terms to attributes",
    "url": "https://community.ataccama.com/data-quality-catalog-94/tagging-of-terms-to-attributes-1530",
    "question": {
      "author": "RAZORX",
      "timestamp": "[No timestamp]",
      "content": "Hi Is it possible to tag Terms to the attribute metadata present in one web, while importing the terms in bulk from one desktop?\nAsk: Attributes and Tables are already present in one web. Importing business terms from csv file using one desktop.\nSo while importing the business terms using one desktop is it possible to directly tag those business terms with their corresponding attributes?(The tagging list also i want to provide in a csv file and don’t want to use the automated tagging from ataccama)"
    },
    "answers": [
      {
        "author": "may_kwok",
        "timestamp": "5 months ago",
        "content": "Hi ​\n@RAZORX\n,\nYes it should be possible.\nAn attribute should have a property which is an array embedded, called termInstances. Inside that termInstances array it should contain a target - which is the reference to the term id, and a displayName which should be the name of the term.\nYou already have the catalog items and the attributes.\nYou need to create the terms, so you get the term id. (you sound like you’re ok with how to do this part?)\nTake that term id and map it to the attribute id or catalog item id you want to tag\nUse another metadata writer step to write the termInstance property for the catalog items and/or attributes. target should be the termid, and displayName should be the name of the term.\nDoes this help?"
      },
      {
        "author": "RAZORX",
        "timestamp": "5 months ago",
        "content": "Hi ​\n@may_kwok\n,\nThank you for the reply, so as per your suggestion, there will be two metadata writer right?\nFirst one is for the attribute entity and the second one is for the termInstance property. As i already have created the terms and have the term Id.\nBut the problem i am facing is in the metadata writer, i am not getting any embedded array termInstance option while choosing for attribute entity, so how will i link the attribute to the termInstance which is a separate entity and only accepts targetId(TermId) and Display name with the already present attributes. If you can help me with any flow diagram of the plan it would be really helpful."
      },
      {
        "author": "RAZORX",
        "timestamp": "5 months ago",
        "content": "Hi ​\n@may_kwok\n,\nI tried your way and seems like it worked. Thank you again for the help."
      },
      {
        "author": "may_kwok",
        "timestamp": "5 months ago",
        "content": "Glad you figured it out before I could come back to this 😁\nFor the benefit of the forum:\nLet’s say I have my term:\nAnd I have my catalog item:\nI’m using a random record generator to just fake out 1 record, but you’ll likely have a text file or a data feed of some sort to load this in bulk:\n(published state because I’m confident that I’ve got this right and I don’t want to click into each tagged instance to request approval. Your mileage might vary at your organisation with how strict your governance policies are)\nThis config means, we are writing into the termInstance, which belongs as a child entity to the catalogItem, under the termInstances property.\nId Column Name is left blank here because I know I just want to create new objects, and I need to let ONE create the ID for me. If I were to want to update that particular tagging instance, then I’d need to query the existing tags beforehand to get the id, then supply it to let ONE know it’s an update.\nThe term instance id, once tagged, will now be supplied under the “Created Id Column Name” output.\nAnd once it’s run, voila!\nYou would use a very similar syntax to tag an attribute. Just get the attribute id instead of the catalog item id, and write it into the term instance attached to the attribute, rather than the catalog item:\nAfter running this, the tagging is now also on the attribute:\nHope this helps!"
      },
      {
        "author": "RAZORX",
        "timestamp": "5 months ago",
        "content": "Hi ​\n@may_kwok\n,\nThank you very much. Much appreciate it. But one final question, is it possible to export it out like the term relationship with tables or attribute from lower environment to higher?\nLike as per my understanding, the Production environment for instance will have a different attribute ids and tables ids same goes for term ids too. So this tagging of ids to each other is a manual work right is there any way to automate it and by automate it i am asking about using the detection rules."
      },
      {
        "author": "Albert de Ruiter",
        "timestamp": "5 months ago",
        "content": "Hi ​\n@RAZORX\n, indeed like you say the id's will differ per environement, so ‘promoting’ the relations to a higher environement will not work.\nWe use the folloowing approach, which you might find useful as well. We apply an import template (csv file) in which we define the relations by entering a term name with the related catalog item atribute name. In the plan the related id's can be looked up via joins. Once you have the plan, you can use it on all environments.\nKind regards,\nAlbert"
      },
      {
        "author": "Cansu",
        "timestamp": "5 months ago",
        "content": "Thank you ​\n@may_kwok\nand ​\n@Albert de Ruiter\nfor sharing your approaches and different solutions here! 🙌 ​\n@RAZORX\nplease mark best answer if you’ve found your solution 🙋‍♀️"
      }
    ]
  },
  {
    "title": "DQ Rule Synchronise name with initials",
    "url": "https://community.ataccama.com/data-quality-catalog-94/dq-rule-synchronise-name-with-initials-1526",
    "question": {
      "author": "JTH",
      "timestamp": "[No timestamp]",
      "content": "Hi,\nI have two columns: first name and initials.\n#\nfirst_name\ninitials\n1\nJohn\nJ.\n2\nRobin Earl\nR.E.\n3\nMark Tate Lynch\nM.T.L.\n4\nBert Ern\nB.E.\n5\nJohan Flugg\nJ.F.\nIs there a DQ rule that can check if the two columns are correctly in synch in what they should be? I understand checking the initials format is one thing, but I actually want to check if it is correctly formatted based on the first_name column. Looking for a preferred solution in ONE Web."
    },
    "answers": [
      {
        "author": "AKislyakov",
        "timestamp": "5 months ago",
        "content": "Hi ​\n@JTH\n,\nThere are 2 solutions:\n1. Using\nset.*\nfunctions\nreplace\n(\nset\n.mapExp(first_name,\n' '\n, (x) {\nleft\n(x,\n1\n)+\n'.'\n}),\n' '\n,\n''\n)\nThis code works with\nfirst_name\nas a set of words separated by\n(space). For each\nword\nin a set function\nleft(word,1)+'.'\nis applied.\nThe result is a set of first letters with dot separated by space. The final replaces removes spaces.\n2. Using Regular expressions\nreplace\n(substituteAll(@\n\"([^ ])([^ ]*)\\b\"\n,\n'$1.'\n,first_name),\n' '\n,\n''\n)\nAgain, each word is replaced with first letter + dot and then spaces removed.\nThe first one is generally faster, the last one requires less Ata-specific knowledge"
      },
      {
        "author": "JTH",
        "timestamp": "5 months ago",
        "content": "​\n@AKislyakov\nthank you! I've done the following in advanced expression:\nfirst_name is not null\nand initials is not null\nand type_person = 'N' -- N = natural person\nand replace(set.mapExp(first_name, ' ', (x) { left(x,1)+'.' }), ' ', '') is not in {initials}"
      }
    ]
  }
]